<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Collective communication · MPI.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MPI.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">MPI.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">MPI.jl</a></li><li><a class="tocitem" href="../configuration/">Configuration</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><a class="tocitem" href="../knownissues/">Known issues</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/01-hello/">Hello world</a></li><li><a class="tocitem" href="../examples/02-broadcast/">Broadcast</a></li><li><a class="tocitem" href="../examples/03-reduce/">Reduce</a></li><li><a class="tocitem" href="../examples/04-sendrecv/">Send/receive</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../library/">Library information</a></li><li><a class="tocitem" href="../environment/">Environment</a></li><li><a class="tocitem" href="../comm/">Communicators</a></li><li><a class="tocitem" href="../buffers/">Buffers</a></li><li><a class="tocitem" href="../pointtopoint/">Point-to-point communication</a></li><li class="is-active"><a class="tocitem" href>Collective communication</a><ul class="internal"><li><a class="tocitem" href="#Synchronization"><span>Synchronization</span></a></li><li><a class="tocitem" href="#Broadcast"><span>Broadcast</span></a></li><li><a class="tocitem" href="#Gather/Scatter"><span>Gather/Scatter</span></a></li><li><a class="tocitem" href="#Reduce/Scan"><span>Reduce/Scan</span></a></li></ul></li><li><a class="tocitem" href="../onesided/">One-sided communication</a></li><li><a class="tocitem" href="../topology/">Topology</a></li><li><a class="tocitem" href="../io/">I/O</a></li><li><a class="tocitem" href="../advanced/">Advanced</a></li></ul></li><li><a class="tocitem" href="../refindex/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Collective communication</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Collective communication</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/collective.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Collective-communication"><a class="docs-heading-anchor" href="#Collective-communication">Collective communication</a><a id="Collective-communication-1"></a><a class="docs-heading-anchor-permalink" href="#Collective-communication" title="Permalink"></a></h1><h2 id="Synchronization"><a class="docs-heading-anchor" href="#Synchronization">Synchronization</a><a id="Synchronization-1"></a><a class="docs-heading-anchor-permalink" href="#Synchronization" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MPI.Barrier" href="#MPI.Barrier"><code>MPI.Barrier</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Barrier(comm::Comm)</code></pre><p>Blocks until <code>comm</code> is synchronized.</p><p>If <code>comm</code> is an intracommunicator, then it blocks until all members of the group have called it.</p><p>If <code>comm</code> is an intercommunicator, then it blocks until all members of the other group have called it.</p><p><strong>External links</strong></p><ul><li><code>MPI_Barrier</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Barrier.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L1-L12">source</a></section></article><h2 id="Broadcast"><a class="docs-heading-anchor" href="#Broadcast">Broadcast</a><a id="Broadcast-1"></a><a class="docs-heading-anchor-permalink" href="#Broadcast" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MPI.Bcast!" href="#MPI.Bcast!"><code>MPI.Bcast!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Bcast!(buf, root::Integer, comm::Comm)</code></pre><p>Broadcast the buffer <code>buf</code> from <code>root</code> to all processes in <code>comm</code>.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.bcast"><code>bcast</code></a></li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Bcast</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Bcast.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L19-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.bcast" href="#MPI.bcast"><code>MPI.bcast</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">bcast(obj, root::Integer, comm::Comm)</code></pre><p>Broadcast the object <code>obj</code> from rank <code>root</code> to all processes on <code>comm</code>. This is able to handle arbitrary data.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Bcast!"><code>Bcast!</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L42-L50">source</a></section></article><h2 id="Gather/Scatter"><a class="docs-heading-anchor" href="#Gather/Scatter">Gather/Scatter</a><a id="Gather/Scatter-1"></a><a class="docs-heading-anchor-permalink" href="#Gather/Scatter" title="Permalink"></a></h2><h3 id="Gather"><a class="docs-heading-anchor" href="#Gather">Gather</a><a id="Gather-1"></a><a class="docs-heading-anchor-permalink" href="#Gather" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgather!" href="#MPI.Allgather!"><code>MPI.Allgather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgather!(sendbuf, recvbuf::UBuffer, comm::Comm)
Allgather!(sendrecvbuf::UBuffer, comm::Comm)</code></pre><p>Each process sends the contents of <code>sendbuf</code> to the other processes, the result of which is stored in rank order into <code>recvbuf</code>.</p><p><code>sendbuf</code> can be a <a href="../buffers/#MPI.Buffer"><code>Buffer</code></a> object, or any object for which <a href="../buffers/#MPI.Buffer_send"><code>Buffer_send</code></a> is defined, and should be the same length on all processes.</p><p><code>recvbuf</code> can be a <a href="../buffers/#MPI.UBuffer"><code>UBuffer</code></a>, or can be an <code>AbstractArray</code> if the length can be determined from <code>sendbuf</code>.</p><p>If only one buffer <code>sendrecvbuf</code> is provided, then on each process the data to send is assumed to be in the area where it would receive its own contribution.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allgather"><code>Allgather</code></a> for the allocating operation</li><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="@ref"><code>Allgatherv</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Gather!"><code>Gather!</code></a> to send only to a single root process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L296-L319">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgather" href="#MPI.Allgather"><code>MPI.Allgather</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgather(sendbuf, comm)</code></pre><p>Each process sends the contents of <code>sendbuf</code> to the other processes, who store the results in rank order allocating the output buffer.</p><p><code>sendbuf</code> can be an <code>AbstractArray</code> or a scalar, and should be the same size on all processes.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allgather!"><code>Allgather!</code></a> for the mutating operation</li><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="@ref"><code>Allgatherv</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Gather!"><code>Gather!</code></a> to send only to a single root process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L345-L361">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgatherv!" href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgatherv!(sendbuf, recvbuf::VBuffer, comm::Comm)
Allgatherv!(sendrecvbuf::VBuffer, comm::Comm)</code></pre><p>Each process sends the contents of <code>sendbuf</code> to all other process. Each process stores the received in the <a href="../buffers/#MPI.VBuffer"><code>VBuffer</code></a> <code>recvbuf</code>.</p><p><code>sendbuf</code> can be a <a href="../buffers/#MPI.Buffer"><code>Buffer</code></a> object, or any object for which <a href="../buffers/#MPI.Buffer_send"><code>Buffer_send</code></a> is defined.</p><p>If only one buffer <code>sendrecvbuf</code> is provided, then for each process, the data to be sent is taken from the interval of <code>recvbuf</code> where it would store its own data.</p><p><strong>See also</strong></p><ul><li><a href="@ref"><code>Allgatherv</code></a> for the allocating operation</li><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="@ref"><code>Gatherv</code></a> to send the result to a single process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgatherv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L370-L389">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Allgatherv</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather!" href="#MPI.Gather!"><code>MPI.Gather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gather!(sendbuf, recvbuf::Union{UBuffer,Nothing}, root::Integer, comm::Comm)</code></pre><p>Each process sends the contents of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>recvbuf</code>.</p><p><code>sendbuf</code> should be a <a href="../buffers/#MPI.Buffer"><code>Buffer</code></a> object, or any object for which <a href="../buffers/#MPI.Buffer_send"><code>Buffer_send</code></a> is defined, with the same length on all processes, and should be the same length on all processes.</p><p>On the root process, <code>sendbuf</code> can be <a href="../buffers/#MPI.IN_PLACE"><code>MPI.IN_PLACE</code></a> on the root process, in which case the corresponding entries in <code>recvbuf</code> are assumed to be already in place (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Gather</code>). For example:</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    MPI.Gather!(MPI.IN_PLACE, UBuffer(buf, count), root, comm)
else
    MPI.Gather!(buf, nothing, root, comm)
end</code></pre><p><code>recvbuf</code> on the root process should be a <a href="../buffers/#MPI.UBuffer"><code>UBuffer</code></a>, or can be an <code>AbstractArray</code> if the length can be determined from <code>sendbuf</code>. On non-root processes it is ignored and can be <code>nothing</code>.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Gather"><code>Gather</code></a> for the allocating operation.</li><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Allgather!"><code>Allgather!</code></a> to send the result to all processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L166-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather" href="#MPI.Gather"><code>MPI.Gather</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gather(sendbuf, root, comm::Comm)</code></pre><p>Each process sends the contents of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p><code>sendbuf</code> can be an <code>AbstractArray</code> or a scalar, and should be the same length on all processes.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Gather!"><code>Gather!</code></a> for the mutating operation.</li><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="@ref"><code>Gatherv</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Allgather!"><code>Allgather!</code></a>/<a href="#MPI.Allgather"><code>Allgather</code></a> to send the result to all processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L223-L239">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gatherv!" href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gatherv!(sendbuf, recvbuf::Union{VBuffer,Nothing}, root, comm)</code></pre><p>Each process sends the contents of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> stores elements in rank order in the buffer <code>recvbuf</code>.</p><p><code>sendbuf</code> should be a <a href="../buffers/#MPI.Buffer"><code>Buffer</code></a> object, or any object for which <a href="../buffers/#MPI.Buffer_send"><code>Buffer_send</code></a> is defined, with the same length on all processes.</p><p>On the root process, <code>sendbuf</code> can be <a href="../buffers/#MPI.IN_PLACE"><code>MPI.IN_PLACE</code></a>, in which case the corresponding entries in <code>recvbuf</code> are assumed to be already in place. For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gatherv!(MPI.IN_PLACE, VBuffer(buf, counts), root, comm)
else
    Gatherv!(buf, nothing, root, comm)
end</code></pre><p><code>recvbuf</code> on the root process should be a <a href="../buffers/#MPI.VBuffer"><code>VBuffer</code></a>, or can be an <code>AbstractArray</code> if the length can be determined from <code>sendbuf</code>. On non-root processes it is ignored and can be <code>nothing</code>.</p><p><strong>See also</strong></p><ul><li><a href="@ref"><code>Gatherv</code></a> for the allocating operation</li><li><a href="#MPI.Gather!"><code>Gather!</code></a></li><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="@ref"><code>Allgatherv</code></a> to send the result to all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gatherv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L245-L275">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Gatherv</code>. Check Documenter&#39;s build log for details.</p></div></div><h3 id="Scatter"><a class="docs-heading-anchor" href="#Scatter">Scatter</a><a id="Scatter-1"></a><a class="docs-heading-anchor-permalink" href="#Scatter" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatter!" href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatter!(sendbuf::Union{UBuffer,Nothing}, recvbuf, root::Integer, comm::Comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks, sending the <code>j</code>-th chunk to the process of rank <code>j-1</code> into the <code>recvbuf</code> buffer.</p><p><code>sendbuf</code> on the root process should be a <a href="../buffers/#MPI.UBuffer"><code>UBuffer</code></a> (an <code>Array</code> can also be passed directly if the sizes can be determined from <code>recvbuf</code>). On non-root processes it is ignored, and <code>nothing</code> can be passed instead.</p><p><code>recvbuf</code> is a <a href="../buffers/#MPI.Buffer"><code>Buffer</code></a> object, or any object for which <code>Buffer(recvbuf)</code> is defined. On the root process, it can also be <a href="../buffers/#MPI.IN_PLACE"><code>MPI.IN_PLACE</code></a>, in which case it is unmodified. For example:</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    MPI.Scatter!(UBuffer(buf, count), MPI.IN_PLACE, root, comm)
else
    MPI.Scatter!(nothing, buf, root, comm)        
end</code></pre><p><strong>See also</strong></p><ul><li><a href="#MPI.Scatterv!"><code>Scatterv!</code></a> if the number of elements varies between processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatter</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatter.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatter.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L69-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatterv!" href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatterv!(sendbuf::Union{VBuffer,Nothing}, recvbuf, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the <code>j</code>th chunk to the process of rank <code>j-1</code> into the <code>recvbuf</code> buffer.</p><p><code>sendbuf</code> on the root process should be a <a href="../buffers/#MPI.VBuffer"><code>VBuffer</code></a>. On non-root processes it is ignored, and <code>nothing</code> can be passed instead.</p><p><code>recvbuf</code> is a <a href="../buffers/#MPI.Buffer"><code>Buffer</code></a> object, or any object for which <code>Buffer(recvbuf)</code> is defined. On the root process, it can also be <a href="../buffers/#MPI.IN_PLACE"><code>MPI.IN_PLACE</code></a>, in which case it is unmodified. For example:</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    MPI.Scatterv!(VBuffer(buf, counts), MPI.IN_PLACE, root, comm)
else
    MPI.Scatterv!(nothing, buf, root, comm)
end</code></pre><p><strong>See also</strong></p><ul><li><a href="#MPI.Scatter!"><code>Scatter!</code></a> if the number of elements are the same for all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatterv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatterv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatterv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L119-L144">source</a></section></article><h3 id="All-to-all"><a class="docs-heading-anchor" href="#All-to-all">All-to-all</a><a id="All-to-all-1"></a><a class="docs-heading-anchor-permalink" href="#All-to-all" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoall!" href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoall!(sendbuf::UBuffer, recvbuf::UBuffer, comm::Comm)
Alltoall!(sendrecvbuf::UBuffer, comm::Comm)</code></pre><p>Every process divides the <a href="../buffers/#MPI.UBuffer"><code>UBuffer</code></a> <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of equal size, sending the <code>j</code>-th chunk to the process of rank <code>j-1</code>.  Every process stores the data received from rank <code>j-1</code> process in the <code>j</code>-th chunk of the buffer <code>recvbuf</code>.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p>If only one buffer <code>sendrecvbuf</code> is used, then data is overwritten.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Alltoall"><code>Alltoall</code></a> for the allocating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoall</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoall.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoall.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L412-L435">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoall" href="#MPI.Alltoall"><code>MPI.Alltoall</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoall(sendbuf::UBuffer, comm::Comm)</code></pre><p>Every process divides the <a href="../buffers/#MPI.UBuffer"><code>UBuffer</code></a> <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of equal size, sending the <code>j</code>-th chunk to the process of rank <code>j-1</code>. Every process allocates the output buffer and stores the data received from the process on rank <code>j-1</code> in the <code>j</code>-th chunk.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p><strong>See also</strong></p><ul><li><a href="#MPI.Alltoall!"><code>Alltoall!</code></a> for the mutating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoall</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoall.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoall.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L458-L479">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoallv!" href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoallv!(sendbuf::VBuffer, recvbuf::VBuffer, comm::Comm)</code></pre><p>Similar to <a href="#MPI.Alltoall!"><code>Alltoall!</code></a>, except with different size chunks per process. </p><p><strong>See also</strong></p><ul><li><a href="../buffers/#MPI.VBuffer"><code>VBuffer</code></a></li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoallv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoallv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoallv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L484-L494">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Alltoallv</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Reduce/Scan"><a class="docs-heading-anchor" href="#Reduce/Scan">Reduce/Scan</a><a id="Reduce/Scan-1"></a><a class="docs-heading-anchor-permalink" href="#Reduce/Scan" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MPI.Reduce!" href="#MPI.Reduce!"><code>MPI.Reduce!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Reduce!(sendbuf, recvbuf, op, root::Integer, comm::Comm)
Reduce!(sendrecvbuf, op, root::Integer, comm::Comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code> and stores the result in <code>recvbuf</code> on the process of rank <code>root</code>.</p><p>On non-root processes <code>recvbuf</code> is ignored, and can be <code>nothing</code>. </p><p>To perform the reduction in place, provide a single buffer <code>sendrecvbuf</code>.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Reduce"><code>Reduce</code></a> to handle allocation of the output buffer.</li><li><a href="#MPI.Allreduce!"><code>Allreduce!</code></a>/<a href="#MPI.Allreduce"><code>Allreduce</code></a> to send reduction to all ranks.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Reduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L521-L539">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Reduce" href="#MPI.Reduce"><code>MPI.Reduce</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Reduce(sendbuf, op, root::Integer, comm::Comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>, returning the result <code>recvbuf</code> on the process of rank <code>root</code>, and <code>nothing</code> on non-root processes.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will be a value of the same type.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Reduce!"><code>Reduce!</code></a> for mutating and in-place operations</li><li><a href="#MPI.Allreduce!"><code>Allreduce!</code></a>/<a href="#MPI.Allreduce"><code>Allreduce</code></a> to send reduction to all ranks.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Reduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L565-L580">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allreduce!" href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allreduce!(sendbuf, recvbuf, op, comm::Comm)
Allreduce!(sendrecvbuf, op, comm::Comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>, storing the result in the <code>recvbuf</code> of all processes in the group.</p><p><code>Allreduce!</code> is equivalent to a <a href="#MPI.Reduce!"><code>Reduce!</code></a> operation followed by a <a href="#MPI.Bcast!"><code>Bcast!</code></a>, but can lead to better performance.</p><p>If only one <code>sendrecvbuf</code> buffer is provided, then the operation is performed in-place.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allreduce"><code>Allreduce</code></a>, to handle allocation of the output buffer.</li><li><a href="#MPI.Reduce!"><code>Reduce!</code></a>/<a href="#MPI.Reduce"><code>Reduce</code></a> to send reduction to a single rank.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allreduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allreduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allreduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L599-L618">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allreduce" href="#MPI.Allreduce"><code>MPI.Allreduce</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Allreduce(sendbuf, op, comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>, returning the result in the <code>recvbuf</code> of all processes in the group.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will be a value of the same type.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allreduce!"><code>Allreduce!</code></a> for mutating or in-place operations.</li><li><a href="#MPI.Reduce!"><code>Reduce!</code></a>/<a href="#MPI.Reduce"><code>Reduce</code></a> to send reduction to a single rank.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allreduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allreduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allreduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L637-L652">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scan!" href="#MPI.Scan!"><code>MPI.Scan!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scan!(sendbuf, recvbuf, op, comm::Comm)
Scan!(sendrecvbuf, op, comm::Comm)</code></pre><p>Inclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i</code>.</p><p>If only a single buffer <code>sendrecvbuf</code> is provided, then operations will be performed in-place.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Scan"><code>Scan</code></a> to handle allocation of the output buffer</li><li><a href="#MPI.Exscan!"><code>Exscan!</code></a>/<a href="#MPI.Exscan"><code>Exscan</code></a> for exclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L661-L678">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scan" href="#MPI.Scan"><code>MPI.Scan</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Scan(sendbuf, op, comm::Comm)</code></pre><p>Inclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i</code>.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will also be a scalar of the same type.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Scan!"><code>Scan!</code></a> for mutating or in-place operations</li><li><a href="#MPI.Exscan!"><code>Exscan!</code></a>/<a href="#MPI.Exscan"><code>Exscan</code></a> for exclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L697-L713">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Exscan!" href="#MPI.Exscan!"><code>MPI.Exscan!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Exscan!(sendbuf, recvbuf, op, comm::Comm)
Exscan!(sendrecvbuf, op, comm::Comm)</code></pre><p>Exclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i-1</code>. The <code>recvbuf</code> on rank <code>0</code> is ignored, and the <code>recvbuf</code> on rank <code>1</code> will contain the contents of <code>sendbuf</code> on rank <code>0</code>.</p><p>If only a single <code>sendrecvbuf</code> is provided, then operations are performed in-place, and <code>buf</code> on rank 0 will remain unchanged.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Exscan"><code>Exscan</code></a> to handle allocation of the output buffer</li><li><a href="#MPI.Scan!"><code>Scan!</code></a>/<a href="#MPI.Scan"><code>Scan</code></a> for inclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Exscan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Exscan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Exscan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L721-L740">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Exscan" href="#MPI.Exscan"><code>MPI.Exscan</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Exscan(sendbuf, op, comm::Comm)</code></pre><p>Exclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i-1</code>. The <code>recvbuf</code> on rank <code>0</code> is undefined, and the <code>recvbuf</code> on rank <code>1</code> will contain the contents of <code>sendbuf</code> on rank <code>0</code>.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Exscan!"><code>Exscan!</code></a> for mutating and in-place operations</li><li><a href="#MPI.Scan!"><code>Scan!</code></a>/<a href="#MPI.Scan"><code>Scan</code></a> for inclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/bb354a340349ea5cf173c71cb6cc8563b9f16800/src/collective.jl#L760-L775">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../pointtopoint/">« Point-to-point communication</a><a class="docs-footer-nextpage" href="../onesided/">One-sided communication »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 12 November 2020 23:13">Thursday 12 November 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
