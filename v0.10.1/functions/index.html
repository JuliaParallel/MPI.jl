<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Function reference · MPI.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MPI.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">MPI.jl</a></li><li><a class="toctext" href="../installation/">Installation</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../examples/01-hello/">Hello world</a></li><li><a class="toctext" href="../examples/02-broadcast/">Broadcast</a></li><li><a class="toctext" href="../examples/03-reduce/">Reduce</a></li><li><a class="toctext" href="../examples/04-sendrecv/">Send/receive</a></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../environment/">Environment</a></li><li><a class="toctext" href="../comm/">Communicators</a></li></ul></li><li class="current"><a class="toctext" href>Function reference</a><ul class="internal"><li><a class="toctext" href="#Datatype-functions-1">Datatype functions</a></li><li><a class="toctext" href="#Point-to-point-communication-1">Point-to-point communication</a></li><li><a class="toctext" href="#Collective-communication-1">Collective communication</a></li><li><a class="toctext" href="#One-sided-communication-1">One-sided communication</a></li><li class="toplevel"><a class="toctext" href="#Info-objects-1">Info objects</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Function reference</a></li></ul><a class="edit-page" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/functions.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Function reference</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Function-reference-1" href="#Function-reference-1">Function reference</a></h1><p>The following functions are currently wrapped, with the convention: <code>MPI_Fun =&gt; MPI.Fun</code></p><p>Constants like <code>MPI_SUM</code> are wrapped as <code>MPI.SUM</code>.   Note also that arbitrary Julia functions <code>f(x,y)</code> can be passed as reduction operations to the MPI <code>Allreduce</code> and <code>Reduce</code> functions.</p><h2><a class="nav-anchor" id="Datatype-functions-1" href="#Datatype-functions-1">Datatype functions</a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_address</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_address.3.php"><code>MPI_Get_address</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.mpitype</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_create_struct.3.php"><code>MPI_Type_create_struct</code></a>/<a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_commit.3.php"><code>MPI_Type_commit</code></a></td></tr></table><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p><code>mpitype</code> is not strictly a wrapper for <code>MPI_Type_create_struct</code> and <code>MPI_Type_commit</code>, it also is an accessor for previously created types.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get_address</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.mpitype</code>. Check Documenter&#39;s build log for details.</p></div></div><h2><a class="nav-anchor" id="Point-to-point-communication-1" href="#Point-to-point-communication-1">Point-to-point communication</a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Cancel!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Cancel.3.php"><code>MPI_Cancel</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_count</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Get_count.3.php"><code>MPI_Get_count</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Iprobe</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Iprobe.3.php"><code>MPI_Iprobe</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Irecv!"><code>MPI.Irecv!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Irecv.3.php"><code>MPI_Irecv</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Isend"><code>MPI.Isend</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Isend.3.php"><code>MPI_Isend</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Probe</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Probe.3.php"><code>MPI_Probe</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Recv!"><code>MPI.Recv!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Recv.3.php"><code>MPI_Recv</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Send"><code>MPI.Send</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Send.3.php"><code>MPI_Send</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Test!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Test.3.php"><code>MPI_Test</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Testall!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testall.3.php"><code>MPI_Testall</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Testany!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testany.3.php"><code>MPI_Testany</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Testsome!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Testsome.3.php"><code>MPI_Testsome</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Wait!"><code>MPI.Wait!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Wait.3.php"><code>MPI_Wait</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Waitall!"><code>MPI.Waitall!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitall.3.php"><code>MPI_Waitall</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Waitany!"><code>MPI.Waitany!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitany.3.php"><code>MPI_Waitany</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Waitsome!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Waitsome.3.php"><code>MPI_Waitsome</code></a></td></tr></table><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Cancel!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get_count</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Iprobe</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Irecv!" href="#MPI.Irecv!"><code>MPI.Irecv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Irecv!(buf::MPIBuffertype{T}, count::Integer, datatype::Datatype,
       src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking receive of up to <code>count</code> elements of type <code>datatype</code> into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the communication <code>Request</code> for the nonblocking receive.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L285-L293">source</a><div><div><pre><code class="language-none">Irecv!(buf::MPIBuffertype{T}, count::Integer, src::Integer, tag::Integer,
       comm::Comm) where T</code></pre><p>Starts a nonblocking receive of up to <code>count</code> elements into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the communication <code>Request</code> for the nonblocking receive.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L306-L314">source</a><div><div><pre><code class="language-none">Irecv!(buf::Array{T}, src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking receive into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the communication <code>Request</code> for the nonblocking receive.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L320-L327">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Isend" href="#MPI.Isend"><code>MPI.Isend</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Isend(buf::MPIBuffertype{T}, count::Integer, datatype::Datatype,
      dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>count</code> elements of type <code>datatype</code> from <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L151-L159">source</a><div><div><pre><code class="language-none">Isend(buf::MPIBuffertype{T}, count::Integer, dest::Integer, tag::Integer,
      comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>count</code> elements of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L172-L180">source</a><div><div><pre><code class="language-none">Isend(buf::Array{T}, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L186-L193">source</a><div><div><pre><code class="language-none">Isend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Starts a nonblocking send of <code>obj</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code>.</p><p>Returns the commication <code>Request</code> for the nonblocking send.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L198-L205">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Probe</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Recv!" href="#MPI.Recv!"><code>MPI.Recv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Recv!(buf::MPIBuffertype{T}, count::Integer, datatype::Datatype,
      src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Completes a blocking receive of up to <code>count</code> elements of type <code>datatype</code> into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the <code>Status</code> of the receive</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L224-L232">source</a><div><div><pre><code class="language-none">Recv!(buf::MPIBuffertype{T}, count::Integer, src::Integer, tag::Integer,
      comm::Comm) where T</code></pre><p>Completes a blocking receive of up to <code>count</code> elements into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the <code>Status</code> of the receive</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L244-L252">source</a><div><div><pre><code class="language-none">Recv!(buf::Array{T}, src::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Completes a blocking receive into <code>buf</code> from MPI rank <code>src</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p><p>Returns the <code>Status</code> of the receive</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L259-L266">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Send" href="#MPI.Send"><code>MPI.Send</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Send(buf::MPIBuffertype{T}, count::Integer, datatype::Datatype,
     dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Complete a blocking send of <code>count</code> elements of type <code>datatype</code> from <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using the message tag <code>tag</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L91-L97">source</a><div><div><pre><code class="language-none">Send(buf::MPIBuffertype{T}, count::Integer, dest::Integer, tag::Integer,
     comm::Comm) where T</code></pre><p>Complete a blocking send of <code>count</code> elements of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L107-L113">source</a><div><div><pre><code class="language-none">Send(buf::AbstractArray{T}, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Complete a blocking send of <code>buf</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L119-L124">source</a><div><div><pre><code class="language-none">Send(obj::T, dest::Integer, tag::Integer, comm::Comm) where T</code></pre><p>Complete a blocking send of <code>obj</code> to MPI rank <code>dest</code> of communicator <code>comm</code> using with the message tag <code>tag</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L129-L134">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Test!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Testall!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Testany!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Testsome!</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Wait!" href="#MPI.Wait!"><code>MPI.Wait!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Wait!(req::Request)</code></pre><p>Wait on the request <code>req</code> to be complete. Returns the <code>Status</code> of the request.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L344-L348">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Waitall!" href="#MPI.Waitall!"><code>MPI.Waitall!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Waitall!(reqs::Vector{Request})</code></pre><p>Wait on all the requests in the array <code>reqs</code> to be complete. Returns an arrays of the all the requests statuses.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L373-L378">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Waitany!" href="#MPI.Waitany!"><code>MPI.Waitany!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Waitany!(reqs::Vector{Request})</code></pre><p>Wait on any the requests in the array <code>reqs</code> to be complete. Returns the index of the completed request and its <code>Status</code> as a tuple.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/pointtopoint.jl#L416-L421">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Waitsome!</code>. Check Documenter&#39;s build log for details.</p></div></div><h2><a class="nav-anchor" id="Collective-communication-1" href="#Collective-communication-1">Collective communication</a></h2><table><tr><th style="text-align: right">Non-Allocating Julia Function</th><th style="text-align: right">Allocating Julia Function</th><th style="text-align: right">Fortran Function</th><th style="text-align: right">Supports <code>MPI_IN_PLACE</code></th></tr><tr><td style="text-align: right"><a href="#MPI.Allgather!"><code>MPI.Allgather!</code></a></td><td style="text-align: right"><a href="#MPI.Allgather"><code>MPI.Allgather</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgather.3.php"><code>MPI_Allgather</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a></td><td style="text-align: right"><a href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgatherv.3.php"><code>MPI_Allgatherv</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a></td><td style="text-align: right"><a href="#MPI.Allreduce"><code>MPI.Allreduce</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allreduce.3.php"><code>MPI_Allreduce</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a></td><td style="text-align: right"><a href="#MPI.Alltoall"><code>MPI.Alltoall</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoall.3.php"><code>MPI_Alltoall</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a></td><td style="text-align: right"><a href="@ref"><code>MPI.Alltoallv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoallv.3.php"><code>MPI_Alltoallv</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right">–</td><td style="text-align: right"><a href="#MPI.Barrier"><code>MPI.Barrier</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Barrier.3.php"><code>MPI_Barrier</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right"><a href="#MPI.Bcast!"><code>MPI.Bcast!</code></a></td><td style="text-align: right"><a href="#MPI.Bcast!"><code>MPI.Bcast!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Bcast.3.php"><code>MPI_Bcast</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right">–</td><td style="text-align: right"><a href="@ref"><code>MPI.Exscan</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Exscan.3.php"><code>MPI_Exscan</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right"><a href="#MPI.Gather!"><code>MPI.Gather!</code></a></td><td style="text-align: right"><a href="#MPI.Gather"><code>MPI.Gather</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gather.3.php"><code>MPI_Gather</code></a></td><td style="text-align: right"><a href="@ref"><code>Gather_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a></td><td style="text-align: right"><a href="#MPI.Gatherv"><code>MPI.Gatherv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gatherv.3.php"><code>MPI_Gatherv</code></a></td><td style="text-align: right"><a href="@ref"><code>Gatherv_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Reduce!"><code>MPI.Reduce!</code></a></td><td style="text-align: right"><a href="#MPI.Reduce"><code>MPI.Reduce</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Reduce.3.php"><code>MPI_Reduce</code></a></td><td style="text-align: right"><a href="@ref"><code>Reduce_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Scan</code></a></td><td style="text-align: right"><a href="@ref"><code>MPI.Scan</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scan.3.php"><code>MPI_Scan</code></a></td><td style="text-align: right">missing</td></tr><tr><td style="text-align: right"><a href="#MPI.Scatter!"><code>MPI.Scatter!</code></a></td><td style="text-align: right"><a href="#MPI.Scatter"><code>MPI.Scatter</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatter.3.php"><code>MPI_Scatter</code></a></td><td style="text-align: right"><a href="@ref"><code>Scatter_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a></td><td style="text-align: right"><a href="#MPI.Scatterv"><code>MPI.Scatterv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatterv.3.php"><code>MPI_Scatterv</code></a></td><td style="text-align: right"><a href="@ref"><code>Scatterv_in_place!</code></a></td></tr></table><p>The non-allocating Julia functions map directly to the corresponding MPI operations, after asserting that the size of the output buffer is sufficient to store the result.</p><p>The allocating Julia functions allocate an output buffer and then call the non-allocating method.</p><p>All-to-all collective communications support in place operations by passing <code>MPI.IN_PLACE</code> with the same syntax documented by MPI. One-to-All communications support it by calling the function <code>*_in_place!</code>, calls the MPI functions with the right syntax on root and non root process.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgather!" href="#MPI.Allgather!"><code>MPI.Allgather!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgather!(sendbuf, recvbuf, count, comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order into <code>recvbuf</code>.</p><p>If <code>sendbuf==MPI.IN_PLACE</code> the input data is assumed to be in the area of <code>recvbuf</code> where the process would receive it&#39;s own contribution.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L487-L497">source</a><div><div><pre><code class="language-none">Allgather!(buf, count, comm)</code></pre><p>Equivalent to <code>Allgather!(MPI.IN_PLACE, buf, count, comm)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L512-L516">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgather" href="#MPI.Allgather"><code>MPI.Allgather</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgather(sendbuf[, count=length(sendbuf)], comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order allocating the output buffer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L522-L528">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgatherv!" href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgatherv!(sendbuf, recvbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process stores the received data in rank order in the buffer <code>recvbuf</code>.</p><p>if <code>sendbuf==MPI.IN_PLACE</code> every process takes the data to be sent is taken from the interval of <code>recvbuf</code> where it would store it&#39;s own data.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L627-L636">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgatherv" href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgatherv(sendbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process allocates an output buffer and stores the received data in rank order.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L653-L659">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allreduce!" href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the buffer <code>sendbuf</code> storing the result in the <code>recvbuf</code> of all processes in the group.</p><p>All-reduce is equivalent to a <a href="#MPI.Reduce!"><code>Reduce!</code></a> operation followed by a <a href="#MPI.Bcast!"><code>Bcast!</code></a>, but can lead to better performance.</p><p>If <code>sendbuf==MPI.IN_PLACE</code> the data is read from <code>recvbuf</code> and then overwritten with the results.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Allreduce"><code>Allreduce</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L173-L187">source</a><div><div><pre><code class="language-none">Allreduce!(buf, op, comm)</code></pre><p>Performs <code>op</code> reduction in place on the buffer <code>sendbuf</code>, overwriting it with the results on all the processes in the group.</p><p>Equivalent to calling <code>Allreduce!(MPI.IN_PLACE, buf, op, comm)</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L211-L218">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allreduce" href="#MPI.Allreduce"><code>MPI.Allreduce</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allreduce(sendbuf, op, comm)</code></pre><p>Performs <code>op</code> reduction on the buffer <code>sendbuf</code>, allocating and returning the output buffer in all processes of the group.</p><p>To specify the output buffer or perform the operation in pace, see <a href="#MPI.Allreduce!"><code>Allreduce!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L223-L230">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoall!" href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoall!(sendbuf, recvbuf, count, comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk of the buffer <code>recvbuf</code>.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p>If <code>sendbuf==MPI.IN_PLACE</code>, data is sent from the <code>recvbuf</code> and then overwritten.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L666-L684">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoall" href="#MPI.Alltoall"><code>MPI.Alltoall</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoall(sendbuf, count, comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process allocates the output buffer and stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L698-L713">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoallv!" href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoallv!(sendbuf::T, recvbuf::T, scounts, rcounts, comm)</code></pre><p><code>MPI.IN_PLACE</code> is not supported for this operation.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L720-L724">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Alltoallv</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Barrier" href="#MPI.Barrier"><code>MPI.Barrier</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Barrier(comm::Comm)</code></pre><p>Blocks until <code>comm</code> is synchronized.</p><p>If <code>comm</code> is an intracommunicator, then it blocks until all members of the group have called it.</p><p>If <code>comm</code> is an intercommunicator, then it blocks until all members of the other group have called it.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L4-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Bcast!" href="#MPI.Bcast!"><code>MPI.Bcast!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Bcast!(buf[, count=length(buf)], root, comm::Comm)</code></pre><p>Broadcast the first <code>count</code> elements of the buffer <code>buf</code> from <code>root</code> to all processes.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L18-L22">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Bcast!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Exscan</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather!" href="#MPI.Gather!"><code>MPI.Gather!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather!(sendbuf, recvbuf, count, root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>recvbuf</code>.</p><p><code>count</code> should be the same for all processes. If the number of elements varies between processes, use <a href="#MPI.Gatherv!"><code>Gatherv!</code></a> instead.</p><p>To perform the reduction in place refer to <a href="#MPI.Gather_in_place!"><code>Gather_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L399-L410">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather" href="#MPI.Gather"><code>MPI.Gather</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather(sendbuf[, count=length(sendbuf)], root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L428-L434">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather_in_place!" href="#MPI.Gather_in_place!"><code>MPI.Gather_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather_in_place!(buf, count, root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>buf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>buf</code>, sending no data to itself.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gather!(MPI.IN_PLACE, buf, count, root, comm)
else
    Gather!(buf, C_NULL, count, root, comm)
end</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L455-L470">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv!" href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> stores elements in rank order in the buffer <code>recvbuf</code>.</p><p>To perform the reduction in place refer to <a href="#MPI.Gatherv_in_place!"><code>Gatherv_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L543-L551">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv" href="#MPI.Gatherv"><code>MPI.Gatherv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv(sendbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L569-L575">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv_in_place!" href="#MPI.Gatherv_in_place!"><code>MPI.Gatherv_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv_in_place!(buf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>buf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gatherv!(MPI.IN_PLACE, buf, counts, root, comm)
else
    Gatherv!(buf, C_NULL, counts, root, comm)
end</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L590-L605">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce!" href="#MPI.Reduce!"><code>MPI.Reduce!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Reduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the  buffer <code>sendbuf</code> and stores the result in <code>recvbuf</code> on the process of rank <code>root</code>.</p><p>On non-root processes <code>recvbuf</code> is ignored.</p><p>To perform the reduction in place, see <a href="#MPI.Reduce_in_place!"><code>Reduce_in_place!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Reduce"><code>Reduce</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L64-L75">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce" href="#MPI.Reduce"><code>MPI.Reduce</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Reduce(sendbuf, count, op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the buffer <code>sendbuf</code> and stores the result in an output buffer allocated on the process of rank <code>root</code>. An empty array will be returned on all other processes.</p><p>To specify the output buffer, see <a href="#MPI.Reduce!"><code>Reduce!</code></a>.</p><p>To perform the reduction in place, see <a href="#MPI.Reduce_in_place!"><code>Reduce_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L99-L109">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce_in_place!" href="#MPI.Reduce_in_place!"><code>MPI.Reduce_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Reduce_in_place!(buf, count, op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the buffer <code>buf</code> and stores the result on <code>buf</code> of the <code>root</code> process in the group.</p><p>This is equivalent to calling</p><pre><code class="language-julia">if root == MPI.Comm_rank(comm)
    Reduce!(MPI.IN_PLACE, buf, count, root, comm)
else
    Reduce!(buf, C_NULL, count, root, comm)
end</code></pre><p>To handle allocation of the output buffer, see <a href="#MPI.Reduce"><code>Reduce</code></a>.</p><p>To specify a separate output buffer, see <a href="#MPI.Reduce!"><code>Reduce!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L132-L150">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Scan</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Scan</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter!" href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter!(sendbuf, recvbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p><code>count</code> should be the same for all processes. If the number of elements varies between processes, use <a href="#MPI.Scatter!"><code>Scatter!</code></a> instead.</p><p>To perform the reduction in place, see <a href="#MPI.Scatter_in_place!"><code>Scatter_in_place!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Scatter"><code>Scatter</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L245-L258">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter" href="#MPI.Scatter"><code>MPI.Scatter</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter(sendbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j, allocating the output buffer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L313-L318">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter_in_place!" href="#MPI.Scatter_in_place!"><code>MPI.Scatter_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter_in_place!(buf, count, root, comm)</code></pre><p>Splits the buffer <code>buf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j. No data is sent to the <code>root</code> process.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatter!(buf, MPI.IN_PLACE, count, root, comm)
else
    Scatter!(C_NULL, buf, count, root, comm)
end</code></pre><p>To specify a separate output buffer, see <a href="#MPI.Scatter!"><code>Scatter!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Scatter"><code>Scatter</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L277-L296">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv!" href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p>To perform the reduction in place refer to <a href="#MPI.Scatterv_in_place!"><code>Scatterv_in_place!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L325-L333">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv" href="#MPI.Scatterv"><code>MPI.Scatterv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv(sendbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j, which allocates the output buffer</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L350-L356">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv_in_place!" href="#MPI.Scatterv_in_place!"><code>MPI.Scatterv_in_place!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv_in_place(buf, counts, root, comm)</code></pre><p>Splits the buffer <code>buf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>buf</code> buffer, which must be of length at least <code>count</code>. The <code>root</code> process sends nothing to itself.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatterv!(buf, MPI.IN_PLACE, counts, root, comm)
else
    Scatterv!(C_NULL, buf, counts, root, comm)
end</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/collective.jl#L363-L379">source</a></section><h2><a class="nav-anchor" id="One-sided-communication-1" href="#One-sided-communication-1">One-sided communication</a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="#MPI.Win_create"><code>MPI.Win_create</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create.3.php"><code>MPI_Win_create</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Win_create_dynamic"><code>MPI.Win_create_dynamic</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create_dynamic.3.php"><code>MPI_Win_create_dynamic</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Win_allocate_shared"><code>MPI.Win_allocate_shared</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_allocate_shared.3.php"><code>MPI_Win_allocate_shared</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_shared_query</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_shared_query.3.php"><code>MPI_Win_shared_query</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_attach</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_attach.3.php"><code>MPI_Win_attach</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_detach</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_detach.3.php"><code>MPI_Win_detach</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_fence</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_fence.3.php"><code>MPI_Win_fence</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_flush</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_flush.3.php"><code>MPI_Win_flush</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_free</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_free.3.php"><code>MPI_Win_free</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_sync</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_sync.3.php"><code>MPI_Win_sync</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_lock</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_lock.3.php"><code>MPI_Win_lock</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_unlock</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_unlock.3.php"><code>MPI_Win_unlock</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get.3.php"><code>MPI_Get</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Put</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Put.3.php"><code>MPI_Put</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Fetch_and_op</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Fetch_and_op.3.php"><code>MPI_Fetch_and_op</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Accumulate</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Accumulate.3.php"><code>MPI_Accumulate</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_accumulate</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_accumulate.3.php"><code>MPI_Get_accumulate</code></a></td></tr></table><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Win_create" href="#MPI.Win_create"><code>MPI.Win_create</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">MPI.Win_create(base::Array, comm::Comm; infokws...)</code></pre><p>Create a window over the array <code>base</code>, returning a <code>Win</code> object used by these processes to perform RMA operations</p><p>This is a collective call over <code>comm</code>.</p><p><code>infokws</code> are info keys providing optimization hints.</p><p><a href="@ref"><code>MPI.free</code></a> should be called on the <code>Win</code> object once operations have been completed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/window.jl#L13-L23">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Win_create_dynamic" href="#MPI.Win_create_dynamic"><code>MPI.Win_create_dynamic</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">MPI.Win_create_dynamic(comm::Comm; infokws...)</code></pre><p>Create a dynamic window returning a <code>Win</code> object used by these processes to perform RMA operations</p><p>This is a collective call over <code>comm</code>.</p><p><code>infokws</code> are info keys providing optimization hints.</p><p><a href="@ref"><code>MPI.free</code></a> should be called on the <code>Win</code> object once operations have been completed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/window.jl#L36-L46">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Win_allocate_shared" href="#MPI.Win_allocate_shared"><code>MPI.Win_allocate_shared</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">(win, ptr) = MPI.Win_allocate_shared(T, len, comm::Comm; infokws...)</code></pre><p>Create and allocate a shared memory window for objects of type <code>T</code> of length <code>len</code>, returning a <code>Win</code> and a <code>Ptr{T}</code> object used by these processes to perform RMA operations</p><p>This is a collective call over <code>comm</code>.</p><p><code>infokws</code> are info keys providing optimization hints.</p><p><a href="@ref"><code>MPI.free</code></a> should be called on the <code>Win</code> object once operations have been completed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/window.jl#L60-L71">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_shared_query</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_attach</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_detach</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_fence</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_flush</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_free</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_sync</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_lock</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Win_unlock</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Put</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Fetch_and_op</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Accumulate</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MPI.Get_accumulate</code>. Check Documenter&#39;s build log for details.</p></div></div><h1><a class="nav-anchor" id="Info-objects-1" href="#Info-objects-1">Info objects</a></h1><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Info" href="#MPI.Info"><code>MPI.Info</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Info &lt;: AbstractDict{Symbol,String}</code></pre><p><code>MPI.Info</code> objects store key-value pairs, and are typically used for passing optional arguments to MPI functions.</p><p><strong>Usage</strong></p><p>These will typically be hidden from user-facing APIs by splatting keywords, e.g.</p><pre><code class="language-julia">function f(args...; kwargs...)
    info = Info(kwargs...)
    # pass `info` object to `ccall`
end</code></pre><p>For manual usage, <code>Info</code> objects act like Julia <code>Dict</code> objects:</p><pre><code class="language-julia">info = Info(init=true) # keyword argument is required
info[key] = value
x = info[key]
delete!(info, key)</code></pre><p>If <code>init=false</code> is used in the costructor (the default), a &quot;null&quot; <code>Info</code> object will be returned: no keys can be added to such an object.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/3af96bcefcb0aec559d4652fd75e1bc3fcfe99da/base/#L0-L26">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.infoval" href="#MPI.infoval"><code>MPI.infoval</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">infoval(x)</code></pre><p>Convert Julia object <code>x</code> to a string representation for storing in an <a href="#MPI.Info"><code>Info</code></a> object.</p><p>The MPI specification allows passing strings, Boolean values, integers, and lists.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/f471b62fea9ff46411f83556b95eac51023a9667/src/info.jl#L60-L66">source</a></section><footer><hr/><a class="previous" href="../comm/"><span class="direction">Previous</span><span class="title">Communicators</span></a></footer></article></body></html>
