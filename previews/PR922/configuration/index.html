<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Configuration · MPI.jl</title><meta name="title" content="Configuration · MPI.jl"/><meta property="og:title" content="Configuration · MPI.jl"/><meta property="twitter:title" content="Configuration · MPI.jl"/><meta name="description" content="Documentation for MPI.jl."/><meta property="og:description" content="Documentation for MPI.jl."/><meta property="twitter:description" content="Documentation for MPI.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MPI.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MPI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">MPI.jl</a></li><li class="is-active"><a class="tocitem" href>Configuration</a><ul class="internal"><li><a class="tocitem" href="#using_jll_mpi"><span>Using a jll MPI backend</span></a></li><li><a class="tocitem" href="#using_mpitrampoline"><span>Using MPItrampoline</span></a></li><li><a class="tocitem" href="#using_system_mpi"><span>Using a system-provided MPI backend</span></a></li><li><a class="tocitem" href="#configure_jll_binary"><span>Using an alternative JLL-provided MPI library</span></a></li><li><a class="tocitem" href="#Configuration-of-the-MPI.jl-testsuite"><span>Configuration of the MPI.jl testsuite</span></a></li><li><a class="tocitem" href="#Migration-from-MPI.jl-v0.19-or-earlier"><span>Migration from MPI.jl v0.19 or earlier</span></a></li></ul></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><a class="tocitem" href="../external/">External libraries and packages</a></li><li><a class="tocitem" href="../knownissues/">Known issues</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/01-hello/">Hello world</a></li><li><a class="tocitem" href="../examples/02-broadcast/">Broadcast</a></li><li><a class="tocitem" href="../examples/03-reduce/">Reduce</a></li><li><a class="tocitem" href="../examples/04-sendrecv/">Send/receive</a></li><li><a class="tocitem" href="../examples/05-job_schedule/">Job Scheduling</a></li><li><a class="tocitem" href="../examples/06-scatterv/">Scatterv and Gatherv</a></li><li><a class="tocitem" href="../examples/07-rma_active/">Active RMA</a></li><li><a class="tocitem" href="../examples/08-rma_passive/">Passive RMA</a></li><li><a class="tocitem" href="../examples/09-graph_communication/">Graph Communication</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../reference/mpipreferences/">MPIPreferences.jl</a></li><li><a class="tocitem" href="../reference/library/">Library information</a></li><li><a class="tocitem" href="../reference/environment/">Environment</a></li><li><a class="tocitem" href="../reference/misc/">Miscellanea</a></li><li><a class="tocitem" href="../reference/comm/">Communicators</a></li><li><a class="tocitem" href="../reference/buffers/">Buffers</a></li><li><a class="tocitem" href="../reference/pointtopoint/">Point-to-point communication</a></li><li><a class="tocitem" href="../reference/collective/">Collective communication</a></li><li><a class="tocitem" href="../reference/onesided/">One-sided communication</a></li><li><a class="tocitem" href="../reference/topology/">Topology</a></li><li><a class="tocitem" href="../reference/io/">I/O</a></li><li><a class="tocitem" href="../reference/advanced/">Advanced</a></li><li><a class="tocitem" href="../reference/api/">Low-level API</a></li></ul></li><li><a class="tocitem" href="../refindex/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Configuration</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Configuration</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaParallel/MPI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/configuration.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Configuration"><a class="docs-heading-anchor" href="#Configuration">Configuration</a><a id="Configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration" title="Permalink"></a></h1><p>By default, MPI.jl will download and link against the following MPI implementations:</p><ul><li><a href="https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi">Microsoft MPI</a> on Windows</li><li><a href="https://www.mpich.org/">MPICH</a> on all other platforms</li></ul><p>This is suitable for most single-node use cases, but for larger systems, such as HPC clusters or multi-GPU machines, you will probably want to configure against a specialized MPI implementation in order to exploit features such as fast network interfaces and CUDA-aware or ROCm-aware MPI interfaces.</p><p>There are three ways to point Julia to an MPI implementation:</p><ul><li><em>jll binaries</em> (preferred). A jll binary is a package that is built via <a href="https://docs.binarybuilder.org/stable/">BinaryBuilder</a> and can be downloaded for all architectures. Almost all external libraries (such as CUDA, HDF5, libpng, OpenSSL, ...) are provided as jll binaries because they are quick and easy to download and work out of the box.</li><li><em>system binaries</em> (for experts only). A system binary is a shared library that you need to configure and build yourself. This is tricky even for experts. The problem is not just build the shared library, in addition one has to be careful to ensure that the shared library is ABI compatible with all other package you are using. (More details on this below.)</li></ul><p>If you want to use an MPI implementation on your system, then you can either use it as <em>system binary</em> (this is for experts only), or you can use MPItrampoline as <em>jll binary</em>, and point MPItrampoline to the other MPI library. This is preferred since it automatically ensures ABI compatibility.</p><p>The MPIPreferences.jl package allows the user to choose which MPI implementation to use in MPI.jl. It uses <a href="https://github.com/JuliaPackaging/Preferences.jl">Preferences.jl</a> to configure the MPI backend for each project separately. This provides a single source of truth that can be used for JLL packages (Julia packages providing C libraries) that link against MPI. It can be installed by</p><pre><code class="language-sh hljs">julia --project -e &#39;using Pkg; Pkg.add(&quot;MPIPreferences&quot;)&#39;</code></pre><div class="admonition is-info" id="Note-b6968b68ae87ade2"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-b6968b68ae87ade2" title="Permalink"></a></header><div class="admonition-body"><p>The way MPI.jl is configured has changed with MPI.jl v0.20. See <a href="#Migration-from-MPI.jl-v0.19-or-earlier">Migration from MPI.jl v0.19 or earlier</a> for more information on how to migrate your configuration from earlier MPI.jl versions.</p></div></div><h2 id="using_jll_mpi"><a class="docs-heading-anchor" href="#using_jll_mpi">Using a jll MPI backend</a><a id="using_jll_mpi-1"></a><a class="docs-heading-anchor-permalink" href="#using_jll_mpi" title="Permalink"></a></h2><p>This is the recommended way to use MPI.jl. By default, MPI.jl will use <code>MPICH_jll</code> as jll MPI backend.</p><p>You can select from four different jll MPI binaries:</p><ul><li><a href="http://www.mpich.org/"><code>MPICH_jll</code></a>,  the default</li><li><a href="https://www.open-mpi.org/"><code>OpenMPI_jll</code></a>, an alternative to MPICH</li><li><a href="https://github.com/eschnett/MPItrampoline"><code>MPItrampoline_jll</code></a>, a forwarding MPI implementation that uses another MPI implementation</li><li><a href="https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi"><code>MicrosoftMPI_jll</code></a> for Windows</li></ul><p>For example, to switch to OpenMPI, you would first use MPIPreferenes.jl to switch:</p><pre><code class="language-sh hljs">julia&gt; using MPIPreferences

julia&gt; MPIPreferences.use_jll_binary(&quot;OpenMPI_jll&quot;)
┌ Info: MPIPreferences changed
└   binary = &quot;OpenMPI_jll&quot;</code></pre><p>Next you need to restart Julia (!) and re-instantiate your packages:</p><pre><code class="language-sh hljs">julia&gt; using Pkg

julia&gt; Pkg.instantiate()</code></pre><p>This is necessary because other jll packages (e.g. <code>HDF5_jll</code>) may depend on MPI, and a different build of <code>HDF5_jll</code> needs to be installed for ABI compatibility.</p><p>To switch back to <code>MPICH_jll</code>, repeat the steps above with <code>&quot;MPICH_JLL&quot;</code> as argument to <code>MPIPreferences.use_jll_binary</code>. Don&#39;t forget to restart Julia and re-instantiate your packages again.</p><h2 id="using_mpitrampoline"><a class="docs-heading-anchor" href="#using_mpitrampoline">Using MPItrampoline</a><a id="using_mpitrampoline-1"></a><a class="docs-heading-anchor-permalink" href="#using_mpitrampoline" title="Permalink"></a></h2><p>MPItrampoline is an easier and safer way to use external MPI implementations. MPItrampoline defines an ABI for MPI calls (similar to the way <a href="https://github.com/JuliaLinearAlgebra/libblastrampoline"><code>libblastrampoline</code></a> works for linear algebra), which allows switching between different MPI backends without recompiling code. Unfortunately this ABI is not yet standardized (but it might be in <a href="https://www.mpi-forum.org">MPI 5</a>!), and therefore you have to install a small wrapper library for every MPI implementation that you want to use via MPItrampoline.</p><p>The general design is:</p><ul><li>There is a pre-installed MPI implementation on your system (e.g. supporting CUDA or Slingshot)</li><li>You (or a system administrator) installs <a href="https://github.com/eschnett/MPIwrapper"><code>MPIwrapper</code></a>, which implements the MPI ABI</li></ul><p>On the Julia side, the following happens:</p><ul><li>MPItrampoline provides the regular MPI functionality, internally calling out via the MPI ABI to any MPI implementation</li><li>Other packages (ADIOS2, HDF5, or MPI.jl) can always rely on MPItrampoline</li><li>At run time (when starting Julia), you choose which MPI implementation you use by setting an environment variable</li></ul><p>The documentation for <a href="https://github.com/eschnett/MPItrampoline">MPItrampoline</a> describes how to install MPIwrapper. This is possibly as simple as</p><pre><code class="language-sh hljs">cmake -B build -DMPIEXEC_EXECUTABLE=mpiexec -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_INSTALL_PREFIX=$HOME/mpiwrapper
cmake --build build
cmake --install build</code></pre><p>but nothing is ever simple on an HPC system. It might be necessary to load certain modules, or to specify more cmake MPI configuration options.</p><p>Define an environment variable to select this MPI implemenation:</p><pre><code class="language-sh hljs">export MPITRAMPOLINE_LIB=&quot;$HOME/mpiwrapper/lib/libmpiwrapper.so&quot;</code></pre><p>pointing to the shared library provided by the MPIwrapper you just built. (If you do not set this environment variable the MPItrampoline will fall back onto a built-in MPICH. That is, in principle MPI.jl could switch over to always using <code>MPItrampoline_jll</code>!)</p><p>Next you switch MPI.jl over to using <code>MPItrampoline_jll</code> as jll binary:</p><pre><code class="language-sh hljs">julia&gt; using MPIPreferences

julia&gt; MPIPreferences.use_jll_binary(&quot;MPItrampoline_jll&quot;)
┌ Info: MPIPreferences changed
└   binary = &quot;MPItrampoline_jll&quot;</code></pre><p>and then restart Julia (see above) and re-instantiate your packages:</p><pre><code class="language-sh hljs">julia&gt; using Pkg

julia&gt; Pkg.instantiate()</code></pre><p>You are ready to test this out:</p><pre><code class="language-sh hljs">julia&gt; using MPI

julia&gt; MPI.Get_library_version()
&quot;MPIwrapper 2.11.0, using MPIABI 2.10.0, wrapping:\nMPICH Version:      4.2.1\nMPICH Release date: Wed Apr 17 15:30:02 CDT 2024\nMPICH ABI:          16:1:4\nMPICH Device:       ch3:nemesis\nMPICH configure:    --build=x86_64-linux-musl --host=x86_64-apple-darwin14 --disable-dependency-tracking --disable-doc --enable-shared=no --enable-static=yes --enable-threads=multiple --with-device=ch3 --prefix=/workspace/destdir/lib/mpich --enable-two-level-namespace\nMPICH CC:           cc -fPIC -DPIC  -fno-common  -O2\nMPICH CXX:          c++ -fPIC -DPIC  -O2\nMPICH F77:          gfortran -fPIC -DPIC  -O2\nMPICH FC:           gfortran -fPIC -DPIC  -O2\nMPICH features:     \n&quot;</code></pre><h2 id="using_system_mpi"><a class="docs-heading-anchor" href="#using_system_mpi">Using a system-provided MPI backend</a><a id="using_system_mpi-1"></a><a class="docs-heading-anchor-permalink" href="#using_system_mpi" title="Permalink"></a></h2><p>This is an alternative way to using an external MPI implementation that does not rely on MPItrampoline.</p><h3 id="Requirements"><a class="docs-heading-anchor" href="#Requirements">Requirements</a><a id="Requirements-1"></a><a class="docs-heading-anchor-permalink" href="#Requirements" title="Permalink"></a></h3><p>MPI.jl requires a shared library installation of a C MPI library, supporting the MPI 3.0 standard or later. The following MPI implementations should work out-of-the-box with MPI.jl:</p><ul><li><a href="https://www.open-mpi.org/">Open MPI</a></li><li><a href="https://www.mpich.org/">MPICH</a> (v3.1 or later)</li><li><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html">Intel MPI</a></li><li><a href="https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi">Microsoft MPI</a></li><li><a href="https://www.ibm.com/products/spectrum-mpi">IBM Spectrum MPI</a></li><li><a href="https://mvapich.cse.ohio-state.edu/">MVAPICH</a></li><li><a href="https://docs.nersc.gov/development/compilers/wrappers/">Cray MPICH</a></li><li><a href="https://www.fujitsu.com/global/about/resources/publications/technicalreview/2020-03/article07.html#cap-03">Fujitsu MPI</a></li><li><a href="https://support.hpe.com/hpesc/public/docDisplay?docLocale=en_US&amp;docId=a00105727en_us">HPE MPT/HMPT</a></li></ul><h3 id="configure_system_binary"><a class="docs-heading-anchor" href="#configure_system_binary">Configuration</a><a id="configure_system_binary-1"></a><a class="docs-heading-anchor-permalink" href="#configure_system_binary" title="Permalink"></a></h3><p>Run <code>MPIPreferences.use_system_binary()</code>. This will attempt to locate and to identify any available MPI implementation, and create a file called <code>LocalPreferences.toml</code> adjacent to the current <code>Project.toml</code>.</p><pre><code class="language-sh hljs">julia --project -e &#39;using MPIPreferences; MPIPreferences.use_system_binary()&#39;</code></pre><p>If the implementation is changed, you will need to call this function again. See the <a href="../reference/mpipreferences/#MPIPreferences.use_system_binary"><code>MPIPreferences.use_system_binary</code></a> documentation for specific options.</p><div class="admonition is-info" id="Note-ca379061a86fa6d6"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-ca379061a86fa6d6" title="Permalink"></a></header><div class="admonition-body"><p>You can copy <code>LocalPreferences.toml</code> to a different project folder, but you must list <code>MPIPreferences</code> in the <code>[extras]</code> or <code>[deps]</code> section of the <code>Project.toml</code> for the settings to take effect.</p></div></div><div class="admonition is-info" id="Note-d7e2a2ddd400ca73"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d7e2a2ddd400ca73" title="Permalink"></a></header><div class="admonition-body"><p>Due to a bug in Julia (until <code>v1.6.5</code> and <code>v1.7.1</code>), getting preferences from transitive dependencies is broken (<a href="https://github.com/JuliaPackaging/Preferences.jl/issues/24">Preferences.jl#24</a>). To fix this update your version of Julia, or add <code>MPIPreferences</code> as a direct dependency to your project.</p></div></div><h3 id="Notes-to-HPC-cluster-administrators"><a class="docs-heading-anchor" href="#Notes-to-HPC-cluster-administrators">Notes to HPC cluster administrators</a><a id="Notes-to-HPC-cluster-administrators-1"></a><a class="docs-heading-anchor-permalink" href="#Notes-to-HPC-cluster-administrators" title="Permalink"></a></h3><p>Preferences are merged across the Julia load path, such that it is feasible to provide a module file that appends a path to <code>JULIA_LOAD_PATH</code> variable that contains system-wide preferences. The steps are as follows:</p><ol><li><p>Run <a href="../reference/mpipreferences/#MPIPreferences.use_system_binary"><code>MPIPreferences.use_system_binary()</code></a>, which will generate a file <code>LocalPreferences.toml</code> containing something like the following:</p><pre><code class="language-toml hljs">[MPIPreferences]
_format = &quot;1.0&quot;
abi = &quot;OpenMPI&quot;
binary = &quot;system&quot;
libmpi = &quot;/software/mpi/lib/libmpi.so&quot;
mpiexec = &quot;/software/mpi/bin/mpiexec&quot;</code></pre></li><li><p>Create a file called <code>Project.toml</code> or <code>JuliaProject.toml</code> in a central location (for example <code>/software/mpi/julia</code>, or in the same directory as the MPI module file), and add the following contents:</p><pre><code class="language-toml hljs">[extras]
MPIPreferences = &quot;3da0fdf6-3ccc-4f1b-acd9-58baa6c99267&quot;

[preferences.MPIPreferences]
_format = &quot;1.0&quot;
abi = &quot;OpenMPI&quot;
binary = &quot;system&quot;
libmpi = &quot;/software/mpi/lib/libmpi.so&quot;
mpiexec = &quot;/software/mpi/bin/mpiexec&quot;</code></pre><p>updating the contents of the <code>[preferences.MPIPreferences]</code> section match those of the <code>[MPIPreferences]</code> in <code>LocalPreferences.toml</code>.</p></li><li><p>Append the directory containing the file to the <a href="https://docs.julialang.org/en/v1/manual/environment-variables/#JULIA_LOAD_PATH"><code>JULIA_LOAD_PATH</code></a> environment variable, with a colon (<code>:</code>) separator.</p><div class="admonition is-info" id="Note-704326058f29bedf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-704326058f29bedf" title="Permalink"></a></header><div class="admonition-body"><p>If this variable is <em>not</em> already set, it should be prefixed with a colon to ensure correct behavior of the Julia load path (e.g. <code>JULIA_LOAD_PATH=&quot;:/software/mpi/julia&quot;</code>)</p></div></div><p>If using environment modules, this can be achieved with</p><pre><code class="nohighlight hljs">append-path -d {} JULIA_LOAD_PATH :/software/mpi/julia</code></pre><p>or if using an older version of environment modules</p><pre><code class="nohighlight hljs">if { ![info exists ::env(JULIA_LOAD_PATH)] } {
    append-path JULIA_LOAD_PATH &quot;&quot;
}
append-path JULIA_LOAD_PATH /software/mpi/julia</code></pre><p>in the corresponding module file (preferably the module file for the MPI installation or for Julia).</p><p>The user can still provide differing MPI configurations for each Julia project that will take precedent by modifying the local <code>Project.toml</code> or by providing a <code>LocalPreferences.toml</code> file.</p></li></ol><h3 id="Notes-about-vendor-provided-MPI-backends"><a class="docs-heading-anchor" href="#Notes-about-vendor-provided-MPI-backends">Notes about vendor-provided MPI backends</a><a id="Notes-about-vendor-provided-MPI-backends-1"></a><a class="docs-heading-anchor-permalink" href="#Notes-about-vendor-provided-MPI-backends" title="Permalink"></a></h3><p><code>MPIPreferences</code> can load vendor-specific libraries and settings using the <code>vendor</code> parameter, eg <code>MPIPreferences.use_system_binary(mpiexec=&quot;srun&quot;, vendor=&quot;cray&quot;)</code> configures <code>MPIPreferences</code> for use on Cray systems with <code>srun</code>.</p><div class="admonition is-info" id="Note-e9633def61aa7a4a"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-e9633def61aa7a4a" title="Permalink"></a></header><div class="admonition-body"><p>Currently <code>vendor</code> only supports Cray systems.</p></div></div><p>This populates the <code>library_names</code>, <code>preloads</code>, <code>preloads_env_switch</code> and <code>cclibs</code> preferences. These are determined by parsing <code>cc --cray-print-opts=all</code> emitted from the Cray Compiler Wrappers. Therefore <code>use_system_binary</code> needs to be run on the target system, with the corresponding <code>PrgEnv</code> loaded.</p><p>The function of these settings are as follows:</p><ul><li><code>preloads</code> specifies a list of libraries that are to be loaded (in order) before <code>libmpi</code>.</li><li><code>preloads_env_switch</code> specifies the name of an environment variable that, if set to <code>0</code>, can disable the <code>preloads</code></li><li><code>cclibs</code> is a list of libraries also linked by the compiler wrappers. This is recorded mainly for debugging purposes, and the libraries listed here are not explicitly loaded by <code>MPI.jl</code>.</li></ul><p>If these are set, the <code>_format</code> key will be set to <code>&quot;1.1&quot;</code>.</p><p>An example of running <code>MPIPreferences.use_system_binary(vendor=&quot;cray&quot;)</code> in <code>PrgEnv-gnu</code> is:</p><pre><code class="language-toml hljs">[MPIPreferences]
_format = &quot;1.1&quot;
abi = &quot;MPICH&quot;
binary = &quot;system&quot;
cclibs = [&quot;cupti&quot;, &quot;cudart&quot;, &quot;cuda&quot;, &quot;sci_gnu_82_mpi&quot;, &quot;sci_gnu_82&quot;, &quot;dl&quot;, &quot;dsmml&quot;, &quot;xpmem&quot;]
libmpi = &quot;libmpi_gnu_91.so&quot;
mpiexec = &quot;mpiexec&quot;
preloads = [&quot;libmpi_gtl_cuda.so&quot;]
preloads_env_switch = &quot;MPICH_GPU_SUPPORT_ENABLED&quot;</code></pre><p>This is an example of CrayMPICH requiring <code>libmpi_gtl_cuda.so</code> to be preloaded, unless <code>MPICH_GPU_SUPPORT_ENABLED=0</code> (the latter allowing MPI-enabled code to run on a non-GPU enabled node without needing a separate <code>LocalPreferences.toml</code>).</p><h2 id="configure_jll_binary"><a class="docs-heading-anchor" href="#configure_jll_binary">Using an alternative JLL-provided MPI library</a><a id="configure_jll_binary-1"></a><a class="docs-heading-anchor-permalink" href="#configure_jll_binary" title="Permalink"></a></h2><p>The following MPI implementations are provided as JLL packages and automatically obtained when installing MPI.jl:</p><ul><li><code>MicrosoftMPI_jll</code>: <a href="https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi">Microsoft MPI</a> Default for Windows</li><li><code>MPICH_jll</code>: <a href="https://www.mpich.org/">MPICH</a>. Default for all other systems</li><li><code>OpenMPI_jll</code>: <a href="https://www.open-mpi.org/">Open MPI</a></li><li><code>MPItrampoline_jll</code>: <a href="https://github.com/eschnett/MPItrampoline">MPItrampoline</a>: an MPI forwarding layer.</li></ul><p>Call <a href="../reference/mpipreferences/#MPIPreferences.use_jll_binary"><code>MPIPreferences.use_jll_binary</code></a>, for example</p><pre><code class="language-sh hljs">julia --project -e &#39;using MPIPreferences; MPIPreferences.use_jll_binary(&quot;MPItrampoline_jll&quot;)&#39;</code></pre><p>If you omit the JLL binary name, the default is selected for the respective operating system.</p><h2 id="Configuration-of-the-MPI.jl-testsuite"><a class="docs-heading-anchor" href="#Configuration-of-the-MPI.jl-testsuite">Configuration of the MPI.jl testsuite</a><a id="Configuration-of-the-MPI.jl-testsuite-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration-of-the-MPI.jl-testsuite" title="Permalink"></a></h2><h3 id="Testing-against-a-different-MPI-implementation"><a class="docs-heading-anchor" href="#Testing-against-a-different-MPI-implementation">Testing against a different MPI implementation</a><a id="Testing-against-a-different-MPI-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-against-a-different-MPI-implementation" title="Permalink"></a></h3><p>The <code>LocalPreferences.toml</code> must be located within the <code>test</code> folder, you can either create it in place or copy it into place.</p><pre><code class="nohighlight hljs">~/MPI&gt; julia --project=test
julia&gt; using MPIPreferences
julia&gt; MPIPreferences.use_system_binary()
~/MPI&gt; rm test/Manifest.toml
~/MPI&gt; julia --project
(MPI) pkg&gt; test</code></pre><h3 id="Testing-GPU-aware-buffers"><a class="docs-heading-anchor" href="#Testing-GPU-aware-buffers">Testing GPU-aware buffers</a><a id="Testing-GPU-aware-buffers-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-GPU-aware-buffers" title="Permalink"></a></h3><p>The test suite can target CUDA-aware interface with <a href="https://github.com/JuliaGPU/CUDA.jl"><code>CUDA.CuArray</code></a> and the ROCm-aware interface with <a href="https://github.com/JuliaGPU/AMDGPU.jl"><code>AMDGPU.ROCArray</code></a> upon selecting the corresponding <code>test_args</code> kwarg when calling <code>Pkg.test</code>.</p><p>Run Pkg.test with <code>--backend=CUDA</code> to test CUDA-aware MPI buffers</p><pre><code class="nohighlight hljs">import Pkg; Pkg.test(&quot;MPI&quot;; test_args=[&quot;--backend=CUDA&quot;])</code></pre><p>and with <code>--backend=AMDGPU</code> to test ROCm-aware MPI buffers</p><pre><code class="nohighlight hljs">import Pkg; Pkg.test(&quot;MPI&quot;; test_args=[&quot;--backend=AMDGPU&quot;])</code></pre><div class="admonition is-info" id="Note-f0327e37ded0831a"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-f0327e37ded0831a" title="Permalink"></a></header><div class="admonition-body"><p>The <code>JULIA_MPI_TEST_ARRAYTYPE</code> environment variable has no effect anymore.</p></div></div><h3 id="Environment-variables"><a class="docs-heading-anchor" href="#Environment-variables">Environment variables</a><a id="Environment-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Environment-variables" title="Permalink"></a></h3><p>The test suite can also be modified by the following variables:</p><ul><li><code>JULIA_MPI_TEST_NPROCS</code>: How many ranks to use within the tests</li><li><code>JULIA_MPI_TEST_BINARY</code>: Check that the specified MPI binary is used for the tests</li><li><code>JULIA_MPI_TEST_ABI</code>: Check that the specified MPI ABI is used for the tests</li></ul><h2 id="Migration-from-MPI.jl-v0.19-or-earlier"><a class="docs-heading-anchor" href="#Migration-from-MPI.jl-v0.19-or-earlier">Migration from MPI.jl v0.19 or earlier</a><a id="Migration-from-MPI.jl-v0.19-or-earlier-1"></a><a class="docs-heading-anchor-permalink" href="#Migration-from-MPI.jl-v0.19-or-earlier" title="Permalink"></a></h2><p>For MPI.jl v0.20, environment variables were used to configure which MPI library to use. These have been removed and no longer have any effect. The following subsections explain how to the same effects can be achieved with v0.20 or later.</p><div class="admonition is-info" id="Note-ff25bfb0a9d9a851"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-ff25bfb0a9d9a851" title="Permalink"></a></header><div class="admonition-body"><p>Please refer to <a href="#Notes-to-HPC-cluster-administrators">Notes to HPC cluster administrators</a> if you want to migrate your MPI.jl preferences on a cluster with a centrally managed MPI.jl configuration.</p></div></div><h3 id="JULIA_MPI_BINARY"><a class="docs-heading-anchor" href="#JULIA_MPI_BINARY"><code>JULIA_MPI_BINARY</code></a><a id="JULIA_MPI_BINARY-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPI_BINARY" title="Permalink"></a></h3><p>Use <a href="../reference/mpipreferences/#MPIPreferences.use_system_binary"><code>MPIPreferences.use_system_binary</code></a> to use a system-provided MPI binary as described <a href="#configure_system_binary">here</a>. To switch back or select a different JLL-provided MPI binary, use <a href="../reference/mpipreferences/#MPIPreferences.use_jll_binary"><code>MPIPreferences.use_jll_binary</code></a> as described <a href="#configure_jll_binary">here</a>.</p><h3 id="JULIA_MPI_PATH"><a class="docs-heading-anchor" href="#JULIA_MPI_PATH"><code>JULIA_MPI_PATH</code></a><a id="JULIA_MPI_PATH-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPI_PATH" title="Permalink"></a></h3><p>Removed without replacement.</p><h3 id="JULIA_MPI_LIBRARY"><a class="docs-heading-anchor" href="#JULIA_MPI_LIBRARY"><code>JULIA_MPI_LIBRARY</code></a><a id="JULIA_MPI_LIBRARY-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPI_LIBRARY" title="Permalink"></a></h3><p>Use <a href="../reference/mpipreferences/#MPIPreferences.use_system_binary"><code>MPIPreferences.use_system_binary</code></a> with keyword argument <code>library_names</code> to specify possible, non-standard library names. Alternatively, you can also specify the full path to the library.</p><h3 id="JULIA_MPI_ABI"><a class="docs-heading-anchor" href="#JULIA_MPI_ABI"><code>JULIA_MPI_ABI</code></a><a id="JULIA_MPI_ABI-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPI_ABI" title="Permalink"></a></h3><p>Use <a href="../reference/mpipreferences/#MPIPreferences.use_system_binary"><code>MPIPreferences.use_system_binary</code></a> with keyword argument <code>abi</code> to specify which ABI to use. See <a href="../reference/mpipreferences/#MPIPreferences.abi"><code>MPIPreferences.abi</code></a> for possible values.</p><h3 id="JULIA_MPIEXEC"><a class="docs-heading-anchor" href="#JULIA_MPIEXEC"><code>JULIA_MPIEXEC</code></a><a id="JULIA_MPIEXEC-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPIEXEC" title="Permalink"></a></h3><p>Use <a href="../reference/mpipreferences/#MPIPreferences.use_system_binary"><code>MPIPreferences.use_system_binary</code></a> with keyword argument <code>mpiexec</code> to specify the MPI launcher executable.</p><h3 id="JULIA_MPIEXEC_ARGS"><a class="docs-heading-anchor" href="#JULIA_MPIEXEC_ARGS"><code>JULIA_MPIEXEC_ARGS</code></a><a id="JULIA_MPIEXEC_ARGS-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPIEXEC_ARGS" title="Permalink"></a></h3><p>Use <a href="../reference/mpipreferences/#MPIPreferences.use_system_binary"><code>MPIPreferences.use_system_binary</code></a> with keyword argument <code>mpiexec</code>, and pass a <a href="https://docs.julialang.org/en/v1/manual/running-external-programs/#Cmd-Objects"><code>Cmd</code></a> object to set the MPI launcher executable and to include specific command line options.</p><h3 id="JULIA_MPI_INCLUDE_PATH"><a class="docs-heading-anchor" href="#JULIA_MPI_INCLUDE_PATH"><code>JULIA_MPI_INCLUDE_PATH</code></a><a id="JULIA_MPI_INCLUDE_PATH-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPI_INCLUDE_PATH" title="Permalink"></a></h3><p>Removed without replacement. Automatic generation of a constants file for unknown MPI ABIs is not supported anymore. See also <a href="https://github.com/JuliaParallel/MPI.jl/issues/574">#574</a>.</p><h3 id="JULIA_MPI_CFLAGS"><a class="docs-heading-anchor" href="#JULIA_MPI_CFLAGS"><code>JULIA_MPI_CFLAGS</code></a><a id="JULIA_MPI_CFLAGS-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPI_CFLAGS" title="Permalink"></a></h3><p>Removed without replacement. Automatic generation of a constants file for unknown MPI ABIs is not supported anymore. See also <a href="https://github.com/JuliaParallel/MPI.jl/issues/574">#574</a>.</p><h3 id="JULIA_MPICC"><a class="docs-heading-anchor" href="#JULIA_MPICC"><code>JULIA_MPICC</code></a><a id="JULIA_MPICC-1"></a><a class="docs-heading-anchor-permalink" href="#JULIA_MPICC" title="Permalink"></a></h3><p>Removed without replacement. Automatic generation of a constants file for unknown MPI ABIs is not supported anymore. See also <a href="https://github.com/JuliaParallel/MPI.jl/issues/574">#574</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« MPI.jl</a><a class="docs-footer-nextpage" href="../usage/">Usage »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Friday 7 November 2025 20:06">Friday 7 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
