<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Configuration · MPI.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MPI.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MPI.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">MPI.jl</a></li><li class="is-active"><a class="tocitem" href>Configuration</a><ul class="internal"><li><a class="tocitem" href="#Julia-wrapper-for-mpiexec"><span>Julia wrapper for <code>mpiexec</code></span></a></li><li><a class="tocitem" href="#Using-MPIPreferences.jl"><span>Using MPIPreferences.jl</span></a></li><li><a class="tocitem" href="#Configuration-of-the-MPI.jl-testsuite"><span>Configuration of the MPI.jl testsuite</span></a></li></ul></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><a class="tocitem" href="../knownissues/">Known issues</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/01-hello/">Hello world</a></li><li><a class="tocitem" href="../examples/02-broadcast/">Broadcast</a></li><li><a class="tocitem" href="../examples/03-reduce/">Reduce</a></li><li><a class="tocitem" href="../examples/04-sendrecv/">Send/receive</a></li><li><a class="tocitem" href="../examples/05-job_schedule/">Job Scheduling</a></li><li><a class="tocitem" href="../examples/06-scatterv/">Scatterv and Gatherv</a></li><li><a class="tocitem" href="../examples/07-rma_active/">Active RMA</a></li><li><a class="tocitem" href="../examples/08-rma_passive/">Passive RMA</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../mpipreferences/">MPIPreferences.jl</a></li><li><a class="tocitem" href="../library/">Library information</a></li><li><a class="tocitem" href="../environment/">Environment</a></li><li><a class="tocitem" href="../comm/">Communicators</a></li><li><a class="tocitem" href="../buffers/">Buffers</a></li><li><a class="tocitem" href="../pointtopoint/">Point-to-point communication</a></li><li><a class="tocitem" href="../collective/">Collective communication</a></li><li><a class="tocitem" href="../onesided/">One-sided communication</a></li><li><a class="tocitem" href="../topology/">Topology</a></li><li><a class="tocitem" href="../io/">I/O</a></li><li><a class="tocitem" href="../advanced/">Advanced</a></li></ul></li><li><a class="tocitem" href="../refindex/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Configuration</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Configuration</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/configuration.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Configuration"><a class="docs-heading-anchor" href="#Configuration">Configuration</a><a id="Configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration" title="Permalink"></a></h1><p>By default, MPI.jl will download and link against the following MPI implementations:</p><ul><li><a href="https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi">Microsoft MPI</a> on Windows</li><li><a href="http://www.mpich.org/">MPICH</a> on all other platforms</li></ul><p>This is suitable for most single-node use cases, but for larger systems, such as HPC clusters or multi-GPU machines, you will probably want to configure against a system-provided MPI implementation in order to exploit features such as fast network interfaces and CUDA-aware MPI interfaces.</p><h2 id="Julia-wrapper-for-mpiexec"><a class="docs-heading-anchor" href="#Julia-wrapper-for-mpiexec">Julia wrapper for <code>mpiexec</code></a><a id="Julia-wrapper-for-mpiexec-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-wrapper-for-mpiexec" title="Permalink"></a></h2><p>Since you can configure <code>MPI.jl</code> to use one of several MPI implementations, you may have different Julia projects using different implementation.  Thus, it may be cumbersome to find out which <code>mpiexec</code> executable is associated to a specific project.  To make this easy, on Unix-based systems <code>MPI.jl</code> comes with a thin project-aware wrapper around <code>mpiexec</code>, called <code>mpiexecjl</code>.</p><h3 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h3><p>You can install <code>mpiexecjl</code> with <a href="../environment/#MPI.install_mpiexecjl"><code>MPI.install_mpiexecjl()</code></a>.  The default destination directory is <code>joinpath(DEPOT_PATH[1], &quot;bin&quot;)</code>, which usually translates to <code>~/.julia/bin</code>, but check the value on your system.  You can also tell <code>MPI.install_mpiexecjl</code> to install to a different directory.</p><pre><code class="language-sh hljs">$ julia
julia&gt; using MPI
julia&gt; MPI.install_mpiexecjl()</code></pre><p>To quickly call this wrapper we recommend you to add the destination directory to your <a href="https://en.wikipedia.org/wiki/PATH_(variable)"><code>PATH</code></a> environment variable.</p><h3 id="Usage"><a class="docs-heading-anchor" href="#Usage">Usage</a><a id="Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Usage" title="Permalink"></a></h3><p><code>mpiexecjl</code> has the same syntax as the <code>mpiexec</code> binary that will be called, but it takes in addition a <code>--project</code> option to call the specific binary associated to the <code>MPI.jl</code> version in the given project.  If no <code>--project</code> flag is used, the <code>MPI.jl</code> in the global Julia environment will be used instead.</p><p>After installing <code>mpiexecjl</code> and adding its directory to <code>PATH</code>, you can run it with:</p><pre><code class="language-sh hljs">$ mpiexecjl --project=/path/to/project -n 20 julia script.jl</code></pre><h2 id="Using-MPIPreferences.jl"><a class="docs-heading-anchor" href="#Using-MPIPreferences.jl">Using MPIPreferences.jl</a><a id="Using-MPIPreferences.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Using-MPIPreferences.jl" title="Permalink"></a></h2><p>MPI.jl uses <a href="https://github.com/JuliaPackaging/Preferences.jl">Preferences.jl</a> to allow the user to choose which MPI implementation to use for a project. This provides a single source of truth that can be used for JLL packages (Julia packages providing C libraries) that link against MPI, localizes the choice of MPI implementation to a project.</p><p>Users can use the provided <a href="@ref"><code>use_system_binary</code></a> or <a href="@ref"><code>use_jll_binary</code></a> to switch MPI implementations. By default, the JLL-provided binaries are used.</p><h3 id="Migration-from-MPI.jl-v0.19"><a class="docs-heading-anchor" href="#Migration-from-MPI.jl-v0.19">Migration from MPI.jl <code>v0.19</code></a><a id="Migration-from-MPI.jl-v0.19-1"></a><a class="docs-heading-anchor-permalink" href="#Migration-from-MPI.jl-v0.19" title="Permalink"></a></h3><p>Prior to MPI.jl <code>v0.20</code> environment variables were used to configure which MPI library to use. These have now been removed and have no effect anymore:</p><ul><li><code>JULIA_MPI_BINARY</code></li><li><code>JULIA_MPIEXEC</code></li><li><code>JULIA_MPI_INCLUDE_PATH</code></li><li><code>JULIA_MPI_CFLAGS</code></li><li><code>JULIA_MPICC</code></li></ul><h3 id="Using-a-system-provided-MPI-backend"><a class="docs-heading-anchor" href="#Using-a-system-provided-MPI-backend">Using a system-provided MPI backend</a><a id="Using-a-system-provided-MPI-backend-1"></a><a class="docs-heading-anchor-permalink" href="#Using-a-system-provided-MPI-backend" title="Permalink"></a></h3><h4 id="Requirements"><a class="docs-heading-anchor" href="#Requirements">Requirements</a><a id="Requirements-1"></a><a class="docs-heading-anchor-permalink" href="#Requirements" title="Permalink"></a></h4><p>MPI.jl requires a shared library installation of a C MPI library, supporting the MPI 3.0 standard or later.</p><h3 id="Configuration-2"><a class="docs-heading-anchor" href="#Configuration-2">Configuration</a><a class="docs-heading-anchor-permalink" href="#Configuration-2" title="Permalink"></a></h3><p>To use the system MPI library, run <code>MPI.use_system_binary()</code>. This will attempt to locate and to identify any available MPI implementation, and create a file called <code>LocalPreferences.toml</code> adjacent to the current <code>Project.toml</code>. Use <code>Base.active_project()</code> to obtain the location of the currently active project.</p><pre><code class="language-sh hljs">julia --project -e &#39;using MPI; MPI.use_system_binary()&#39;</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>You can copy <code>LocalPreferences.toml</code> to a different project folder, but you must list <code>MPIPreferences</code> in the <code>[extras]</code> section of the <code>Project.toml</code> for the settings to take effect. Due to a bug in Julia (until <code>v1.6.5</code> and <code>v1.7.1</code>), getting preferences from transitive dependencies is broken (https://github.com/JuliaPackaging/Preferences.jl/issues/24). To fix this update your version of Julia, or add <code>MPIPreferences</code> as a direct dependency to your project.</p></div></div><p>The following MPI implementations should work out-of-the-box with MPI.jl:</p><ul><li><a href="http://www.open-mpi.org/">Open MPI</a></li><li><a href="http://www.mpich.org/">MPICH</a> (v3.1 or later)</li><li><a href="https://software.intel.com/en-us/mpi-library">Intel MPI</a></li><li><a href="https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi">Microsoft MPI</a></li><li><a href="https://www.ibm.com/us-en/marketplace/spectrum-mpi">IBM Spectrum MPI</a></li><li><a href="http://mvapich.cse.ohio-state.edu/">MVAPICH</a></li><li><a href="https://docs.nersc.gov/development/compilers/wrappers/">Cray MPICH</a></li><li><a href="https://www.fujitsu.com/global/about/resources/publications/technicalreview/2020-03/article07.html#cap-03">Fujitsu MPI</a></li></ul><p>If the implementation is changed, you will need to use <a href="@ref"><code>MPI.use_system_binary()</code></a> or <a href="@ref"><code>MPI.use_jll_binary()</code></a>.</p><h4 id="Advanced-options"><a class="docs-heading-anchor" href="#Advanced-options">Advanced options</a><a id="Advanced-options-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-options" title="Permalink"></a></h4><pre><code class="language- hljs">MPI.use_system_binary</code></pre><p>You can use the argument <code>mpiexec</code> to provide the name (or full path) of the MPI launcher executable. The default is <code>mpiexec</code>, but some clusters require using the scheduler launcher interface (e.g. <code>srun</code> on Slurm, <code>aprun</code> on PBS). If the MPI library has an uncommon name you can provide it in <code>library_names</code>. The MPI standard does not specify the exact application binary interface (ABI). In case ABI detection fails you can provide a manual choice (either <code>MPICH</code>, <code>MPItrampoline</code>, <code>OpenMPI</code>, or <code>MicrosoftMPI</code>), but also open an issue such that the automatic detection can be improved. <code>export_prefs=true</code> can be used to copy the preferences into the <code>Project.toml</code> instead of creating a <code>LocalPreferences.toml</code> file to hold them.</p><h4 id="Notes-to-HPC-cluster-adminstators"><a class="docs-heading-anchor" href="#Notes-to-HPC-cluster-adminstators">Notes to HPC cluster adminstators</a><a id="Notes-to-HPC-cluster-adminstators-1"></a><a class="docs-heading-anchor-permalink" href="#Notes-to-HPC-cluster-adminstators" title="Permalink"></a></h4><p>Preferences are merged across the Julia load path, such that it is feasible to provide a module file that appends a path to <code>JULIA_LOAD_PATH</code> variable that contains system-wide preferences.</p><p>As an example you can use <a href="@ref"><code>MPI.use_system_binary()</code></a> to create a file <code>LocalPreferences.toml</code> containing:</p><pre><code class="language-toml hljs">[MPIPreferences]
abi = &quot;OpenMPI&quot;
binary = &quot;system&quot;
libmpi = &quot;/software/mpi/lib/libmpi.so&quot;
mpiexec = &quot;/software/mpi/bin/mpiexec&quot;</code></pre><p>Copying this <code>LocalPreferences.toml</code> to a central location such as <code>/software/mpi/julia</code> and create adjacent to it a <code>Project.toml</code> containing:</p><pre><code class="language-toml hljs">[extras]
MPIPreferences = &quot;3da0fdf6-3ccc-4f1b-acd9-58baa6c99267&quot;</code></pre><p>Now exporting the environment variable <code>JULIA_LOAD_PATH=&quot;:/software/mpi/julia&quot;</code> (note the <code>:</code> before the path) in the corresponding module file (preferably the module file for the MPI installation or for Julia), will cause MPI.jl to default to your cluster MPI installation.</p><p>The user can still provide differing MPI configurations for each Julia project that will take precedent by modifying the local <code>Project.toml</code> or by providing a <code>LocalPreferences.toml</code> file.</p><h3 id="Using-a-different-JLL-provided-MPI-library"><a class="docs-heading-anchor" href="#Using-a-different-JLL-provided-MPI-library">Using a different JLL provided MPI library</a><a id="Using-a-different-JLL-provided-MPI-library-1"></a><a class="docs-heading-anchor-permalink" href="#Using-a-different-JLL-provided-MPI-library" title="Permalink"></a></h3><p>The following MPI implementations are provided as JLL packages and automatically obtained when installing MPI.jl:</p><ul><li><code>MicrosoftMPI_jll</code>: Default for Windows</li><li><code>MPICH_jll</code>: Default for all Unix-like systems</li><li><a href="https://github.com/eschnett/MPItrampoline"><code>MPItrampoline_jll</code></a>: Binaries built against MPItrampoline can be efficiently retargetted to a system MPI implementation.</li><li><code>OpenMPI_jll</code>:</li></ul><pre><code class="language- hljs">MPI.use_jll_binary</code></pre><h2 id="Configuration-of-the-MPI.jl-testsuite"><a class="docs-heading-anchor" href="#Configuration-of-the-MPI.jl-testsuite">Configuration of the MPI.jl testsuite</a><a id="Configuration-of-the-MPI.jl-testsuite-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration-of-the-MPI.jl-testsuite" title="Permalink"></a></h2><h3 id="Testing-against-a-different-MPI-implementation"><a class="docs-heading-anchor" href="#Testing-against-a-different-MPI-implementation">Testing against a different MPI implementation</a><a id="Testing-against-a-different-MPI-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-against-a-different-MPI-implementation" title="Permalink"></a></h3><p>The <code>LocalPreferences.toml</code> must be located within the <code>test</code> folder, you can either create it in place or copy it into place.</p><pre><code class="nohighlight hljs">~/MPI&gt; julia --project=test
julia&gt; using MPIPreferences
julia&gt; MPIPreferences.use_system_binary()
~/MPI&gt; rm test/Manifest.toml
~/MPI&gt; julia --project
(MPI) pkg&gt; test</code></pre><h3 id="Environment-variables"><a class="docs-heading-anchor" href="#Environment-variables">Environment variables</a><a id="Environment-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Environment-variables" title="Permalink"></a></h3><p>The test suite can also be modified by the following variables:</p><ul><li><code>JULIA_MPI_TEST_NPROCS</code>: How many ranks to use within the tests</li><li><code>JULIA_MPI_TEST_ARRAYTYPE</code>: Set to <code>CuArray</code> to test the CUDA-aware interface with <a href="https://github.com/JuliaGPU/CUDA.jl">`CUDA.CuArray</a> buffers.</li><li><code>JULIA_MPI_TEST_BINARY</code>: Check that the specified MPI binary is used for the tests</li><li><code>JULIA_MPI_TEST_ABI</code>: Check that the specified MPI ABI is used for the tests</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« MPI.jl</a><a class="docs-footer-nextpage" href="../usage/">Usage »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.15 on <span class="colophon-date" title="Monday 18 April 2022 18:54">Monday 18 April 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
