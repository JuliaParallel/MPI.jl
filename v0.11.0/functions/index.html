<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Function reference · MPI.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MPI.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">MPI.jl</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/01-hello/">Hello world</a></li><li><a class="tocitem" href="../examples/02-broadcast/">Broadcast</a></li><li><a class="tocitem" href="../examples/03-reduce/">Reduce</a></li><li><a class="tocitem" href="../examples/04-sendrecv/">Send/receive</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../environment/">Environment</a></li><li><a class="tocitem" href="../comm/">Communicators</a></li><li><a class="tocitem" href="../pointtopoint/">Point-to-point communication</a></li></ul></li><li class="is-active"><a class="tocitem" href>Function reference</a><ul class="internal"><li><a class="tocitem" href="#Datatype-functions-1"><span>Datatype functions</span></a></li><li><a class="tocitem" href="#Collective-communication-1"><span>Collective communication</span></a></li><li><a class="tocitem" href="#One-sided-communication-1"><span>One-sided communication</span></a></li><li class="toplevel"><a class="tocitem" href="#Info-objects-1"><span>Info objects</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Function reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Function reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/functions.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Function-reference-1"><a class="docs-heading-anchor" href="#Function-reference-1">Function reference</a><a class="docs-heading-anchor-permalink" href="#Function-reference-1" title="Permalink"></a></h1><p>The following functions are currently wrapped, with the convention: <code>MPI_Fun =&gt; MPI.Fun</code></p><p>Constants like <code>MPI_SUM</code> are wrapped as <code>MPI.SUM</code>.   Note also that arbitrary Julia functions <code>f(x,y)</code> can be passed as reduction operations to the MPI <code>Allreduce</code> and <code>Reduce</code> functions.</p><h2 id="Datatype-functions-1"><a class="docs-heading-anchor" href="#Datatype-functions-1">Datatype functions</a><a class="docs-heading-anchor-permalink" href="#Datatype-functions-1" title="Permalink"></a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_address</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_address.3.php"><code>MPI_Get_address</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.mpitype</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_create_struct.3.php"><code>MPI_Type_create_struct</code></a>/<a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Type_commit.3.php"><code>MPI_Type_commit</code></a></td></tr></table><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><code>mpitype</code> is not strictly a wrapper for <code>MPI_Type_create_struct</code> and <code>MPI_Type_commit</code>, it also is an accessor for previously created types.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Get_address</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.mpitype</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Collective-communication-1"><a class="docs-heading-anchor" href="#Collective-communication-1">Collective communication</a><a class="docs-heading-anchor-permalink" href="#Collective-communication-1" title="Permalink"></a></h2><table><tr><th style="text-align: right">Non-Allocating Julia Function</th><th style="text-align: right">Allocating Julia Function</th><th style="text-align: right">Fortran Function</th><th style="text-align: right">Supports <code>MPI_IN_PLACE</code></th></tr><tr><td style="text-align: right"><a href="#MPI.Allgather!"><code>MPI.Allgather!</code></a></td><td style="text-align: right"><a href="#MPI.Allgather"><code>MPI.Allgather</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgather.3.php"><code>MPI_Allgather</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a></td><td style="text-align: right"><a href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allgatherv.3.php"><code>MPI_Allgatherv</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a></td><td style="text-align: right"><a href="#MPI.Allreduce"><code>MPI.Allreduce</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Allreduce.3.php"><code>MPI_Allreduce</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a></td><td style="text-align: right"><a href="#MPI.Alltoall"><code>MPI.Alltoall</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoall.3.php"><code>MPI_Alltoall</code></a></td><td style="text-align: right">✅</td></tr><tr><td style="text-align: right"><a href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a></td><td style="text-align: right"><a href="@ref"><code>MPI.Alltoallv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Alltoallv.3.php"><code>MPI_Alltoallv</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right">–</td><td style="text-align: right"><a href="#MPI.Barrier"><code>MPI.Barrier</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Barrier.3.php"><code>MPI_Barrier</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right"><a href="#MPI.Bcast!"><code>MPI.Bcast!</code></a></td><td style="text-align: right"><a href="#MPI.Bcast!"><code>MPI.Bcast!</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Bcast.3.php"><code>MPI_Bcast</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right">–</td><td style="text-align: right"><a href="@ref"><code>MPI.Exscan</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Exscan.3.php"><code>MPI_Exscan</code></a></td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right"><a href="#MPI.Gather!"><code>MPI.Gather!</code></a></td><td style="text-align: right"><a href="#MPI.Gather"><code>MPI.Gather</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gather.3.php"><code>MPI_Gather</code></a></td><td style="text-align: right"><a href="@ref"><code>Gather_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a></td><td style="text-align: right"><a href="#MPI.Gatherv"><code>MPI.Gatherv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Gatherv.3.php"><code>MPI_Gatherv</code></a></td><td style="text-align: right"><a href="@ref"><code>Gatherv_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Reduce!"><code>MPI.Reduce!</code></a></td><td style="text-align: right"><a href="#MPI.Reduce"><code>MPI.Reduce</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Reduce.3.php"><code>MPI_Reduce</code></a></td><td style="text-align: right"><a href="@ref"><code>Reduce_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Scan</code></a></td><td style="text-align: right"><a href="@ref"><code>MPI.Scan</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scan.3.php"><code>MPI_Scan</code></a></td><td style="text-align: right">missing</td></tr><tr><td style="text-align: right"><a href="#MPI.Scatter!"><code>MPI.Scatter!</code></a></td><td style="text-align: right"><a href="#MPI.Scatter"><code>MPI.Scatter</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatter.3.php"><code>MPI_Scatter</code></a></td><td style="text-align: right"><a href="@ref"><code>Scatter_in_place!</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a></td><td style="text-align: right"><a href="#MPI.Scatterv"><code>MPI.Scatterv</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v1.10/man3/MPI_Scatterv.3.php"><code>MPI_Scatterv</code></a></td><td style="text-align: right"><a href="@ref"><code>Scatterv_in_place!</code></a></td></tr></table><p>The non-allocating Julia functions map directly to the corresponding MPI operations, after asserting that the size of the output buffer is sufficient to store the result.</p><p>The allocating Julia functions allocate an output buffer and then call the non-allocating method.</p><p>All-to-all collective communications support in place operations by passing <code>MPI.IN_PLACE</code> with the same syntax documented by MPI. One-to-All communications support it by calling the function <code>*_in_place!</code>, calls the MPI functions with the right syntax on root and non root process.</p><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgather!" href="#MPI.Allgather!"><code>MPI.Allgather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgather!(sendbuf, recvbuf, count, comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order into <code>recvbuf</code>.</p><p>If <code>sendbuf==MPI.IN_PLACE</code> the input data is assumed to be in the area of <code>recvbuf</code> where the process would receive it&#39;s own contribution.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L487-L497">source</a></section><section><div><pre><code class="language-none">Allgather!(buf, count, comm)</code></pre><p>Equivalent to <code>Allgather!(MPI.IN_PLACE, buf, count, comm)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L512-L516">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgather" href="#MPI.Allgather"><code>MPI.Allgather</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgather(sendbuf[, count=length(sendbuf)], comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order allocating the output buffer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L522-L528">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgatherv!" href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgatherv!(sendbuf, recvbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process stores the received data in rank order in the buffer <code>recvbuf</code>.</p><p>if <code>sendbuf==MPI.IN_PLACE</code> every process takes the data to be sent is taken from the interval of <code>recvbuf</code> where it would store it&#39;s own data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L627-L636">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgatherv" href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgatherv(sendbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process allocates an output buffer and stores the received data in rank order.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L653-L659">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allreduce!" href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the buffer <code>sendbuf</code> storing the result in the <code>recvbuf</code> of all processes in the group.</p><p>All-reduce is equivalent to a <a href="#MPI.Reduce!"><code>Reduce!</code></a> operation followed by a <a href="#MPI.Bcast!"><code>Bcast!</code></a>, but can lead to better performance.</p><p>If <code>sendbuf==MPI.IN_PLACE</code> the data is read from <code>recvbuf</code> and then overwritten with the results.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Allreduce"><code>Allreduce</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L173-L187">source</a></section><section><div><pre><code class="language-none">Allreduce!(buf, op, comm)</code></pre><p>Performs <code>op</code> reduction in place on the buffer <code>sendbuf</code>, overwriting it with the results on all the processes in the group.</p><p>Equivalent to calling <code>Allreduce!(MPI.IN_PLACE, buf, op, comm)</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L211-L218">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allreduce" href="#MPI.Allreduce"><code>MPI.Allreduce</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allreduce(sendbuf, op, comm)</code></pre><p>Performs <code>op</code> reduction on the buffer <code>sendbuf</code>, allocating and returning the output buffer in all processes of the group.</p><p>To specify the output buffer or perform the operation in pace, see <a href="#MPI.Allreduce!"><code>Allreduce!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L223-L230">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoall!" href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoall!(sendbuf, recvbuf, count, comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk of the buffer <code>recvbuf</code>.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p>If <code>sendbuf==MPI.IN_PLACE</code>, data is sent from the <code>recvbuf</code> and then overwritten.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L666-L684">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoall" href="#MPI.Alltoall"><code>MPI.Alltoall</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoall(sendbuf, count, comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process allocates the output buffer and stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L698-L713">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoallv!" href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoallv!(sendbuf::T, recvbuf::T, scounts, rcounts, comm)</code></pre><p><code>MPI.IN_PLACE</code> is not supported for this operation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L720-L724">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Alltoallv</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="MPI.Barrier" href="#MPI.Barrier"><code>MPI.Barrier</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Barrier(comm::Comm)</code></pre><p>Blocks until <code>comm</code> is synchronized.</p><p>If <code>comm</code> is an intracommunicator, then it blocks until all members of the group have called it.</p><p>If <code>comm</code> is an intercommunicator, then it blocks until all members of the other group have called it.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L4-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Bcast!" href="#MPI.Bcast!"><code>MPI.Bcast!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Bcast!(buf[, count=length(buf)], root, comm::Comm)</code></pre><p>Broadcast the first <code>count</code> elements of the buffer <code>buf</code> from <code>root</code> to all processes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L18-L22">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Bcast!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Exscan</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather!" href="#MPI.Gather!"><code>MPI.Gather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gather!(sendbuf, recvbuf, count, root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>recvbuf</code>.</p><p><code>count</code> should be the same for all processes. If the number of elements varies between processes, use <a href="#MPI.Gatherv!"><code>Gatherv!</code></a> instead.</p><p>To perform the reduction in place refer to <a href="#MPI.Gather_in_place!"><code>Gather_in_place!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L399-L410">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather" href="#MPI.Gather"><code>MPI.Gather</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gather(sendbuf[, count=length(sendbuf)], root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L428-L434">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather_in_place!" href="#MPI.Gather_in_place!"><code>MPI.Gather_in_place!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gather_in_place!(buf, count, root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>buf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>buf</code>, sending no data to itself.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gather!(MPI.IN_PLACE, buf, count, root, comm)
else
    Gather!(buf, C_NULL, count, root, comm)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L455-L470">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gatherv!" href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gatherv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> stores elements in rank order in the buffer <code>recvbuf</code>.</p><p>To perform the reduction in place refer to <a href="#MPI.Gatherv_in_place!"><code>Gatherv_in_place!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L543-L551">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gatherv" href="#MPI.Gatherv"><code>MPI.Gatherv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gatherv(sendbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L569-L575">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gatherv_in_place!" href="#MPI.Gatherv_in_place!"><code>MPI.Gatherv_in_place!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gatherv_in_place!(buf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>buf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gatherv!(MPI.IN_PLACE, buf, counts, root, comm)
else
    Gatherv!(buf, C_NULL, counts, root, comm)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L590-L605">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Reduce!" href="#MPI.Reduce!"><code>MPI.Reduce!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Reduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the  buffer <code>sendbuf</code> and stores the result in <code>recvbuf</code> on the process of rank <code>root</code>.</p><p>On non-root processes <code>recvbuf</code> is ignored.</p><p>To perform the reduction in place, see <a href="#MPI.Reduce_in_place!"><code>Reduce_in_place!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Reduce"><code>Reduce</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L64-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Reduce" href="#MPI.Reduce"><code>MPI.Reduce</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Reduce(sendbuf, count, op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the buffer <code>sendbuf</code> and stores the result in an output buffer allocated on the process of rank <code>root</code>. An empty array will be returned on all other processes.</p><p>To specify the output buffer, see <a href="#MPI.Reduce!"><code>Reduce!</code></a>.</p><p>To perform the reduction in place, see <a href="#MPI.Reduce_in_place!"><code>Reduce_in_place!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L99-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Reduce_in_place!" href="#MPI.Reduce_in_place!"><code>MPI.Reduce_in_place!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Reduce_in_place!(buf, count, op, root, comm)</code></pre><p>Performs <code>op</code> reduction on the first <code>count</code> elements of the buffer <code>buf</code> and stores the result on <code>buf</code> of the <code>root</code> process in the group.</p><p>This is equivalent to calling</p><pre><code class="language-julia">if root == MPI.Comm_rank(comm)
    Reduce!(MPI.IN_PLACE, buf, count, root, comm)
else
    Reduce!(buf, C_NULL, count, root, comm)
end</code></pre><p>To handle allocation of the output buffer, see <a href="#MPI.Reduce"><code>Reduce</code></a>.</p><p>To specify a separate output buffer, see <a href="#MPI.Reduce!"><code>Reduce!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L132-L150">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Scan</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Scan</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatter!" href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatter!(sendbuf, recvbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p><code>count</code> should be the same for all processes. If the number of elements varies between processes, use <a href="#MPI.Scatter!"><code>Scatter!</code></a> instead.</p><p>To perform the reduction in place, see <a href="#MPI.Scatter_in_place!"><code>Scatter_in_place!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Scatter"><code>Scatter</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L245-L258">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatter" href="#MPI.Scatter"><code>MPI.Scatter</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatter(sendbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j, allocating the output buffer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L313-L318">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatter_in_place!" href="#MPI.Scatter_in_place!"><code>MPI.Scatter_in_place!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatter_in_place!(buf, count, root, comm)</code></pre><p>Splits the buffer <code>buf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the j-th chunk to the process of rank j. No data is sent to the <code>root</code> process.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatter!(buf, MPI.IN_PLACE, count, root, comm)
else
    Scatter!(C_NULL, buf, count, root, comm)
end</code></pre><p>To specify a separate output buffer, see <a href="#MPI.Scatter!"><code>Scatter!</code></a>.</p><p>To handle allocation of the output buffer, see <a href="#MPI.Scatter"><code>Scatter</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L277-L296">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatterv!" href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatterv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p>To perform the reduction in place refer to <a href="#MPI.Scatterv_in_place!"><code>Scatterv_in_place!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L325-L333">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatterv" href="#MPI.Scatterv"><code>MPI.Scatterv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatterv(sendbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j, which allocates the output buffer</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L350-L356">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatterv_in_place!" href="#MPI.Scatterv_in_place!"><code>MPI.Scatterv_in_place!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatterv_in_place(buf, counts, root, comm)</code></pre><p>Splits the buffer <code>buf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>buf</code> buffer, which must be of length at least <code>count</code>. The <code>root</code> process sends nothing to itself.</p><p>This is functionally equivalent to calling</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatterv!(buf, MPI.IN_PLACE, counts, root, comm)
else
    Scatterv!(C_NULL, buf, counts, root, comm)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/collective.jl#L363-L379">source</a></section></article><h2 id="One-sided-communication-1"><a class="docs-heading-anchor" href="#One-sided-communication-1">One-sided communication</a><a class="docs-heading-anchor-permalink" href="#One-sided-communication-1" title="Permalink"></a></h2><table><tr><th style="text-align: right">Julia Function (assuming <code>import MPI</code>)</th><th style="text-align: right">Fortran Function</th></tr><tr><td style="text-align: right"><a href="#MPI.Win_create"><code>MPI.Win_create</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create.3.php"><code>MPI_Win_create</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Win_create_dynamic"><code>MPI.Win_create_dynamic</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_create_dynamic.3.php"><code>MPI_Win_create_dynamic</code></a></td></tr><tr><td style="text-align: right"><a href="#MPI.Win_allocate_shared"><code>MPI.Win_allocate_shared</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_allocate_shared.3.php"><code>MPI_Win_allocate_shared</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_shared_query</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_shared_query.3.php"><code>MPI_Win_shared_query</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_attach</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_attach.3.php"><code>MPI_Win_attach</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_detach</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_detach.3.php"><code>MPI_Win_detach</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_fence</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_fence.3.php"><code>MPI_Win_fence</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_flush</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_flush.3.php"><code>MPI_Win_flush</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_free</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_free.3.php"><code>MPI_Win_free</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_sync</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_sync.3.php"><code>MPI_Win_sync</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_lock</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_lock.3.php"><code>MPI_Win_lock</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Win_unlock</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_unlock.3.php"><code>MPI_Win_unlock</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get.3.php"><code>MPI_Get</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Put</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Put.3.php"><code>MPI_Put</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Fetch_and_op</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Fetch_and_op.3.php"><code>MPI_Fetch_and_op</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Accumulate</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Accumulate.3.php"><code>MPI_Accumulate</code></a></td></tr><tr><td style="text-align: right"><a href="@ref"><code>MPI.Get_accumulate</code></a></td><td style="text-align: right"><a href="https://www.open-mpi.org/doc/v3.0/man3/MPI_Get_accumulate.3.php"><code>MPI_Get_accumulate</code></a></td></tr></table><article class="docstring"><header><a class="docstring-binding" id="MPI.Win_create" href="#MPI.Win_create"><code>MPI.Win_create</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">MPI.Win_create(base::Array, comm::Comm; infokws...)</code></pre><p>Create a window over the array <code>base</code>, returning a <code>Win</code> object used by these processes to perform RMA operations</p><p>This is a collective call over <code>comm</code>.</p><p><code>infokws</code> are info keys providing optimization hints.</p><p><a href="@ref"><code>MPI.free</code></a> should be called on the <code>Win</code> object once operations have been completed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/window.jl#L13-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Win_create_dynamic" href="#MPI.Win_create_dynamic"><code>MPI.Win_create_dynamic</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">MPI.Win_create_dynamic(comm::Comm; infokws...)</code></pre><p>Create a dynamic window returning a <code>Win</code> object used by these processes to perform RMA operations</p><p>This is a collective call over <code>comm</code>.</p><p><code>infokws</code> are info keys providing optimization hints.</p><p><a href="@ref"><code>MPI.free</code></a> should be called on the <code>Win</code> object once operations have been completed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/window.jl#L36-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Win_allocate_shared" href="#MPI.Win_allocate_shared"><code>MPI.Win_allocate_shared</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">(win, ptr) = MPI.Win_allocate_shared(T, len, comm::Comm; infokws...)</code></pre><p>Create and allocate a shared memory window for objects of type <code>T</code> of length <code>len</code>, returning a <code>Win</code> and a <code>Ptr{T}</code> object used by these processes to perform RMA operations</p><p>This is a collective call over <code>comm</code>.</p><p><code>infokws</code> are info keys providing optimization hints.</p><p><a href="@ref"><code>MPI.free</code></a> should be called on the <code>Win</code> object once operations have been completed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/window.jl#L60-L71">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_shared_query</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_attach</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_detach</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_fence</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_flush</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_free</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_sync</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_lock</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Win_unlock</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Get</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Put</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Fetch_and_op</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Accumulate</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>MPI.Get_accumulate</code>. Check Documenter&#39;s build log for details.</p></div></div><h1 id="Info-objects-1"><a class="docs-heading-anchor" href="#Info-objects-1">Info objects</a><a class="docs-heading-anchor-permalink" href="#Info-objects-1" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="MPI.Info" href="#MPI.Info"><code>MPI.Info</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Info &lt;: AbstractDict{Symbol,String}</code></pre><p><code>MPI.Info</code> objects store key-value pairs, and are typically used for passing optional arguments to MPI functions.</p><p><strong>Usage</strong></p><p>These will typically be hidden from user-facing APIs by splatting keywords, e.g.</p><pre><code class="language-julia">function f(args...; kwargs...)
    info = Info(kwargs...)
    # pass `info` object to `ccall`
end</code></pre><p>For manual usage, <code>Info</code> objects act like Julia <code>Dict</code> objects:</p><pre><code class="language-julia">info = Info(init=true) # keyword argument is required
info[key] = value
x = info[key]
delete!(info, key)</code></pre><p>If <code>init=false</code> is used in the costructor (the default), a &quot;null&quot; <code>Info</code> object will be returned: no keys can be added to such an object.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/3af96bcefcb0aec559d4652fd75e1bc3fcfe99da/base/#L0-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.infoval" href="#MPI.infoval"><code>MPI.infoval</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">infoval(x)</code></pre><p>Convert Julia object <code>x</code> to a string representation for storing in an <a href="#MPI.Info"><code>Info</code></a> object.</p><p>The MPI specification allows passing strings, Boolean values, integers, and lists.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/23165acd90bf1d99dac9f9572e4d5b2159074d9a/src/info.jl#L60-L66">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../pointtopoint/">« Point-to-point communication</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 4 December 2019 22:47">Wednesday 4 December 2019</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
