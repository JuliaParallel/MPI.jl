var documenterSearchIndex = {"docs":
[{"location":"environment/#Environment-1","page":"Environment","title":"Environment","text":"","category":"section"},{"location":"environment/#Functions-1","page":"Environment","title":"Functions","text":"","category":"section"},{"location":"environment/#","page":"Environment","title":"Environment","text":"MPI.Abort\nMPI.Init\nMPI.Initialized\nMPI.Finalize\nMPI.Finalized\nMPI.refcount_inc\nMPI.refcount_dec","category":"page"},{"location":"environment/#MPI.Abort","page":"Environment","title":"MPI.Abort","text":"Abort(comm::Comm, errcode::Integer)\n\nMake a “best attempt” to abort all tasks in the group of comm. This function does not require that the invoking environment take any action with the error code. However, a Unix or POSIX environment should handle this as a return errorcode from the main program.\n\nExternal links\n\nMPI_Abort man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Init","page":"Environment","title":"MPI.Init","text":"Init()\n\nInitialize MPI in the current process.\n\nAll MPI programs must contain exactly one call to MPI.Init(). In particular, note that it is not valid to call MPI.Init again after calling MPI.Finalize.\n\nThe only MPI functions that may be called before MPI.Init() are MPI.Initialized and MPI.Finalized.\n\nExternal links\n\nMPI_Init man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Initialized","page":"Environment","title":"MPI.Initialized","text":"Initialized()\n\nReturns true if MPI.Init has been called, false otherwise.\n\nIt is unaffected by MPI.Finalize, and is one of the few functions that may be called before MPI.Init.\n\nExternal links\n\nMPI_Intialized man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Finalize","page":"Environment","title":"MPI.Finalize","text":"Finalize()\n\nMarks MPI state for cleanup. This should be called after Init, at most once, and no further MPI calls (other than Initialized or Finalized) should be made after it is called.\n\nNote that this does not correspond exactly to MPI_FINALIZE in the MPI specification. In particular:\n\nIt may not finalize MPI immediately. Julia will wait until all MPI-related objects are garbage collected before finalizing MPI. As a result, Finalized() may return false after Finalize() has been called. See Finalizers for more details.\nIt is optional: Init will automatically insert a hook to finalize MPI when Julia exits.\n\nExternal links\n\nMPI_Finalize man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Finalized","page":"Environment","title":"MPI.Finalized","text":"Finalized()\n\nReturns true if MPI.Finalize has completed, false otherwise.\n\nIt is safe to call before MPI.Init and after MPI.Finalize.\n\nExternal links\n\nMPI_Finalized man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.refcount_inc","page":"Environment","title":"MPI.refcount_inc","text":"refcount_inc()\n\nIncrement the MPI reference counter. This should be called at initialization of any object which calls an MPI routine in its finalizer. A matching refcount_dec should be added to the finalizer.\n\nFor more details, see Finalizers.\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.refcount_dec","page":"Environment","title":"MPI.refcount_dec","text":"refcount_dec()\n\nDecrement the MPI reference counter. This should be added after an MPI call in an object finalizer, with a matching refcount_inc when the object is initialized.\n\nFor more details, see Finalizers.\n\n\n\n\n\n","category":"function"},{"location":"comm/#Communicators-1","page":"Communicators","title":"Communicators","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"An MPI communicator specifies the communication context for a communication operation. In particular, it specifies the set of processes which share the context, and assigns each each process a unique rank (see MPI.Comm_rank) taking an integer value in 0:n-1, where n is the number of processes in the communicator (see MPI.Comm_size.","category":"page"},{"location":"comm/#Types-1","page":"Communicators","title":"Types","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.Comm","category":"page"},{"location":"comm/#MPI.Comm","page":"Communicators","title":"MPI.Comm","text":"MPI.Comm\n\nAn MPI Communicator object.\n\n\n\n\n\n","category":"type"},{"location":"comm/#Constants-1","page":"Communicators","title":"Constants","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.COMM_WORLD\nMPI.COMM_SELF","category":"page"},{"location":"comm/#MPI.COMM_WORLD","page":"Communicators","title":"MPI.COMM_WORLD","text":"MPI.COMM_WORLD\n\nA communicator containing all processes with which the local rank can communicate at initialization. In a typical \"static-process\" model, this will be all processes.\n\n\n\n\n\n","category":"constant"},{"location":"comm/#MPI.COMM_SELF","page":"Communicators","title":"MPI.COMM_SELF","text":"MPI.COMM_SELF\n\nA communicator containing only the local process.\n\n\n\n\n\n","category":"constant"},{"location":"comm/#Functions-1","page":"Communicators","title":"Functions","text":"","category":"section"},{"location":"comm/#Accessors-1","page":"Communicators","title":"Accessors","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.Comm_size\nMPI.Comm_rank","category":"page"},{"location":"comm/#MPI.Comm_size","page":"Communicators","title":"MPI.Comm_size","text":"Comm_size(comm::Comm)\n\nThe number of processes involved in communicator.\n\nSee also\n\nMPI.Comm_rank.\n\nExternal links\n\nMPI_Comm_size man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_rank","page":"Communicators","title":"MPI.Comm_rank","text":"Comm_rank(comm::Comm)\n\nThe rank of the process in the particular communicator's group.\n\nReturns an integer in the range 0:MPI.Comm_size()-1.\n\nSee also\n\nMPI.Comm_size.\n\nExternal links\n\nMPI_Comm_rank man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#Constructors-1","page":"Communicators","title":"Constructors","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.Comm_dup\nMPI.Comm_get_parent\nMPI.Comm_spawn\nMPI.Comm_split\nMPI.Comm_split_type\nMPI.Intercomm_merge","category":"page"},{"location":"comm/#MPI.Comm_dup","page":"Communicators","title":"MPI.Comm_dup","text":"Comm_dup(comm::Comm)\n\nExternal links\n\nMPI_Comm_dup man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_get_parent","page":"Communicators","title":"MPI.Comm_get_parent","text":"Comm_get_parent()\n\nExternal links\n\nMPI_Comm_get_parent man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_spawn","page":"Communicators","title":"MPI.Comm_spawn","text":"Comm_spawn(command, argv::Vector{String}, nprocs::Integer, comm::Comm[, errors::Vector{Cint}]; kwargs...)\n\nExternal links\n\nMPI_Comm_spawn man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_split","page":"Communicators","title":"MPI.Comm_split","text":"Comm_split(comm::Comm, color::Integer, key::Integer)\n\nExternal links\n\nMPI_Comm_split man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_split_type","page":"Communicators","title":"MPI.Comm_split_type","text":"Comm_split_type(comm::Comm, split_type::Integer, key::Integer; kwargs...)\n\nExternal links\n\nMPI_Comm_split_type man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Intercomm_merge","page":"Communicators","title":"MPI.Intercomm_merge","text":"Intercomm_merge(intercomm::Comm, flag::Bool)\n\nExternal links\n\nMPI_Intercomm_merge man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#Miscellaneous-1","page":"Communicators","title":"Miscellaneous","text":"","category":"section"},{"location":"comm/#","page":"Communicators","title":"Communicators","text":"MPI.universe_size","category":"page"},{"location":"examples/04-sendrecv/#Send/receive-1","page":"Send/receive","title":"Send/receive","text":"","category":"section"},{"location":"examples/04-sendrecv/#","page":"Send/receive","title":"Send/receive","text":"# examples/04-sendrecv.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nsize = MPI.Comm_size(comm)\n\ndst = mod(rank+1, size)\nsrc = mod(rank-1, size)\n\nN = 4\n\nsend_mesg = Array{Float64}(undef, N)\nrecv_mesg = Array{Float64}(undef, N)\n\nfill!(send_mesg, Float64(rank))\n\nrreq = MPI.Irecv!(recv_mesg, src,  src+32, comm)\n\nprint(\"$rank: Sending   $rank -> $dst = $send_mesg\\n\")\nsreq = MPI.Isend(send_mesg, dst, rank+32, comm)\n\nstats = MPI.Waitall!([rreq, sreq])\n\nprint(\"$rank: Received $src -> $rank = $recv_mesg\\n\")\n\nMPI.Barrier(comm)","category":"page"},{"location":"examples/04-sendrecv/#","page":"Send/receive","title":"Send/receive","text":"> mpiexec -n 3 julia examples/04-sendrecv.jl\n1: Sending   1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n2: Sending   2 -> 0 = [2.0, 2.0, 2.0, 2.0]\n0: Sending   0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n1: Received 0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n2: Received 1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n0: Received 2 -> 0 = [2.0, 2.0, 2.0, 2.0]","category":"page"},{"location":"installation/#Installation-1","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Requirements-1","page":"Installation","title":"Requirements","text":"","category":"section"},{"location":"installation/#Unix-systems-(Linux-and-MacOS)-1","page":"Installation","title":"Unix systems (Linux and MacOS)","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"MPI.jl requires:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"A shared library MPI installation for C (supporting MPI standard 3.0), and\nA C compiler available via the mpicc command: this is required as part of the build process to determine the necessary type definitions and constants.","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"This has been tested with:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Open MPI\nMPICH\nIntel MPI","category":"page"},{"location":"installation/#Windows-1","page":"Installation","title":"Windows","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"MPI.jl requires the Microsoft MPI (MS-MPI) runtime to be installed.","category":"page"},{"location":"installation/#Building-1","page":"Installation","title":"Building","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"The MPI.jl package can be installed via add MPI in the Julia package manager. ","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"The build script will attempt to find the shared library and constants: this can be controlled with the optional environment variables:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"JULIA_MPI_PATH: the top-level installation directory of MPI.\nJULIA_MPI_LIBRARY: the path of the MPI shared library.\nJULIA_MPI_LIBRARY_PATH: the directory containing the MPI library files.\nJULIA_MPI_INCLUDE_PATH: the directory containing the MPI header files.\nJULIA_MPI_CFLAGS: C flags passed to the constant generation build (default: -lmpi)\nJULIA_MPICC: MPI C compiler (default: mpicc)\nJULIA_MPIEXEC: MPI startup command (default: mpiexec)","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"If your MPI installation changes (e.g. it is upgraded by the system, or you switch libraries), you will need to re-run build MPI at the package prompt.","category":"page"},{"location":"functions/#Function-reference-1","page":"Function reference","title":"Function reference","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"The following functions are currently wrapped, with the convention: MPI_Fun => MPI.Fun","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Constants like MPI_SUM are wrapped as MPI.SUM.   Note also that arbitrary Julia functions f(x,y) can be passed as reduction operations to the MPI Allreduce and Reduce functions.","category":"page"},{"location":"functions/#Datatype-functions-1","page":"Function reference","title":"Datatype functions","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia Function (assuming import MPI) Fortran Function\nMPI.Get_address MPI_Get_address\nMPI.mpitype MPI_Type_create_struct/MPI_Type_commit","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"note: Note\nmpitype is not strictly a wrapper for MPI_Type_create_struct and MPI_Type_commit, it also is an accessor for previously created types.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Get_address\nMPI.mpitype","category":"page"},{"location":"functions/#Collective-communication-1","page":"Function reference","title":"Collective communication","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Non-Allocating Julia Function Allocating Julia Function Fortran Function Supports MPI_IN_PLACE\nMPI.Allgather! MPI.Allgather MPI_Allgather ✅\nMPI.Allgatherv! MPI.Allgatherv MPI_Allgatherv ✅\nMPI.Allreduce! MPI.Allreduce MPI_Allreduce ✅\nMPI.Alltoall! MPI.Alltoall MPI_Alltoall ✅\nMPI.Alltoallv! MPI.Alltoallv MPI_Alltoallv ❌\n– MPI.Barrier MPI_Barrier ❌\nMPI.Bcast! MPI.Bcast! MPI_Bcast ❌\n– MPI.Exscan MPI_Exscan ❌\nMPI.Gather! MPI.Gather MPI_Gather Gather_in_place!\nMPI.Gatherv! MPI.Gatherv MPI_Gatherv Gatherv_in_place!\nMPI.Reduce! MPI.Reduce MPI_Reduce Reduce_in_place!\nMPI.Scan MPI.Scan MPI_Scan missing\nMPI.Scatter! MPI.Scatter MPI_Scatter Scatter_in_place!\nMPI.Scatterv! MPI.Scatterv MPI_Scatterv Scatterv_in_place!","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"The non-allocating Julia functions map directly to the corresponding MPI operations, after asserting that the size of the output buffer is sufficient to store the result.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"The allocating Julia functions allocate an output buffer and then call the non-allocating method.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"All-to-all collective communications support in place operations by passing MPI.IN_PLACE with the same syntax documented by MPI. One-to-All communications support it by calling the function *_in_place!, calls the MPI functions with the right syntax on root and non root process.","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Allgather!\nMPI.Allgather\nMPI.Allgatherv!\nMPI.Allgatherv\nMPI.Allreduce!\nMPI.Allreduce\nMPI.Alltoall!\nMPI.Alltoall\nMPI.Alltoallv!\nMPI.Alltoallv\nMPI.Barrier\nMPI.Bcast!\nMPI.Bcast!\nMPI.Exscan\nMPI.Gather!\nMPI.Gather\nMPI.Gather_in_place!\nMPI.Gatherv!\nMPI.Gatherv\nMPI.Gatherv_in_place!\nMPI.Reduce!\nMPI.Reduce\nMPI.Reduce_in_place!\nMPI.Scan\nMPI.Scan\nMPI.Scatter!\nMPI.Scatter\nMPI.Scatter_in_place!\nMPI.Scatterv!\nMPI.Scatterv\nMPI.Scatterv_in_place!","category":"page"},{"location":"functions/#MPI.Allgather!","page":"Function reference","title":"MPI.Allgather!","text":"Allgather!(sendbuf, recvbuf, count, comm)\n\nEach process sends the first count elements of sendbuf to the other processes, who store the results in rank order into recvbuf.\n\nIf sendbuf==MPI.IN_PLACE the input data is assumed to be in the area of recvbuf where the process would receive it's own contribution.\n\n\n\n\n\nAllgather!(buf, count, comm)\n\nEquivalent to Allgather!(MPI.IN_PLACE, buf, count, comm).\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allgather","page":"Function reference","title":"MPI.Allgather","text":"Allgather(sendbuf[, count=length(sendbuf)], comm)\n\nEach process sends the first count elements of sendbuf to the other processes, who store the results in rank order allocating the output buffer.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allgatherv!","page":"Function reference","title":"MPI.Allgatherv!","text":"Allgatherv!(sendbuf, recvbuf, counts, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to all other process. Each process stores the received data in rank order in the buffer recvbuf.\n\nif sendbuf==MPI.IN_PLACE every process takes the data to be sent is taken from the interval of recvbuf where it would store it's own data.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allgatherv","page":"Function reference","title":"MPI.Allgatherv","text":"Allgatherv(sendbuf, counts, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to all other process. Each process allocates an output buffer and stores the received data in rank order.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allreduce!","page":"Function reference","title":"MPI.Allreduce!","text":"Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)\n\nPerforms op reduction on the first count elements of the buffer sendbuf storing the result in the recvbuf of all processes in the group.\n\nAll-reduce is equivalent to a Reduce! operation followed by a Bcast!, but can lead to better performance.\n\nIf sendbuf==MPI.IN_PLACE the data is read from recvbuf and then overwritten with the results.\n\nTo handle allocation of the output buffer, see Allreduce.\n\n\n\n\n\nAllreduce!(buf, op, comm)\n\nPerforms op reduction in place on the buffer sendbuf, overwriting it with the results on all the processes in the group.\n\nEquivalent to calling Allreduce!(MPI.IN_PLACE, buf, op, comm)\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Allreduce","page":"Function reference","title":"MPI.Allreduce","text":"Allreduce(sendbuf, op, comm)\n\nPerforms op reduction on the buffer sendbuf, allocating and returning the output buffer in all processes of the group.\n\nTo specify the output buffer or perform the operation in pace, see Allreduce!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Alltoall!","page":"Function reference","title":"MPI.Alltoall!","text":"Alltoall!(sendbuf, recvbuf, count, comm)\n\nEvery process divides the buffer sendbuf into Comm_size(comm) chunks of length count, sending the j-th chunk to the j-th process. Every process stores the data received from the j-th process in the j-th chunk of the buffer recvbuf.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\nIf sendbuf==MPI.IN_PLACE, data is sent from the recvbuf and then overwritten.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Alltoall","page":"Function reference","title":"MPI.Alltoall","text":"Alltoall(sendbuf, count, comm)\n\nEvery process divides the buffer sendbuf into Comm_size(comm) chunks of length count, sending the j-th chunk to the j-th process. Every process allocates the output buffer and stores the data received from the j-th process in the j-th chunk.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Alltoallv!","page":"Function reference","title":"MPI.Alltoallv!","text":"Alltoallv!(sendbuf::T, recvbuf::T, scounts, rcounts, comm)\n\nMPI.IN_PLACE is not supported for this operation.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Barrier","page":"Function reference","title":"MPI.Barrier","text":"Barrier(comm::Comm)\n\nBlocks until comm is synchronized.\n\nIf comm is an intracommunicator, then it blocks until all members of the group have called it.\n\nIf comm is an intercommunicator, then it blocks until all members of the other group have called it.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Bcast!","page":"Function reference","title":"MPI.Bcast!","text":"Bcast!(buf[, count=length(buf)], root, comm::Comm)\n\nBroadcast the first count elements of the buffer buf from root to all processes.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gather!","page":"Function reference","title":"MPI.Gather!","text":"Gather!(sendbuf, recvbuf, count, root, comm)\n\nEach process sends the first count elements of the buffer sendbuf to the root process. The root process stores elements in rank order in the buffer buffer recvbuf.\n\ncount should be the same for all processes. If the number of elements varies between processes, use Gatherv! instead.\n\nTo perform the reduction in place refer to Gather_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gather","page":"Function reference","title":"MPI.Gather","text":"Gather(sendbuf[, count=length(sendbuf)], root, comm)\n\nEach process sends the first count elements of the buffer sendbuf to the root process. The root allocates the output buffer and stores elements in rank order.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gather_in_place!","page":"Function reference","title":"MPI.Gather_in_place!","text":"Gather_in_place!(buf, count, root, comm)\n\nEach process sends the first count elements of the buffer buf to the root process. The root process stores elements in rank order in the buffer buffer buf, sending no data to itself.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Gather!(MPI.IN_PLACE, buf, count, root, comm)\nelse\n    Gather!(buf, C_NULL, count, root, comm)\nend\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gatherv!","page":"Function reference","title":"MPI.Gatherv!","text":"Gatherv!(sendbuf, recvbuf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to the root process. The root stores elements in rank order in the buffer recvbuf.\n\nTo perform the reduction in place refer to Gatherv_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gatherv","page":"Function reference","title":"MPI.Gatherv","text":"Gatherv(sendbuf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer sendbuf to the root process. The root allocates the output buffer and stores elements in rank order.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Gatherv_in_place!","page":"Function reference","title":"MPI.Gatherv_in_place!","text":"Gatherv_in_place!(buf, counts, root, comm)\n\nEach process sends the first counts[rank] elements of the buffer buf to the root process. The root allocates the output buffer and stores elements in rank order.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Gatherv!(MPI.IN_PLACE, buf, counts, root, comm)\nelse\n    Gatherv!(buf, C_NULL, counts, root, comm)\nend\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Reduce!","page":"Function reference","title":"MPI.Reduce!","text":"Reduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, root, comm)\n\nPerforms op reduction on the first count elements of the  buffer sendbuf and stores the result in recvbuf on the process of rank root.\n\nOn non-root processes recvbuf is ignored.\n\nTo perform the reduction in place, see Reduce_in_place!.\n\nTo handle allocation of the output buffer, see Reduce.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Reduce","page":"Function reference","title":"MPI.Reduce","text":"Reduce(sendbuf, count, op, root, comm)\n\nPerforms op reduction on the buffer sendbuf and stores the result in an output buffer allocated on the process of rank root. An empty array will be returned on all other processes.\n\nTo specify the output buffer, see Reduce!.\n\nTo perform the reduction in place, see Reduce_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Reduce_in_place!","page":"Function reference","title":"MPI.Reduce_in_place!","text":"Reduce_in_place!(buf, count, op, root, comm)\n\nPerforms op reduction on the first count elements of the buffer buf and stores the result on buf of the root process in the group.\n\nThis is equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Reduce!(MPI.IN_PLACE, buf, count, root, comm)\nelse\n    Reduce!(buf, C_NULL, count, root, comm)\nend\n\nTo handle allocation of the output buffer, see Reduce.\n\nTo specify a separate output buffer, see Reduce!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatter!","page":"Function reference","title":"MPI.Scatter!","text":"Scatter!(sendbuf, recvbuf, count, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks and sends the j-th chunk to the process of rank j into the recvbuf buffer, which must be of length at least count.\n\ncount should be the same for all processes. If the number of elements varies between processes, use Scatter! instead.\n\nTo perform the reduction in place, see Scatter_in_place!.\n\nTo handle allocation of the output buffer, see Scatter.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatter","page":"Function reference","title":"MPI.Scatter","text":"Scatter(sendbuf, count, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks and sends the j-th chunk to the process of rank j, allocating the output buffer.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatter_in_place!","page":"Function reference","title":"MPI.Scatter_in_place!","text":"Scatter_in_place!(buf, count, root, comm)\n\nSplits the buffer buf in the root process into Comm_size(comm) chunks and sends the j-th chunk to the process of rank j. No data is sent to the root process.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Scatter!(buf, MPI.IN_PLACE, count, root, comm)\nelse\n    Scatter!(C_NULL, buf, count, root, comm)\nend\n\nTo specify a separate output buffer, see Scatter!.\n\nTo handle allocation of the output buffer, see Scatter.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatterv!","page":"Function reference","title":"MPI.Scatterv!","text":"Scatterv!(sendbuf, recvbuf, counts, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j into the recvbuf buffer, which must be of length at least count.\n\nTo perform the reduction in place refer to Scatterv_in_place!.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatterv","page":"Function reference","title":"MPI.Scatterv","text":"Scatterv(sendbuf, counts, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j, which allocates the output buffer\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Scatterv_in_place!","page":"Function reference","title":"MPI.Scatterv_in_place!","text":"Scatterv_in_place(buf, counts, root, comm)\n\nSplits the buffer buf in the root process into Comm_size(comm) chunks of length counts[j] and sends the j-th chunk to the process of rank j into the buf buffer, which must be of length at least count. The root process sends nothing to itself.\n\nThis is functionally equivalent to calling\n\nif root == MPI.Comm_rank(comm)\n    Scatterv!(buf, MPI.IN_PLACE, counts, root, comm)\nelse\n    Scatterv!(C_NULL, buf, counts, root, comm)\nend\n\n\n\n\n\n","category":"function"},{"location":"functions/#One-sided-communication-1","page":"Function reference","title":"One-sided communication","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"Julia Function (assuming import MPI) Fortran Function\nMPI.Win_create MPI_Win_create\nMPI.Win_create_dynamic MPI_Win_create_dynamic\nMPI.Win_allocate_shared MPI_Win_allocate_shared\nMPI.Win_shared_query MPI_Win_shared_query\nMPI.Win_attach MPI_Win_attach\nMPI.Win_detach MPI_Win_detach\nMPI.Win_fence MPI_Win_fence\nMPI.Win_flush MPI_Win_flush\nMPI.Win_free MPI_Win_free\nMPI.Win_sync MPI_Win_sync\nMPI.Win_lock MPI_Win_lock\nMPI.Win_unlock MPI_Win_unlock\nMPI.Get MPI_Get\nMPI.Put MPI_Put\nMPI.Fetch_and_op MPI_Fetch_and_op\nMPI.Accumulate MPI_Accumulate\nMPI.Get_accumulate MPI_Get_accumulate","category":"page"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Win_create\nMPI.Win_create_dynamic\nMPI.Win_allocate_shared\nMPI.Win_shared_query\nMPI.Win_attach\nMPI.Win_detach\nMPI.Win_fence\nMPI.Win_flush\nMPI.Win_free\nMPI.Win_sync\nMPI.Win_lock\nMPI.Win_unlock\nMPI.Get\nMPI.Put\nMPI.Fetch_and_op\nMPI.Accumulate\nMPI.Get_accumulate","category":"page"},{"location":"functions/#MPI.Win_create","page":"Function reference","title":"MPI.Win_create","text":"MPI.Win_create(base::Array, comm::Comm; infokws...)\n\nCreate a window over the array base, returning a Win object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Win_create_dynamic","page":"Function reference","title":"MPI.Win_create_dynamic","text":"MPI.Win_create_dynamic(comm::Comm; infokws...)\n\nCreate a dynamic window returning a Win object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"functions/#MPI.Win_allocate_shared","page":"Function reference","title":"MPI.Win_allocate_shared","text":"(win, ptr) = MPI.Win_allocate_shared(T, len, comm::Comm; infokws...)\n\nCreate and allocate a shared memory window for objects of type T of length len, returning a Win and a Ptr{T} object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Info-objects-1","page":"Function reference","title":"Info objects","text":"","category":"section"},{"location":"functions/#","page":"Function reference","title":"Function reference","text":"MPI.Info\nMPI.infoval","category":"page"},{"location":"functions/#MPI.Info","page":"Function reference","title":"MPI.Info","text":"Info <: AbstractDict{Symbol,String}\n\nMPI.Info objects store key-value pairs, and are typically used for passing optional arguments to MPI functions.\n\nUsage\n\nThese will typically be hidden from user-facing APIs by splatting keywords, e.g.\n\nfunction f(args...; kwargs...)\n    info = Info(kwargs...)\n    # pass `info` object to `ccall`\nend\n\nFor manual usage, Info objects act like Julia Dict objects:\n\ninfo = Info(init=true) # keyword argument is required\ninfo[key] = value\nx = info[key]\ndelete!(info, key)\n\nIf init=false is used in the costructor (the default), a \"null\" Info object will be returned: no keys can be added to such an object.\n\n\n\n\n\n","category":"type"},{"location":"functions/#MPI.infoval","page":"Function reference","title":"MPI.infoval","text":"infoval(x)\n\nConvert Julia object x to a string representation for storing in an Info object.\n\nThe MPI specification allows passing strings, Boolean values, integers, and lists.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Point-to-point-communication-1","page":"Point-to-point communication","title":"Point-to-point communication","text":"","category":"section"},{"location":"pointtopoint/#Types-1","page":"Point-to-point communication","title":"Types","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Request\nMPI.Status","category":"page"},{"location":"pointtopoint/#MPI.Request","page":"Point-to-point communication","title":"MPI.Request","text":"MPI.Request\n\nAn MPI Request object, representing a non-blocking communication. This also contains a reference to the buffer used in the communication to ensure it isn't garbage-collected during communication.\n\nThe status of a Request can be checked by the Wait! and Test! functions or their multiple-request variants, which will deallocate the request once it is determined to be complete. Alternatively, it will be deallocated at finalization, meaning that it is safe to ignore the request objects if the status of the communication can be checked by other means.\n\nSee also Cancel!.\n\n\n\n\n\n","category":"type"},{"location":"pointtopoint/#MPI.Status","page":"Point-to-point communication","title":"MPI.Status","text":"MPI.Status\n\nThe status of an MPI receive communication. It has 3 accessible fields\n\nsource: source of the received message\ntag: tag of the received message\nerror: error code. This is only set if a function returns multiple statuses.\n\nAdditionally, the accessor function MPI.Get_count can be used to determine the number of entries received.\n\n\n\n\n\n","category":"type"},{"location":"pointtopoint/#Functions-1","page":"Point-to-point communication","title":"Functions","text":"","category":"section"},{"location":"pointtopoint/#Accessor-1","page":"Point-to-point communication","title":"Accessor","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Get_count","category":"page"},{"location":"pointtopoint/#MPI.Get_count","page":"Point-to-point communication","title":"MPI.Get_count","text":"MPI.Get_count(status::Status, T)\n\nThe number of entries received. T should match the argument provided by the receive call that set the status variable.\n\nIf the number of entries received exceeds the limits of the count parameter, then it returns MPI_UNDEFINED.\n\nExternal links\n\nMPI_Get_count man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Blocking-1","page":"Point-to-point communication","title":"Blocking","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Send\nMPI.Recv!\nMPI.Sendrecv!","category":"page"},{"location":"pointtopoint/#MPI.Send","page":"Point-to-point communication","title":"MPI.Send","text":"Send(buf, [count::Integer, [datatype::Datatype,]]\n     dest::Integer, tag::Integer, comm::Comm) where T\n\nPerform a blocking send of count elements of type datatype from buf to MPI rank dest of communicator comm using the message tag tag\n\nIf not provided, datatype and count are derived from the element type and length of buf, respectively.\n\nExternal links\n\nMPI_Send man page: OpenMPI, MPICH\n\n\n\n\n\nSend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T\n\nComplete a blocking send of obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Recv!","page":"Point-to-point communication","title":"MPI.Recv!","text":"Recv!(buf, [count::Integer, [datatype::Datatype,]]\n      src::Integer, tag::Integer, comm::Comm)\n\nCompletes a blocking receive of up to count elements of type datatype into buf from MPI rank src of communicator comm using with the message tag tag\n\nIf not provided, datatype and count are derived from the element type and length of buf, respectively.\n\nReturns the Status of the receive.\n\nExternal links\n\nMPI_Recv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Sendrecv!","page":"Point-to-point communication","title":"MPI.Sendrecv!","text":"Sendrecv!(sendbuf, [sendcount::Integer, [sendtype::Union{Datatype, MPI_Datatype}]],\n         dest::Integer, sendtag::Integer,\n         recvbuf, [recvcount::Integer, [recvtype::Union{Datatype, MPI_Datatype}]],\n         source::Integer, recvtag::Integer,\n         comm::Comm)\n\nComplete a blocking send-receive operation over the MPI communicator comm. Send sendcount elements of type sendtype from sendbuf to the MPI rank dest using message tag tag, and receive recvcount elements of type recvtype from MPI rank source into the buffer recvbuf using message tag tag. Return a Status object.\n\nIf not provided, sendtype/recvtype and sendcount/recvcount are derived from the element type and length of sendbuf/recvbuf, respectively.\n\nExternal links\n\nMPI_Sendrecv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Non-blocking-1","page":"Point-to-point communication","title":"Non-blocking","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Isend\nMPI.Irecv!\nMPI.Test!\nMPI.Testall!\nMPI.Testany!\nMPI.Testsome!\nMPI.Wait!\nMPI.Waitall!\nMPI.Waitany!\nMPI.Waitsome!","category":"page"},{"location":"pointtopoint/#MPI.Isend","page":"Point-to-point communication","title":"MPI.Isend","text":"Isend(buf, [count::Integer, [datatype::Datatype,]]\n      dest::Integer, tag::Integer, comm::Comm)\n\nStarts a nonblocking send of count elements of type datatype from buf to MPI rank dest of communicator comm using with the message tag tag\n\nIf not provided, datatype and count are derived from the element type and length of buf, respectively.\n\nReturns the Request object for the nonblocking send.\n\nExternal links\n\nMPI_Isend man page: OpenMPI, MPICH\n\n\n\n\n\nIsend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking send of obj to MPI rank dest of communicator comm using with the message tag tag.\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Irecv!","page":"Point-to-point communication","title":"MPI.Irecv!","text":"Irecv!(buf, [count::Integer, [datatype::Datatype,]]\n       src::Integer, tag::Integer, comm::Comm) where T\n\nStarts a nonblocking receive of up to count elements of type datatype into buf from MPI rank src of communicator comm using with the message tag tag\n\nReturns the Request for the nonblocking receive.\n\nExternal links\n\nMPI_Irecv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Test!","page":"Point-to-point communication","title":"MPI.Test!","text":"(flag, status) = Test!(req::Request)\n\nCheck if the request req is complete. If so, the request is deallocated and flag is returned true and status as the Status of the request. Otherwise flag is returned false and status is nothing.\n\nExternal links\n\nMPI_Test man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testall!","page":"Point-to-point communication","title":"MPI.Testall!","text":"(flag, statuses) = Testall!(reqs::Vector{Request})\n\nCheck if all active requests in the array reqs are complete. If so, the requests are deallocated and flag is returned as true and statuses is an array of the Status objects corresponding to each request is returned. Otherwise no requests are modified a tuple of false, nothing is returned.\n\nExternal links\n\nMPI_Testall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testany!","page":"Point-to-point communication","title":"MPI.Testany!","text":"(flag, index, status) = Testany!(reqs::Vector{Request})\n\nCheck if any one of the requests in the array reqs is complete.\n\nIf one or more requests are complete, then one is chosen arbitrarily, deallocated and flag is returned as true, along with the index and the Status of the request.\n\nOtherwise, if there are no complete requests, then index is returned as 0, status as nothing, and flag as true if there are no active requests and false otherwise. \n\nExternal links\n\nMPI_Testany man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testsome!","page":"Point-to-point communication","title":"MPI.Testsome!","text":"(indices, statuses) = Testsome!(reqs::Vector{Request})\n\nSimilar to Waitsome! except that it returns immediately: if no operations have completed then indices and statuses will be empty.\n\nExternal links\n\nMPI_Testsome man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Wait!","page":"Point-to-point communication","title":"MPI.Wait!","text":"status = Wait!(req::Request)\n\nBlock until the request req is complete and deallocated. Returns the Status of the request.\n\nExternal links\n\nMPI_Wait man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitall!","page":"Point-to-point communication","title":"MPI.Waitall!","text":"statuses = Waitall!(reqs::Vector{Request})\n\nBlock until all active requests in the array reqs are complete. Returns an array of the Status objects corresponding to each request.\n\nExternal links\n\nMPI_Waitall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitany!","page":"Point-to-point communication","title":"MPI.Waitany!","text":"(index, status) = Waitany!(reqs::Vector{Request})\n\nBlocks until one of the requests in the array reqs is complete: if more than one is complete, one is chosen arbitrarily. The request is deallocated and a tuple of the index of the completed request and its Status is returned. If there are no active requests, then index is returned as 0.\n\nExternal links\n\nMPI_Waitany man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitsome!","page":"Point-to-point communication","title":"MPI.Waitsome!","text":"(indices, statuses) = Waitsome!(reqs::Vector{Request})\n\nBlock until at least one of the active requests in the array reqs is complete. The completed requests are deallocated, and a tuple of their indices in reqs and their corresponding Status objects are returned. If there are no active requests, then the function returns immediately and indices and statuses are empty.\n\nExternal links\n\nMPI_Waitsome man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Probe-and-cancel-1","page":"Point-to-point communication","title":"Probe and cancel","text":"","category":"section"},{"location":"pointtopoint/#","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Iprobe\nMPI.Probe\nMPI.Cancel!","category":"page"},{"location":"pointtopoint/#MPI.Iprobe","page":"Point-to-point communication","title":"MPI.Iprobe","text":"ismessage, (status|nothing) = Iprobe(src::Integer, tag::Integer, comm::Comm)\n\nChecks if there is a message that can be received matching src, tag and comm. If so, returns a tuple true and a Status object, otherwise returns a tuple false, nothing.\n\nExternal links\n\nMPI_Iprobe man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Probe","page":"Point-to-point communication","title":"MPI.Probe","text":"status = Probe(src::Integer, tag::Integer, comm::Comm)\n\nBlocks until there is a message that can be received matching src, tag and comm. Returns the corresponding Status object.\n\nExternal links\n\nMPI_Probe man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Cancel!","page":"Point-to-point communication","title":"MPI.Cancel!","text":"Cancel!(req::Request)\n\nMarks a pending Irecv! operation for cancellation (cancelling a Isend, while supported in some implementations, is deprecated as of MPI 3.1). Note that the request is not deallocated, and can still be queried using the test or wait functions.\n\nExternal links\n\nMPI_Cancel man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"examples/02-broadcast/#Broadcast-1","page":"Broadcast","title":"Broadcast","text":"","category":"section"},{"location":"examples/02-broadcast/#","page":"Broadcast","title":"Broadcast","text":"# examples/02-broadcast.jl\nimport MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nN = 5\nroot = 0\n\nif MPI.Comm_rank(comm) == root\n    print(\" Running on $(MPI.Comm_size(comm)) processes\\n\")\nend\nMPI.Barrier(comm)\n\nif MPI.Comm_rank(comm) == root\n    A = [i*(1.0 + im*2.0) for i = 1:N]\nelse\n    A = Array{ComplexF64}(undef, N)\nend\n\nMPI.Bcast!(A, root, comm)\n\nprint(\"rank = $(MPI.Comm_rank(comm)), A = $A\\n\")\n\nif MPI.Comm_rank(comm) == root\n    B = Dict(\"foo\" => \"bar\")\nelse\n    B = nothing\nend\n\nB = MPI.bcast(B, root, comm)\nprint(\"rank = $(MPI.Comm_rank(comm)), B = $B\\n\")\n\nif MPI.Comm_rank(comm) == root\n    f = x -> x^2 + 2x - 1\nelse\n    f = nothing\nend\n\nf = MPI.bcast(f, root, comm)\nprint(\"rank = $(MPI.Comm_rank(comm)), f(3) = $(f(3))\\n\")","category":"page"},{"location":"examples/02-broadcast/#","page":"Broadcast","title":"Broadcast","text":"> mpiexec -n 3 julia examples/02-broadcast.jl\n Running on 3 processes\nrank = 1, A = Complex{Float64}[1.0+2.0im, 2.0+4.0im, 3.0+6.0im, 4.0+8.0im, 5.0+10.0im]\nrank = 2, A = Complex{Float64}[1.0+2.0im, 2.0+4.0im, 3.0+6.0im, 4.0+8.0im, 5.0+10.0im]\nrank = 0, A = Complex{Float64}[1.0+2.0im, 2.0+4.0im, 3.0+6.0im, 4.0+8.0im, 5.0+10.0im]\nrank = 2, B = Dict(\"foo\"=>\"bar\")\nrank = 0, B = Dict(\"foo\"=>\"bar\")\nrank = 1, B = Dict(\"foo\"=>\"bar\")\nrank = 0, f(3) = 14\nrank = 2, f(3) = 14\nrank = 1, f(3) = 14","category":"page"},{"location":"examples/03-reduce/#Reduce-1","page":"Reduce","title":"Reduce","text":"","category":"section"},{"location":"examples/03-reduce/#","page":"Reduce","title":"Reduce","text":"# examples/03-reduce.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nroot = 0\n\nr = MPI.Comm_rank(comm)\n\nsr = MPI.Reduce(r, +, root, comm)\n\nif MPI.Comm_rank(comm) == root\n    println(\"sum of ranks = $sr\")\nend\n","category":"page"},{"location":"examples/03-reduce/#","page":"Reduce","title":"Reduce","text":"> mpiexec -n 3 julia examples/03-reduce.jl\nsum of ranks = 3","category":"page"},{"location":"usage/#Usage-1","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"MPI is based on a single program, multiple data (SPMD) model, where multiple processes are launched running independent programs, which then communicate as necessary via messages.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"A script should include using MPI and MPI.Init() statements, for example","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"# examples/01-hello.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nprintln(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\")\nMPI.Barrier(comm)","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"The program can then be launched via an MPI launch command (typically mpiexec, mpirun or srun), e.g.","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"$ mpiexec -n 3 julia --project examples/01-hello.jl\nHello world, I am rank 0 of 3\nHello world, I am rank 2 of 3\nHello world, I am rank 1 of 3","category":"page"},{"location":"usage/#CUDA-aware-MPI-support-1","page":"Usage","title":"CUDA-aware MPI support","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"If your MPI implementation has been compiled with CUDA support, then CuArrays (from the CuArrays.jl package) can be passed directly as send and receive buffers for point-to-point and collective operations (they may also work with one-sided operations, but these are not often supported).","category":"page"},{"location":"usage/#Finalizers-1","page":"Usage","title":"Finalizers","text":"","category":"section"},{"location":"usage/#","page":"Usage","title":"Usage","text":"In order to ensure MPI routines are called in the correct order at finalization time, MPI.jl maintains a reference count. If you define an object that needs to call an MPI routine during its finalization, you should call MPI.refcount_inc() when it is initialized, and MPI.refcount_dec() in its finalizer (after the relevant MPI call).","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"For example","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"mutable struct MyObject\n    ...\n    function MyObject(args...)\n        obj = new(args...)\n        # MPI call to create object\n        refcount_inc()\n        finalizer(obj) do x\n            # MPI call to free object\n            refcount_dec()\n        end\n        return obj\n    end\nend","category":"page"},{"location":"usage/#","page":"Usage","title":"Usage","text":"","category":"page"},{"location":"examples/01-hello/#Hello-world-1","page":"Hello world","title":"Hello world","text":"","category":"section"},{"location":"examples/01-hello/#","page":"Hello world","title":"Hello world","text":"# examples/01-hello.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nprint(\"Hello world, I am rank $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\\n\")\nMPI.Barrier(comm)","category":"page"},{"location":"examples/01-hello/#","page":"Hello world","title":"Hello world","text":"> mpiexec -n 3 julia examples/01-hello.jl\nHello world, I am rank 1 of 3\nHello world, I am rank 2 of 3\nHello world, I am rank 0 of 3","category":"page"},{"location":"#MPI.jl-1","page":"MPI.jl","title":"MPI.jl","text":"","category":"section"},{"location":"#","page":"MPI.jl","title":"MPI.jl","text":"This is a basic Julia wrapper for the portable message passing system Message Passing Interface (MPI). Inspiration is taken from mpi4py, although we generally follow the C and not the C++ MPI API. (The C++ MPI API is deprecated.)","category":"page"}]
}
