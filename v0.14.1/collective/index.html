<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Collective communication · MPI.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MPI.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">MPI.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">MPI.jl</a></li><li><a class="tocitem" href="../configuration/">Configuration</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><a class="tocitem" href="../knownissues/">Known issues</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/01-hello/">Hello world</a></li><li><a class="tocitem" href="../examples/02-broadcast/">Broadcast</a></li><li><a class="tocitem" href="../examples/03-reduce/">Reduce</a></li><li><a class="tocitem" href="../examples/04-sendrecv/">Send/receive</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../library/">Library information</a></li><li><a class="tocitem" href="../environment/">Environment</a></li><li><a class="tocitem" href="../comm/">Communicators</a></li><li><a class="tocitem" href="../pointtopoint/">Point-to-point communication</a></li><li class="is-active"><a class="tocitem" href>Collective communication</a><ul class="internal"><li><a class="tocitem" href="#Synchronization-1"><span>Synchronization</span></a></li><li><a class="tocitem" href="#Broadcast-1"><span>Broadcast</span></a></li><li><a class="tocitem" href="#Gather/Scatter-1"><span>Gather/Scatter</span></a></li><li><a class="tocitem" href="#Reduce/Scan-1"><span>Reduce/Scan</span></a></li></ul></li><li><a class="tocitem" href="../onesided/">One-sided communication</a></li><li><a class="tocitem" href="../topology/">Topology</a></li><li><a class="tocitem" href="../io/">I/O</a></li><li><a class="tocitem" href="../advanced/">Advanced</a></li></ul></li><li><a class="tocitem" href="../refindex/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Collective communication</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Collective communication</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/collective.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Collective-communication-1"><a class="docs-heading-anchor" href="#Collective-communication-1">Collective communication</a><a class="docs-heading-anchor-permalink" href="#Collective-communication-1" title="Permalink"></a></h1><h2 id="Synchronization-1"><a class="docs-heading-anchor" href="#Synchronization-1">Synchronization</a><a class="docs-heading-anchor-permalink" href="#Synchronization-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MPI.Barrier" href="#MPI.Barrier"><code>MPI.Barrier</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Barrier(comm::Comm)</code></pre><p>Blocks until <code>comm</code> is synchronized.</p><p>If <code>comm</code> is an intracommunicator, then it blocks until all members of the group have called it.</p><p>If <code>comm</code> is an intercommunicator, then it blocks until all members of the other group have called it.</p><p><strong>External links</strong></p><ul><li><code>MPI_Barrier</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Barrier.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L3-L14">source</a></section></article><h2 id="Broadcast-1"><a class="docs-heading-anchor" href="#Broadcast-1">Broadcast</a><a class="docs-heading-anchor-permalink" href="#Broadcast-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MPI.Bcast!" href="#MPI.Bcast!"><code>MPI.Bcast!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Bcast!(buf[, count=length(buf)], root::Integer, comm::Comm)</code></pre><p>Broadcast the first <code>count</code> elements of the buffer <code>buf</code> from <code>root</code> to all processes.</p><p><strong>External links</strong></p><ul><li><code>MPI_Bcast</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Bcast.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L21-L28">source</a></section></article><h2 id="Gather/Scatter-1"><a class="docs-heading-anchor" href="#Gather/Scatter-1">Gather/Scatter</a><a class="docs-heading-anchor-permalink" href="#Gather/Scatter-1" title="Permalink"></a></h2><h3 id="Gather-1"><a class="docs-heading-anchor" href="#Gather-1">Gather</a><a class="docs-heading-anchor-permalink" href="#Gather-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgather!" href="#MPI.Allgather!"><code>MPI.Allgather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgather!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], comm::Comm)
Allgather!(sendrecvbuf, count::Integer, comm::Comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order into <code>recvbuf</code>.</p><p>If only one buffer <code>sendrecvbuf</code> is provided, then each process send data is assumed to be in the area where it would receive it&#39;s own contribution.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allgather"><code>Allgather</code></a> for the allocating operation</li><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="#MPI.Allgatherv"><code>Allgatherv</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Gather!"><code>Gather!</code></a> to send only to a single root process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L277-L294">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgather" href="#MPI.Allgather"><code>MPI.Allgather</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgather(sendbuf[, count=length(sendbuf)], comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order allocating the output buffer.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allgather!"><code>Allgather!</code></a> for the mutating operation</li><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="#MPI.Allgatherv"><code>Allgatherv</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Gather!"><code>Gather!</code></a> to send only to a single root process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L313-L326">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgatherv!" href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgatherv!(sendbuf, recvbuf, counts, comm)
Allgatherv!(sendrecvbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process stores the received data in rank order in the buffer <code>recvbuf</code>.</p><p>If only one buffer <code>sendrecvbuf</code> is provided, then for each process, the data to be sent is taken from the interval of <code>recvbuf</code> where it would store it&#39;s own data.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allgatherv"><code>Allgatherv</code></a> for the allocating operation</li><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="#MPI.Gatherv"><code>Gatherv</code></a> to send the result to a single process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgatherv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L406-L423">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgatherv" href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allgatherv(sendbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process allocates an output buffer and stores the received data in rank order.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a> for the mutating operation.</li><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="#MPI.Gatherv"><code>Gatherv</code></a> to send the result to a single process.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgatherv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L444-L457">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather!" href="#MPI.Gather!"><code>MPI.Gather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gather!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], root::Integer, comm::Comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>recvbuf</code>.</p><p><code>sendbuf</code> can be <code>nothing</code> on the root process, in which case the corresponding entries in <code>recvbuf</code> are assumed to be already in place (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Gather</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gather!(nothing, buf, count, root, comm)
else
    Gather!(buf, nothing, count, root, comm)
end</code></pre><p><code>recvbuf</code> on the root process should be a buffer of length <code>count*Comm_size(comm)</code>, and on non-root processes it is ignored and can be <code>nothing</code>.</p><p><code>count</code> should be the same for all processes.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Gather"><code>Gather</code></a> for the allocating operation.</li><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Allgather!"><code>Allgather!</code></a> to send the result to all processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L199-L229">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather" href="#MPI.Gather"><code>MPI.Gather</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gather(sendbuf[, count=length(sendbuf)], root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Gather!"><code>Gather!</code></a> for the mutating operation.</li><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="#MPI.Gatherv"><code>Gatherv</code></a> if the number of elements varies between processes.</li><li><a href="#MPI.Allgather!"><code>Allgather!</code></a>/<a href="#MPI.Allgather"><code>Allgather</code></a> to send the result to all processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gather.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L253-L266">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gatherv!" href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gatherv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> stores elements in rank order in the buffer <code>recvbuf</code>.</p><p><code>sendbuf</code> can be <code>nothing</code> on the root process, in which case the corresponding entries in <code>recvbuf</code> are assumed to be already in place (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Gatherv</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gatherv!(nothing, buf, counts, root, comm)
else
    Gatherv!(buf, nothing, counts, root, comm)
end</code></pre><p><strong>See also</strong></p><ul><li><a href="#MPI.Gatherv"><code>Gatherv</code></a> for the allocating operation</li><li><a href="#MPI.Gather!"><code>Gather!</code></a></li><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="#MPI.Allgatherv"><code>Allgatherv</code></a> to send the result to all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gatherv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L337-L362">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Gatherv" href="#MPI.Gatherv"><code>MPI.Gatherv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Gatherv(sendbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Gatherv!"><code>Gatherv!</code></a> for the mutating operation</li><li><a href="#MPI.Gather!"><code>Gather!</code></a>/<a href="#MPI.Gather"><code>Gather</code></a></li><li><a href="#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="#MPI.Allgatherv"><code>Allgatherv</code></a> to send the result to all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gatherv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L386-L400">source</a></section></article><h3 id="Scatter-1"><a class="docs-heading-anchor" href="#Scatter-1">Scatter</a><a class="docs-heading-anchor-permalink" href="#Scatter-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatter!" href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatter!(sendbuf, recvbuf[, count=length(recvbuf)], root::Integer, comm::Comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>count</code>, and sends the <code>j</code>-th chunk to the process of rank <code>j</code> into the <code>recvbuf</code> buffer.</p><p><code>sendbuf</code> on the root process should be a buffer of length <code>count*Comm_size(comm)</code>, and on non-root processes it is ignored and can be <code>nothing</code>.</p><p><code>recvbuf</code> can be <code>nothing</code> on the root process, in which case it is unmodified (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Scatter</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatter!(buf, nothing, count, root, comm)
else
    Scatter!(nothing, buf, count, root, comm)        
end</code></pre><p><code>count</code> should be the same for all processes.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Scatter"><code>Scatter</code></a> to allocate the output buffer.</li><li><a href="#MPI.Scatterv!"><code>Scatterv!</code></a> if the number of elements varies between processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatter</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatter.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatter.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L62-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatter" href="#MPI.Scatter"><code>MPI.Scatter</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatter(sendbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the <code>j</code>-th chunk to the process of rank <code>j</code>, allocating the output buffer.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Scatter!"><code>Scatter!</code></a> for the mutating operation.</li><li><a href="#MPI.Scatterv!"><code>Scatterv!</code></a> if the number of elements varies between processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatter</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatter.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatter.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L114-L126">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatterv!" href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatterv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p><code>recvbuf</code> can be <code>nothing</code> on the root process, in which case it is unmodified (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Scatterv</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatterv!(buf, nothing, counts, root, comm)
else
    Scatterv!(nothing, buf, counts, root, comm)
end</code></pre><p><strong>See also</strong></p><ul><li><a href="#MPI.Scatterv"><code>Scatterv</code></a> for the allocating operation</li><li><a href="#MPI.Scatter!"><code>Scatter!</code></a>/<a href="#MPI.Scatter"><code>Scatter</code></a> if the counts are the same for all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatterv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatterv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatterv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L132-L155">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatterv" href="#MPI.Scatterv"><code>MPI.Scatterv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scatterv(sendbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j, which allocates the output buffer</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Scatterv!"><code>Scatterv!</code></a> for the mutating operation</li><li><a href="#MPI.Scatter!"><code>Scatter!</code></a>/<a href="#MPI.Scatter"><code>Scatter</code></a> if the counts are the same for all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatterv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatterv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatterv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L179-L192">source</a></section></article><h3 id="All-to-all-1"><a class="docs-heading-anchor" href="#All-to-all-1">All-to-all</a><a class="docs-heading-anchor-permalink" href="#All-to-all-1" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoall!" href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoall!(sendbuf, recvbuf, count::Integer, comm::Comm)
Alltoall!(sendrecvbuf, count::Integer, comm::Comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk of the buffer <code>recvbuf</code>.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p>If only one buffer <code>sendrecvbuf</code> then data is overwritten.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Alltoall"><code>Alltoall</code></a> for the allocating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoall</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoall.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoall.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L464-L488">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoall" href="#MPI.Alltoall"><code>MPI.Alltoall</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoall(sendbuf, count::Integer, comm::Comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process allocates the output buffer and stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p><strong>See also</strong></p><ul><li><a href="#MPI.Alltoall!"><code>Alltoall!</code></a> for the mutating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoall</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoall.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoall.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L507-L528">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoallv!" href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoallv!(sendbuf, recvbuf, scounts::Vector, rcounts::Vector, comm::Comm)</code></pre><p>Similar to <a href="#MPI.Alltoall!"><code>Alltoall!</code></a>, except with different size chunks per process.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Alltoallv"><code>Alltoallv</code></a> for the allocating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoallv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoallv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoallv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L534-L544">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Alltoallv" href="#MPI.Alltoallv"><code>MPI.Alltoallv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Alltoallv(sendbuf, recvbuf, scounts::Vector, rcounts::Vector, comm::Comm)</code></pre><p>Similar to <a href="#MPI.Alltoall"><code>Alltoall</code></a>, except with different size chunks per process.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Alltoallv!"><code>Alltoallv!</code></a> for the mutating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoallv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoallv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoallv.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L563-L573">source</a></section></article><h2 id="Reduce/Scan-1"><a class="docs-heading-anchor" href="#Reduce/Scan-1">Reduce/Scan</a><a class="docs-heading-anchor-permalink" href="#Reduce/Scan-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MPI.Reduce!" href="#MPI.Reduce!"><code>MPI.Reduce!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Reduce!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], op, root::Integer, comm::Comm)
Reduce!(sendrecvbuf, op, root::Integer, comm::Comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the first <code>count</code> elements of the buffer <code>sendbuf</code> and stores the result in <code>recvbuf</code> on the process of rank <code>root</code>.</p><p>On non-root processes <code>recvbuf</code> is ignored, and can be <code>nothing</code>. </p><p>To perform the reduction in place, provide a single buffer <code>sendrecvbuf</code>.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Reduce"><code>Reduce</code></a> to handle allocation of the output buffer.</li><li><a href="#MPI.Allreduce!"><code>Allreduce!</code></a>/<a href="#MPI.Allreduce"><code>Allreduce</code></a> to send reduction to all ranks.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Reduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L586-L604">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Reduce" href="#MPI.Reduce"><code>MPI.Reduce</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Reduce(sendbuf, op, root::Integer, comm::Comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>, returning the result <code>recvbuf</code> on the process of rank <code>root</code>, and <code>nothing</code> on non-root processes.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will be a value of the same type.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Reduce!"><code>Reduce!</code></a> for mutating and in-place operations</li><li><a href="#MPI.Allreduce!"><code>Allreduce!</code></a>/<a href="#MPI.Allreduce"><code>Allreduce</code></a> to send reduction to all ranks.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Reduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L641-L656">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allreduce!" href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)
Allreduce!(sendrecvbuf, op, comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the first <code>count</code> elements of the buffer <code>sendbuf</code>, storing the result in the <code>recvbuf</code> of all processes in the group.</p><p><code>Allreduce!</code> is equivalent to a <a href="#MPI.Reduce!"><code>Reduce!</code></a> operation followed by a <a href="#MPI.Bcast!"><code>Bcast!</code></a>, but can lead to better performance.</p><p>If only one <code>sendrecvbuf</code> buffer is provided, then the operation is performed in-place.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allreduce"><code>Allreduce</code></a>, to handle allocation of the output buffer.</li><li><a href="#MPI.Reduce!"><code>Reduce!</code></a>/<a href="#MPI.Reduce"><code>Reduce</code></a> to send reduction to a single rank.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allreduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allreduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allreduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L671-L690">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allreduce" href="#MPI.Allreduce"><code>MPI.Allreduce</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Allreduce(sendbuf, op, comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>, returning the result in the <code>recvbuf</code> of all processes in the group.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will be a value of the same type.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Allreduce!"><code>Allreduce!</code></a> for mutating or in-place operations.</li><li><a href="#MPI.Reduce!"><code>Reduce!</code></a>/<a href="#MPI.Reduce"><code>Reduce</code></a> to send reduction to a single rank.</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allreduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allreduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allreduce.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L717-L732">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scan!" href="#MPI.Scan!"><code>MPI.Scan!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Scan!(sendbuf, recvbuf[, count::Integer], op, comm::Comm)
Scan!(buf[, count::Integer], op, comm::Comm)</code></pre><p>Inclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i</code>.</p><p>If only a single buffer is provided, then operations will be performed in-place in <code>buf</code>.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Scan"><code>Scan</code></a> to handle allocation of the output buffer</li><li><a href="#MPI.Exscan!"><code>Exscan!</code></a>/<a href="#MPI.Exscan"><code>Exscan</code></a> for exclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L743-L759">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Scan" href="#MPI.Scan"><code>MPI.Scan</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Scan(sendbuf, op, comm::Comm)</code></pre><p>Inclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i</code>.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will also be a scalar of the same type.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Scan!"><code>Scan!</code></a> for mutating or in-place operations</li><li><a href="#MPI.Exscan!"><code>Exscan!</code></a>/<a href="#MPI.Exscan"><code>Exscan</code></a> for exclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L786-L802">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Exscan!" href="#MPI.Exscan!"><code>MPI.Exscan!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Exscan!(sendbuf, recvbuf[, count::Integer], op, comm::Comm)
Exscan!(buf[, count::Integer], op, comm::Comm)</code></pre><p>Exclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i-1</code>. The <code>recvbuf</code> on rank <code>0</code> is ignored, and the <code>recvbuf</code> on rank <code>1</code> will contain the contents of <code>sendbuf</code> on rank <code>0</code>.</p><p>If only a single <code>buf</code> is provided, then operations are performed in-place, and <code>buf</code> on rank 0 will remain unchanged.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Exscan"><code>Exscan</code></a> to handle allocation of the output buffer</li><li><a href="#MPI.Scan!"><code>Scan!</code></a>/<a href="#MPI.Scan"><code>Scan</code></a> for inclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Exscan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Exscan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Exscan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L814-L833">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Exscan" href="#MPI.Exscan"><code>MPI.Exscan</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recvbuf = Exscan(sendbuf, op, comm::Comm)</code></pre><p>Exclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i-1</code>. The <code>recvbuf</code> on rank <code>0</code> is undefined, and the <code>recvbuf</code> on rank <code>1</code> will contain the contents of <code>sendbuf</code> on rank <code>0</code>.</p><p><strong>See also</strong></p><ul><li><a href="#MPI.Exscan!"><code>Exscan!</code></a> for mutating and in-place operations</li><li><a href="#MPI.Scan!"><code>Scan!</code></a>/<a href="#MPI.Scan"><code>Scan</code></a> for inclusive scan</li><li><a href="../advanced/#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7eaf42dc68a22b39c4e7da0986a81387b1011123/src/collective.jl#L861-L876">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../pointtopoint/">« Point-to-point communication</a><a class="docs-footer-nextpage" href="../onesided/">One-sided communication »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 23 May 2020 21:04">Saturday 23 May 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
