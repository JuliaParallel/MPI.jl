<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Collective communication · MPI.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MPI.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">MPI.jl</a></li><li><a class="toctext" href="installation.html">Installation</a></li><li><a class="toctext" href="usage.html">Usage</a></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="examples/01-hello.html">Hello world</a></li><li><a class="toctext" href="examples/02-broadcast.html">Broadcast</a></li><li><a class="toctext" href="examples/03-reduce.html">Reduce</a></li><li><a class="toctext" href="examples/04-sendrecv.html">Send/receive</a></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="environment.html">Environment</a></li><li><a class="toctext" href="comm.html">Communicators</a></li><li><a class="toctext" href="pointtopoint.html">Point-to-point communication</a></li><li class="current"><a class="toctext" href="collective.html">Collective communication</a><ul class="internal"><li><a class="toctext" href="#Synchronization-1">Synchronization</a></li><li><a class="toctext" href="#Broadcast-1">Broadcast</a></li><li><a class="toctext" href="#Gather/Scatter-1">Gather/Scatter</a></li><li><a class="toctext" href="#Reduce/Scan-1">Reduce/Scan</a></li></ul></li><li><a class="toctext" href="onesided.html">One-sided communication</a></li><li><a class="toctext" href="topology.html">Topology</a></li><li><a class="toctext" href="io.html">I/O</a></li><li><a class="toctext" href="advanced.html">Advanced</a></li></ul></li><li><a class="toctext" href="refindex.html">Index</a></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href="collective.html">Collective communication</a></li></ul><a class="edit-page" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/collective.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Collective communication</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Collective-communication-1" href="#Collective-communication-1">Collective communication</a></h1><h2><a class="nav-anchor" id="Synchronization-1" href="#Synchronization-1">Synchronization</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Barrier" href="#MPI.Barrier"><code>MPI.Barrier</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Barrier(comm::Comm)</code></pre><p>Blocks until <code>comm</code> is synchronized.</p><p>If <code>comm</code> is an intracommunicator, then it blocks until all members of the group have called it.</p><p>If <code>comm</code> is an intercommunicator, then it blocks until all members of the other group have called it.</p><p><strong>External links</strong></p><ul><li><code>MPI_Barrier</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Barrier.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L3-L14">source</a></section><h2><a class="nav-anchor" id="Broadcast-1" href="#Broadcast-1">Broadcast</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Bcast!" href="#MPI.Bcast!"><code>MPI.Bcast!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Bcast!(buf[, count=length(buf)], root::Integer, comm::Comm)</code></pre><p>Broadcast the first <code>count</code> elements of the buffer <code>buf</code> from <code>root</code> to all processes.</p><p><strong>External links</strong></p><ul><li><code>MPI_Bcast</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Bcast.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L21-L28">source</a></section><h2><a class="nav-anchor" id="Gather/Scatter-1" href="#Gather/Scatter-1">Gather/Scatter</a></h2><h3><a class="nav-anchor" id="Gather-1" href="#Gather-1">Gather</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgather!" href="#MPI.Allgather!"><code>MPI.Allgather!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgather!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], comm::Comm)
Allgather!(sendrecvbuf, count::Integer, comm::Comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order into <code>recvbuf</code>.</p><p>If only one buffer <code>sendrecvbuf</code> is provided, then each process send data is assumed to be in the area where it would receive it&#39;s own contribution.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Allgather"><code>Allgather</code></a> for the allocating operation</li><li><a href="collective.html#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="collective.html#MPI.Allgatherv"><code>Allgatherv</code></a> if the number of elements varies between processes.</li><li><a href="collective.html#MPI.Gather!"><code>Gather!</code></a> to send only to a single root process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgather.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L277-L294">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgather" href="#MPI.Allgather"><code>MPI.Allgather</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgather(sendbuf[, count=length(sendbuf)], comm)</code></pre><p>Each process sends the first <code>count</code> elements of <code>sendbuf</code> to the other processes, who store the results in rank order allocating the output buffer.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Allgather!"><code>Allgather!</code></a> for the mutating operation</li><li><a href="collective.html#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="collective.html#MPI.Allgatherv"><code>Allgatherv</code></a> if the number of elements varies between processes.</li><li><a href="collective.html#MPI.Gather!"><code>Gather!</code></a> to send only to a single root process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgather.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L313-L326">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgatherv!" href="#MPI.Allgatherv!"><code>MPI.Allgatherv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgatherv!(sendbuf, recvbuf, counts, comm)
Allgatherv!(sendrecvbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process stores the received data in rank order in the buffer <code>recvbuf</code>.</p><p>If only one buffer <code>sendrecvbuf</code> is provided, then for each process, the data to be sent is taken from the interval of <code>recvbuf</code> where it would store it&#39;s own data.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Allgatherv"><code>Allgatherv</code></a> for the allocating operation</li><li><a href="collective.html#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="collective.html#MPI.Gatherv"><code>Gatherv</code></a> to send the result to a single process</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgatherv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L406-L423">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allgatherv" href="#MPI.Allgatherv"><code>MPI.Allgatherv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allgatherv(sendbuf, counts, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to all other process. Each process allocates an output buffer and stores the received data in rank order.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Allgatherv!"><code>Allgatherv!</code></a> for the mutating operation.</li><li><a href="collective.html#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="collective.html#MPI.Gatherv"><code>Gatherv</code></a> to send the result to a single process.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allgatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allgatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allgatherv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L444-L457">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather!" href="#MPI.Gather!"><code>MPI.Gather!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], root::Integer, comm::Comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> process stores elements in rank order in the buffer buffer <code>recvbuf</code>.</p><p><code>sendbuf</code> can be <code>nothing</code> on the root process, in which case the corresponding entries in <code>recvbuf</code> are assumed to be already in place (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Gather</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gather!(nothing, buf, count, root, comm)
else
    Gather!(buf, nothing, count, root, comm)
end</code></pre><p><code>recvbuf</code> on the root process should be a buffer of length <code>count*Comm_size(comm)</code>, and on non-root processes it is ignored and can be <code>nothing</code>.</p><p><code>count</code> should be the same for all processes.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Gather"><code>Gather</code></a> for the allocating operation.</li><li><a href="collective.html#MPI.Gatherv!"><code>Gatherv!</code></a> if the number of elements varies between processes.</li><li><a href="collective.html#MPI.Allgather!"><code>Allgather!</code></a> to send the result to all processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gather.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L199-L229">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gather" href="#MPI.Gather"><code>MPI.Gather</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gather(sendbuf[, count=length(sendbuf)], root, comm)</code></pre><p>Each process sends the first <code>count</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Gather!"><code>Gather!</code></a> for the mutating operation.</li><li><a href="collective.html#MPI.Gatherv!"><code>Gatherv!</code></a>/<a href="collective.html#MPI.Gatherv"><code>Gatherv</code></a> if the number of elements varies between processes.</li><li><a href="collective.html#MPI.Allgather!"><code>Allgather!</code></a>/<a href="collective.html#MPI.Allgather"><code>Allgather</code></a> to send the result to all processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gather</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gather.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gather.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L253-L266">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv!" href="#MPI.Gatherv!"><code>MPI.Gatherv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> stores elements in rank order in the buffer <code>recvbuf</code>.</p><p><code>sendbuf</code> can be <code>nothing</code> on the root process, in which case the corresponding entries in <code>recvbuf</code> are assumed to be already in place (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Gatherv</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Gatherv!(nothing, buf, counts, root, comm)
else
    Gatherv!(buf, nothing, counts, root, comm)
end</code></pre><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Gatherv"><code>Gatherv</code></a> for the allocating operation</li><li><a href="collective.html#MPI.Gather!"><code>Gather!</code></a></li><li><a href="collective.html#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="collective.html#MPI.Allgatherv"><code>Allgatherv</code></a> to send the result to all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gatherv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L337-L362">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Gatherv" href="#MPI.Gatherv"><code>MPI.Gatherv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Gatherv(sendbuf, counts, root, comm)</code></pre><p>Each process sends the first <code>counts[rank]</code> elements of the buffer <code>sendbuf</code> to the <code>root</code> process. The <code>root</code> allocates the output buffer and stores elements in rank order.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Gatherv!"><code>Gatherv!</code></a> for the mutating operation</li><li><a href="collective.html#MPI.Gather!"><code>Gather!</code></a>/<a href="collective.html#MPI.Gather"><code>Gather</code></a></li><li><a href="collective.html#MPI.Allgatherv!"><code>Allgatherv!</code></a>/<a href="collective.html#MPI.Allgatherv"><code>Allgatherv</code></a> to send the result to all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Gatherv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Gatherv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Gatherv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L386-L400">source</a></section><h3><a class="nav-anchor" id="Scatter-1" href="#Scatter-1">Scatter</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter!" href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter!(sendbuf, recvbuf[, count=length(recvbuf)], root::Integer, comm::Comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>count</code>, and sends the <code>j</code>-th chunk to the process of rank <code>j</code> into the <code>recvbuf</code> buffer.</p><p><code>sendbuf</code> on the root process should be a buffer of length <code>count*Comm_size(comm)</code>, and on non-root processes it is ignored and can be <code>nothing</code>.</p><p><code>recvbuf</code> can be <code>nothing</code> on the root process, in which case it is unmodified (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Scatter</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatter!(buf, nothing, count, root, comm)
else
    Scatter!(nothing, buf, count, root, comm)        
end</code></pre><p><code>count</code> should be the same for all processes.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Scatter"><code>Scatter</code></a> to allocate the output buffer.</li><li><a href="collective.html#MPI.Scatterv!"><code>Scatterv!</code></a> if the number of elements varies between processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatter</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatter.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatter.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L62-L89">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatter" href="#MPI.Scatter"><code>MPI.Scatter</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatter(sendbuf, count, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks and sends the <code>j</code>-th chunk to the process of rank <code>j</code>, allocating the output buffer.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Scatter!"><code>Scatter!</code></a> for the mutating operation.</li><li><a href="collective.html#MPI.Scatterv!"><code>Scatterv!</code></a> if the number of elements varies between processes.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatter</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatter.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatter.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L114-L126">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv!" href="#MPI.Scatterv!"><code>MPI.Scatterv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv!(sendbuf, recvbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j into the <code>recvbuf</code> buffer, which must be of length at least <code>count</code>.</p><p><code>recvbuf</code> can be <code>nothing</code> on the root process, in which case it is unmodified (this corresponds the behaviour of <code>MPI_IN_PLACE</code> in <code>MPI_Scatterv</code>). For example</p><pre><code class="language-none">if root == MPI.Comm_rank(comm)
    Scatterv!(buf, nothing, counts, root, comm)
else
    Scatterv!(nothing, buf, counts, root, comm)
end</code></pre><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Scatterv"><code>Scatterv</code></a> for the allocating operation</li><li><a href="collective.html#MPI.Scatter!"><code>Scatter!</code></a>/<a href="collective.html#MPI.Scatter"><code>Scatter</code></a> if the counts are the same for all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatterv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatterv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatterv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L132-L155">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scatterv" href="#MPI.Scatterv"><code>MPI.Scatterv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scatterv(sendbuf, counts, root, comm)</code></pre><p>Splits the buffer <code>sendbuf</code> in the <code>root</code> process into <code>Comm_size(comm)</code> chunks of length <code>counts[j]</code> and sends the j-th chunk to the process of rank j, which allocates the output buffer</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Scatterv!"><code>Scatterv!</code></a> for the mutating operation</li><li><a href="collective.html#MPI.Scatter!"><code>Scatter!</code></a>/<a href="collective.html#MPI.Scatter"><code>Scatter</code></a> if the counts are the same for all processes</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scatterv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scatterv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scatterv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L179-L192">source</a></section><h3><a class="nav-anchor" id="All-to-all-1" href="#All-to-all-1">All-to-all</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoall!" href="#MPI.Alltoall!"><code>MPI.Alltoall!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoall!(sendbuf, recvbuf, count::Integer, comm::Comm)
Alltoall!(sendrecvbuf, count::Integer, comm::Comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk of the buffer <code>recvbuf</code>.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p>If only one buffer <code>sendrecvbuf</code> then data is overwritten.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Alltoall"><code>Alltoall</code></a> for the allocating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoall</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoall.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoall.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L464-L488">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoall" href="#MPI.Alltoall"><code>MPI.Alltoall</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoall(sendbuf, count::Integer, comm::Comm)</code></pre><p>Every process divides the buffer <code>sendbuf</code> into <code>Comm_size(comm)</code> chunks of length <code>count</code>, sending the <code>j</code>-th chunk to the <code>j</code>-th process. Every process allocates the output buffer and stores the data received from the <code>j</code>-th process in the <code>j</code>-th chunk.</p><pre><code class="language-none">rank    send buf                        recv buf
----    --------                        --------
 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β
 1      A,B,C,D,E,F  ----------------&gt;  c,d,C,D,γ,ψ
 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν</code></pre><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Alltoall!"><code>Alltoall!</code></a> for the mutating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoall</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoall.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoall.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L507-L528">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoallv!" href="#MPI.Alltoallv!"><code>MPI.Alltoallv!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoallv!(sendbuf, recvbuf, scounts::Vector, rcounts::Vector, comm::Comm)</code></pre><p>Similar to <a href="collective.html#MPI.Alltoall!"><code>Alltoall!</code></a>, except with different size chunks per process.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Alltoallv"><code>Alltoallv</code></a> for the allocating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoallv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoallv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoallv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L534-L544">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Alltoallv" href="#MPI.Alltoallv"><code>MPI.Alltoallv</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Alltoallv(sendbuf, recvbuf, scounts::Vector, rcounts::Vector, comm::Comm)</code></pre><p>Similar to <a href="collective.html#MPI.Alltoall"><code>Alltoall</code></a>, except with different size chunks per process.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Alltoallv!"><code>Alltoallv!</code></a> for the mutating operation</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Alltoallv</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Alltoallv.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Alltoallv.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L563-L573">source</a></section><h2><a class="nav-anchor" id="Reduce/Scan-1" href="#Reduce/Scan-1">Reduce/Scan</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce!" href="#MPI.Reduce!"><code>MPI.Reduce!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Reduce!(sendbuf, recvbuf[, count::Integer=length(sendbuf)], op, root::Integer, comm::Comm)
Reduce!(sendrecvbuf, op, root::Integer, comm::Comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the first <code>count</code> elements of the buffer <code>sendbuf</code> and stores the result in <code>recvbuf</code> on the process of rank <code>root</code>.</p><p>On non-root processes <code>recvbuf</code> is ignored, and can be <code>nothing</code>. </p><p>To perform the reduction in place, provide a single buffer <code>sendrecvbuf</code>.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Reduce"><code>Reduce</code></a> to handle allocation of the output buffer.</li><li><a href="collective.html#MPI.Allreduce!"><code>Allreduce!</code></a>/<a href="collective.html#MPI.Allreduce"><code>Allreduce</code></a> to send reduction to all ranks.</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Reduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L586-L604">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Reduce" href="#MPI.Reduce"><code>MPI.Reduce</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">recvbuf = Reduce(sendbuf, op, root::Integer, comm::Comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>, returning the result <code>recvbuf</code> on the process of rank <code>root</code>, and <code>nothing</code> on non-root processes.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will be a value of the same type.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Reduce!"><code>Reduce!</code></a> for mutating and in-place operations</li><li><a href="collective.html#MPI.Allreduce!"><code>Allreduce!</code></a>/<a href="collective.html#MPI.Allreduce"><code>Allreduce</code></a> to send reduction to all ranks.</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Reduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L641-L656">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allreduce!" href="#MPI.Allreduce!"><code>MPI.Allreduce!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Allreduce!(sendbuf, recvbuf[, count=length(sendbuf)], op, comm)
Allreduce!(sendrecvbuf, op, comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the first <code>count</code> elements of the buffer <code>sendbuf</code>, storing the result in the <code>recvbuf</code> of all processes in the group.</p><p><code>Allreduce!</code> is equivalent to a <a href="collective.html#MPI.Reduce!"><code>Reduce!</code></a> operation followed by a <a href="collective.html#MPI.Bcast!"><code>Bcast!</code></a>, but can lead to better performance.</p><p>If only one <code>sendrecvbuf</code> buffer is provided, then the operation is performed in-place.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Allreduce"><code>Allreduce</code></a>, to handle allocation of the output buffer.</li><li><a href="collective.html#MPI.Reduce!"><code>Reduce!</code></a>/<a href="collective.html#MPI.Reduce"><code>Reduce</code></a> to send reduction to a single rank.</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allreduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allreduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allreduce.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L671-L690">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Allreduce" href="#MPI.Allreduce"><code>MPI.Allreduce</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">recvbuf = Allreduce(sendbuf, op, comm)</code></pre><p>Performs elementwise reduction using the operator <code>op</code> on the buffer <code>sendbuf</code>, returning the result in the <code>recvbuf</code> of all processes in the group.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will be a value of the same type.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Allreduce!"><code>Allreduce!</code></a> for mutating or in-place operations.</li><li><a href="collective.html#MPI.Reduce!"><code>Reduce!</code></a>/<a href="collective.html#MPI.Reduce"><code>Reduce</code></a> to send reduction to a single rank.</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Allreduce</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Allreduce.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Allreduce.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L717-L732">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scan!" href="#MPI.Scan!"><code>MPI.Scan!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Scan!(sendbuf, recvbuf[, count::Integer], op, comm::Comm)
Scan!(buf[, count::Integer], op, comm::Comm)</code></pre><p>Inclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i</code>.</p><p>If only a single buffer is provided, then operations will be performed in-place in <code>buf</code>.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Scan"><code>Scan</code></a> to handle allocation of the output buffer</li><li><a href="collective.html#MPI.Exscan!"><code>Exscan!</code></a>/<a href="collective.html#MPI.Exscan"><code>Exscan</code></a> for exclusive scan</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L743-L759">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Scan" href="#MPI.Scan"><code>MPI.Scan</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">recvbuf = Scan(sendbuf, op, comm::Comm)</code></pre><p>Inclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i</code>.</p><p><code>sendbuf</code> can also be a scalar, in which case <code>recvbuf</code> will also be a scalar of the same type.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Scan!"><code>Scan!</code></a> for mutating or in-place operations</li><li><a href="collective.html#MPI.Exscan!"><code>Exscan!</code></a>/<a href="collective.html#MPI.Exscan"><code>Exscan</code></a> for exclusive scan</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L786-L802">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Exscan!" href="#MPI.Exscan!"><code>MPI.Exscan!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Exscan!(sendbuf, recvbuf[, count::Integer], op, comm::Comm)
Exscan!(buf[, count::Integer], op, comm::Comm)</code></pre><p>Exclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i-1</code>. The <code>recvbuf</code> on rank <code>0</code> is ignored, and the <code>recvbuf</code> on rank <code>1</code> will contain the contents of <code>sendbuf</code> on rank <code>0</code>.</p><p>If only a single <code>buf</code> is provided, then operations are performed in-place, and <code>buf</code> on rank 0 will remain unchanged.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Exscan"><code>Exscan</code></a> to handle allocation of the output buffer</li><li><a href="collective.html#MPI.Scan!"><code>Scan!</code></a>/<a href="collective.html#MPI.Scan"><code>Scan</code></a> for inclusive scan</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Exscan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Exscan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Exscan.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L814-L833">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MPI.Exscan" href="#MPI.Exscan"><code>MPI.Exscan</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">recvbuf = Exscan(sendbuf, op, comm::Comm)</code></pre><p>Exclusive prefix reduction (analagous to <code>accumulate</code> in Julia): <code>recvbuf</code> on rank <code>i</code> will contain the the result of reducing <code>sendbuf</code> by <code>op</code> from ranks <code>0:i-1</code>. The <code>recvbuf</code> on rank <code>0</code> is undefined, and the <code>recvbuf</code> on rank <code>1</code> will contain the contents of <code>sendbuf</code> on rank <code>0</code>.</p><p><strong>See also</strong></p><ul><li><a href="collective.html#MPI.Exscan!"><code>Exscan!</code></a> for mutating and in-place operations</li><li><a href="collective.html#MPI.Scan!"><code>Scan!</code></a>/<a href="collective.html#MPI.Scan"><code>Scan</code></a> for inclusive scan</li><li><a href="advanced.html#MPI.Op"><code>Op</code></a> for details on reduction operators.</li></ul><p><strong>External links</strong></p><ul><li><code>MPI_Scan</code> man page: <a href="https://www.open-mpi.org/doc/current/man3/MPI_Scan.3.php">OpenMPI</a>, <a href="https://www.mpich.org/static/docs/latest/www3/MPI_Scan.html">MPICH</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaParallel/MPI.jl/blob/7536834fd4e36daaa9efb4b0b54eeb39a6b8deee/src/collective.jl#L861-L876">source</a></section><footer><hr/><a class="previous" href="pointtopoint.html"><span class="direction">Previous</span><span class="title">Point-to-point communication</span></a><a class="next" href="onesided.html"><span class="direction">Next</span><span class="title">One-sided communication</span></a></footer></article></body></html>
