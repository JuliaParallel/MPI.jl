var documenterSearchIndex = {"docs":
[{"location":"onesided/#One-sided-communication","page":"One-sided communication","title":"One-sided communication","text":"","category":"section"},{"location":"onesided/","page":"One-sided communication","title":"One-sided communication","text":"MPI.Win_create\nMPI.Win_create_dynamic\nMPI.Win_allocate_shared","category":"page"},{"location":"onesided/#MPI.Win_create","page":"One-sided communication","title":"MPI.Win_create","text":"MPI.Win_create(base::Array, comm::Comm; infokws...)\n\nCreate a window over the array base, returning a Win object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"onesided/#MPI.Win_create_dynamic","page":"One-sided communication","title":"MPI.Win_create_dynamic","text":"MPI.Win_create_dynamic(comm::Comm; infokws...)\n\nCreate a dynamic window returning a Win object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"onesided/#MPI.Win_allocate_shared","page":"One-sided communication","title":"MPI.Win_allocate_shared","text":"(win, ptr) = MPI.Win_allocate_shared(T, len, comm::Comm; infokws...)\n\nCreate and allocate a shared memory window for objects of type T of length len, returning a Win and a Ptr{T} object used by these processes to perform RMA operations\n\nThis is a collective call over comm.\n\ninfokws are info keys providing optimization hints.\n\nMPI.free should be called on the Win object once operations have been completed.\n\n\n\n\n\n","category":"function"},{"location":"environment/#Environment","page":"Environment","title":"Environment","text":"","category":"section"},{"location":"environment/#Launching-MPI-programs","page":"Environment","title":"Launching MPI programs","text":"","category":"section"},{"location":"environment/","page":"Environment","title":"Environment","text":"mpiexec\nMPI.install_mpiexecjl","category":"page"},{"location":"environment/#MPI.mpiexec","page":"Environment","title":"MPI.mpiexec","text":"mpiexec(fn)\n\nA wrapper function for the MPI launcher executable. Calls fn(cmd), where cmd is a Cmd object of the MPI launcher.\n\nEnvironment Variables\n\nThe behaviour of mpiexec can be controlled by the following environment variables:\n\nJULIA_MPIEXEC: the name or path of the launcher executable (set at compile time).\nJULIA_MPIEXEC_ARGS: additional arguments that are passed to the launcher. These are space seperated, supporting the same quoting rules as Julia Cmd objects. These can be modified at run time.\n\nUsage\n\njulia> mpiexec(cmd -> run(`$cmd -n 3 echo hello world`));\nhello world\nhello world\nhello world\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.install_mpiexecjl","page":"Environment","title":"MPI.install_mpiexecjl","text":"MPI.install_mpiexecjl(; command::String = \"mpiexecjl\",\n                      destdir::String = joinpath(DEPOT_PATH[1], \"bin\"),\n                      force::Bool = false, verbose::Bool = true)\n\nInstall the mpiexec wrapper to destdir directory, with filename command. Set force to true to overwrite an existing destination file with the same path.  If verbose is true, the installation prints information about the progress of the process.\n\n\n\n\n\n","category":"function"},{"location":"environment/#Enums","page":"Environment","title":"Enums","text":"","category":"section"},{"location":"environment/","page":"Environment","title":"Environment","text":"MPI.ThreadLevel","category":"page"},{"location":"environment/#MPI.ThreadLevel","page":"Environment","title":"MPI.ThreadLevel","text":"ThreadLevel\n\nAn Enum denoting the level of threading support in the current process:\n\nMPI.THREAD_SINGLE: Only one thread will execute.\nMPI.THREAD_FUNNELED: The process may be multi-threaded, but the application must ensure that only the main thread makes MPI calls. See Is_thread_main.\nMPI.THREAD_SERIALIZED: The process may be multi-threaded, and multiple threads may make MPI calls, but only one at a time (i.e. all MPI calls are serialized).\nMPI.THREAD_MULTIPLE: Multiple threads may call MPI, with no restrictions.\n\nSee also\n\nInit_thread\nQuery_thread\n\n\n\n\n\n","category":"type"},{"location":"environment/#Functions","page":"Environment","title":"Functions","text":"","category":"section"},{"location":"environment/","page":"Environment","title":"Environment","text":"MPI.Abort\nMPI.Init\nMPI.Init_thread\nMPI.Query_thread\nMPI.Is_thread_main\nMPI.Initialized\nMPI.Finalize\nMPI.Finalized\nMPI.universe_size","category":"page"},{"location":"environment/#MPI.Abort","page":"Environment","title":"MPI.Abort","text":"Abort(comm::Comm, errcode::Integer)\n\nMake a “best attempt” to abort all tasks in the group of comm. This function does not require that the invoking environment take any action with the error code. However, a Unix or POSIX environment should handle this as a return errorcode from the main program.\n\nExternal links\n\nMPI_Abort man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Init","page":"Environment","title":"MPI.Init","text":"Init(;finalize_atexit=true)\n\nInitialize MPI in the current process, and if finalize_atexit is true and adds an atexit hook to call MPI.Finalize if it hasn't already been called.\n\nAll MPI programs must contain exactly one call to MPI.Init or MPI.Init_thread. In particular, note that it is not valid to call MPI.Init or MPI.Init_thread again after calling MPI.Finalize.\n\nThe only MPI functions that may be called before MPI.Init/MPI.Init_thread are MPI.Initialized and MPI.Finalized.\n\nExternal links\n\nMPI_Init man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Init_thread","page":"Environment","title":"MPI.Init_thread","text":"Init_thread(required::ThreadLevel; finalize_atexit=true)\n\nInitialize MPI and the MPI thread environment in the current process, and if finalize_atexit is true and adds an atexit hook to call MPI.Finalize if it hasn't already been called. The argument specifies the required level of threading support, see ThreadLevel.\n\nThe function will return the provided ThreadLevel, and values may be compared via inequalities, i.e.\n\nprovided = Init_thread(required)\n@assert provided >= required\n\nAll MPI programs must contain exactly one call to MPI.Init or MPI.Init_thread. In particular, note that it is not valid to call MPI.Init or MPI.Init_thread again after calling MPI.Finalize.\n\nThe only MPI functions that may be called before MPI.Init/MPI.Init_thread are MPI.Initialized and MPI.Finalized.\n\nExternal links\n\nMPI_Init_thread man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Query_thread","page":"Environment","title":"MPI.Query_thread","text":"Query_thread()\n\nQuery the level of threading support in the current process. Returns a ThreadLevel value denoting\n\nExternal links\n\nMPI_Query_thread man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Is_thread_main","page":"Environment","title":"MPI.Is_thread_main","text":"Is_thread_main()\n\nQueries whether the current thread is the main thread according to MPI. This can be called by any thread, and is useful for the  THREAD_FUNNELED ThreadLevel.\n\nExternal links\n\nMPI_Is_thread_main man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Initialized","page":"Environment","title":"MPI.Initialized","text":"Initialized()\n\nReturns true if MPI.Init has been called, false otherwise.\n\nIt is unaffected by MPI.Finalize, and is one of the few functions that may be called before MPI.Init.\n\nExternal links\n\nMPI_Intialized man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Finalize","page":"Environment","title":"MPI.Finalize","text":"Finalize()\n\nMarks MPI state for cleanup. This should be called after MPI.Init or MPI.Init_thread, and can be called at most once. No further MPI calls (other than Initialized or Finalized) should be made after it is called.\n\nMPI.Init and MPI.Init_thread will automatically insert a hook to call this function when Julia exits, if it hasn't already been called.\n\nExternal links\n\nMPI_Finalize man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.Finalized","page":"Environment","title":"MPI.Finalized","text":"Finalized()\n\nReturns true if MPI.Finalize has completed, false otherwise.\n\nIt is safe to call before MPI.Init and after MPI.Finalize.\n\nExternal links\n\nMPI_Finalized man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"environment/#MPI.universe_size","page":"Environment","title":"MPI.universe_size","text":"universe_size()\n\nThe total number of available slots, or nothing if it is not defined. This is determined by the MPI_UNIVERSE_SIZE attribute of COMM_WORLD.\n\nThis is typically dependent on the MPI implementation: for MPICH-based implementations, this is specified by the -usize argument. OpenMPI defines a default value based on the number of processes available.\n\n\n\n\n\n","category":"function"},{"location":"comm/#Communicators","page":"Communicators","title":"Communicators","text":"","category":"section"},{"location":"comm/","page":"Communicators","title":"Communicators","text":"An MPI communicator specifies the communication context for a communication operation. In particular, it specifies the set of processes which share the context, and assigns each each process a unique rank (see MPI.Comm_rank) taking an integer value in 0:n-1, where n is the number of processes in the communicator (see MPI.Comm_size.","category":"page"},{"location":"comm/#Types-and-enums","page":"Communicators","title":"Types and enums","text":"","category":"section"},{"location":"comm/","page":"Communicators","title":"Communicators","text":"MPI.Comm\nMPI.Comparison","category":"page"},{"location":"comm/#MPI.Comm","page":"Communicators","title":"MPI.Comm","text":"MPI.Comm\n\nAn MPI Communicator object.\n\n\n\n\n\n","category":"type"},{"location":"comm/#MPI.Comparison","page":"Communicators","title":"MPI.Comparison","text":"Comparison\n\nAn enum denoting the result of Comm_compare:\n\nMPI.IDENT: the objects are handles for the same object (identical groups and same contexts).\nMPI.CONGRUENT: the underlying groups are identical in constituents and rank order; these communicators differ only by context.\nMPI.SIMILAR: members of both objects are the same but the rank order differs.\nMPI.UNEQUAL: otherwise\n\n\n\n\n\n","category":"type"},{"location":"comm/#Constants","page":"Communicators","title":"Constants","text":"","category":"section"},{"location":"comm/","page":"Communicators","title":"Communicators","text":"MPI.COMM_WORLD\nMPI.COMM_SELF","category":"page"},{"location":"comm/#MPI.COMM_WORLD","page":"Communicators","title":"MPI.COMM_WORLD","text":"MPI.COMM_WORLD\n\nA communicator containing all processes with which the local rank can communicate at initialization. In a typical \"static-process\" model, this will be all processes.\n\n\n\n\n\n","category":"constant"},{"location":"comm/#MPI.COMM_SELF","page":"Communicators","title":"MPI.COMM_SELF","text":"MPI.COMM_SELF\n\nA communicator containing only the local process.\n\n\n\n\n\n","category":"constant"},{"location":"comm/#Functions","page":"Communicators","title":"Functions","text":"","category":"section"},{"location":"comm/#Operations","page":"Communicators","title":"Operations","text":"","category":"section"},{"location":"comm/","page":"Communicators","title":"Communicators","text":"MPI.Comm_size\nMPI.Comm_rank\nMPI.Comm_compare","category":"page"},{"location":"comm/#MPI.Comm_size","page":"Communicators","title":"MPI.Comm_size","text":"Comm_size(comm::Comm)\n\nThe number of processes involved in communicator.\n\nSee also\n\nMPI.Comm_rank.\n\nExternal links\n\nMPI_Comm_size man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_rank","page":"Communicators","title":"MPI.Comm_rank","text":"Comm_rank(comm::Comm)\n\nThe rank of the process in the particular communicator's group.\n\nReturns an integer in the range 0:MPI.Comm_size()-1.\n\nSee also\n\nMPI.Comm_size.\n\nExternal links\n\nMPI_Comm_rank man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_compare","page":"Communicators","title":"MPI.Comm_compare","text":"Comm_compare(comm1::Comm, comm2::Comm)::MPI.Comparison\n\nCompare two communicators, returning an element of the Comparison enum.\n\nExternal links\n\nMPI_Comm_compare man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#Constructors","page":"Communicators","title":"Constructors","text":"","category":"section"},{"location":"comm/","page":"Communicators","title":"Communicators","text":"MPI.Comm_dup\nMPI.Comm_get_parent\nMPI.Comm_spawn\nMPI.Comm_split\nMPI.Comm_split_type\nMPI.Intercomm_merge","category":"page"},{"location":"comm/#MPI.Comm_dup","page":"Communicators","title":"MPI.Comm_dup","text":"Comm_dup(comm::Comm)\n\nExternal links\n\nMPI_Comm_dup man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_get_parent","page":"Communicators","title":"MPI.Comm_get_parent","text":"Comm_get_parent()\n\nExternal links\n\nMPI_Comm_get_parent man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_spawn","page":"Communicators","title":"MPI.Comm_spawn","text":"Comm_spawn(command, argv::Vector{String}, nprocs::Integer, comm::Comm[, errors::Vector{Cint}]; kwargs...)\n\nExternal links\n\nMPI_Comm_spawn man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_split","page":"Communicators","title":"MPI.Comm_split","text":"Comm_split(comm::Comm, color::Integer, key::Integer)\n\nExternal links\n\nMPI_Comm_split man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Comm_split_type","page":"Communicators","title":"MPI.Comm_split_type","text":"Comm_split_type(comm::Comm, split_type::Integer, key::Integer; kwargs...)\n\nExternal links\n\nMPI_Comm_split_type man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#MPI.Intercomm_merge","page":"Communicators","title":"MPI.Intercomm_merge","text":"Intercomm_merge(intercomm::Comm, flag::Bool)\n\nExternal links\n\nMPI_Intercomm_merge man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"comm/#Miscellaneous","page":"Communicators","title":"Miscellaneous","text":"","category":"section"},{"location":"comm/","page":"Communicators","title":"Communicators","text":"MPI.universe_size","category":"page"},{"location":"examples/04-sendrecv/#Send/receive","page":"Send/receive","title":"Send/receive","text":"","category":"section"},{"location":"examples/04-sendrecv/","page":"Send/receive","title":"Send/receive","text":"# examples/04-sendrecv.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nsize = MPI.Comm_size(comm)\n\ndst = mod(rank+1, size)\nsrc = mod(rank-1, size)\n\nN = 4\n\nsend_mesg = Array{Float64}(undef, N)\nrecv_mesg = Array{Float64}(undef, N)\n\nfill!(send_mesg, Float64(rank))\n\nrreq = MPI.Irecv!(recv_mesg, src,  src+32, comm)\n\nprint(\"$rank: Sending   $rank -> $dst = $send_mesg\\n\")\nsreq = MPI.Isend(send_mesg, dst, rank+32, comm)\n\nstats = MPI.Waitall!([rreq, sreq])\n\nprint(\"$rank: Received $src -> $rank = $recv_mesg\\n\")\n\nMPI.Barrier(comm)","category":"page"},{"location":"examples/04-sendrecv/","page":"Send/receive","title":"Send/receive","text":"> mpiexecjl -n 3 julia examples/04-sendrecv.jl\n1: Sending   1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n0: Sending   0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n2: Sending   2 -> 0 = [2.0, 2.0, 2.0, 2.0]\n1: Received 0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n0: Received 2 -> 0 = [2.0, 2.0, 2.0, 2.0]\n2: Received 1 -> 2 = [1.0, 1.0, 1.0, 1.0]","category":"page"},{"location":"knownissues/#Known-issues","page":"Known issues","title":"Known issues","text":"","category":"section"},{"location":"knownissues/#Julia-module-precompilation","page":"Known issues","title":"Julia module precompilation","text":"","category":"section"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"If multiple MPI ranks trigger Julia's module precompilation, then a race condition can result in an error such as:","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"ERROR: LoadError: IOError: mkdir: file already exists (EEXIST)\nStacktrace:\n [1] uv_error at ./libuv.jl:97 [inlined]\n [2] mkdir(::String; mode::UInt16) at ./file.jl:177\n [3] mkpath(::String; mode::UInt16) at ./file.jl:227\n [4] mkpath at ./file.jl:222 [inlined]\n [5] compilecache_path(::Base.PkgId) at ./loading.jl:1210\n [6] compilecache(::Base.PkgId, ::String) at ./loading.jl:1240\n [7] _require(::Base.PkgId) at ./loading.jl:1029\n [8] require(::Base.PkgId) at ./loading.jl:927\n [9] require(::Module, ::Symbol) at ./loading.jl:922\n [10] include(::Module, ::String) at ./Base.jl:377\n [11] exec_options(::Base.JLOptions) at ./client.jl:288\n [12] _start() at ./client.jl:484","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"See julia issue #30174 for more discussion of this problem. There are similar issues with Pkg operations, see Pkg issue #1219.","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"This can be worked around be either:","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"Triggering precompilation before launching MPI processes, for example:\njulia --project -e 'using Pkg; pkg\"instantiate\"'\njulia --project -e 'using Pkg; pkg\"precompile\"'\nmpiexec julia --project script.jl\nLaunching julia with the  --compiled-modules=no option. This can result in much longer package load times.","category":"page"},{"location":"knownissues/#UCX","page":"Known issues","title":"UCX","text":"","category":"section"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"UCX is a communication framework used by several MPI implementations.","category":"page"},{"location":"knownissues/#Memory-cache","page":"Known issues","title":"Memory cache","text":"","category":"section"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"When used with CUDA, UCX intercepts cudaMalloc so it can determine whether the pointer passed to MPI is on the host (main memory) or the device (GPU). Unfortunately, there are several known issues with how this works with Julia:","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"UCX issue #5061\nUCX issue #4001 (fixed in UCX v1.7.0)","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"By default, MPI.jl disables this by setting","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"ENV[\"UCX_MEMTYPE_CACHE\"] = \"no\"","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"at __init__ which may result in reduced performance, especially for smaller messages.","category":"page"},{"location":"knownissues/#Multi-threading-and-signal-handling","page":"Known issues","title":"Multi-threading and signal handling","text":"","category":"section"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"When using Julia multi-threading, the Julia garbage collector internally uses SIGSEGV to synchronize threads.","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"By default, UCX will error if this signal is raised (#337), resulting in a message such as:","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0xXXXXXXXX)","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"This signal interception can be controlled by setting the environment variable UCX_ERROR_SIGNALS: if not already defined, MPI.jl will set it as:","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"ENV[\"UCX_ERROR_SIGNALS\"] = \"SIGILL,SIGBUS,SIGFPE\"","category":"page"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"at __init__. If set externally, it should be modified to exclude SIGSEGV from the list.","category":"page"},{"location":"knownissues/#Microsoft-MPI","page":"Known issues","title":"Microsoft MPI","text":"","category":"section"},{"location":"knownissues/#Custom-operators-on-32-bit-Windows","page":"Known issues","title":"Custom operators on 32-bit Windows","text":"","category":"section"},{"location":"knownissues/","page":"Known issues","title":"Known issues","text":"It is not possible to use custom operators with 32-bit Microsoft MPI, as it uses the stdcall calling convention, which is not supported by Julia's C-compatible function pointers.","category":"page"},{"location":"io/#I/O","page":"I/O","title":"I/O","text":"","category":"section"},{"location":"io/#File-manipulation","page":"I/O","title":"File manipulation","text":"","category":"section"},{"location":"io/","page":"I/O","title":"I/O","text":"MPI.File.open","category":"page"},{"location":"io/#MPI.File.open","page":"I/O","title":"MPI.File.open","text":"MPI.File.open(comm::Comm, filename::AbstractString; keywords...)\n\nOpen the file identified by filename. This is a collective operation on comm.\n\nSupported keywords are as follows:\n\nread, write, create, append have the same behaviour and defaults as Base.open.\nsequential: file will only be accessed sequentially (default: false)\nuniqueopen: file will not be concurrently opened elsewhere (default: false)\ndeleteonclose: delete file on close (default: false)\n\nAny additional keywords are passed via an Info object, and are implementation dependent.\n\nExternal links\n\nMPI_File_open man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#Views","page":"I/O","title":"Views","text":"","category":"section"},{"location":"io/","page":"I/O","title":"I/O","text":"MPI.File.set_view!\nMPI.File.get_byte_offset","category":"page"},{"location":"io/#MPI.File.set_view!","page":"I/O","title":"MPI.File.set_view!","text":"MPI.File.set_view!(file::FileHandle, disp::Integer, etype::Datatype, filetype::Datatype, datarep::AbstractString; kwargs...)\n\nSet the current process's view of file.\n\nThe start of the view is set to disp; the type of data is set to etype; the distribution of data to processes is set to filetype; and the representation of data in the file is set to datarep: one of \"native\" (default), \"internal\", or \"external32\".\n\nExternal links\n\nMPI_File_set_view man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.get_byte_offset","page":"I/O","title":"MPI.File.get_byte_offset","text":"MPI.File.get_byte_offset(file::FileHandle, offset::Integer)\n\nConverts a view-relative offset into an absolute byte position. Returns the absolute byte position (from the beginning of the file) of offset relative to the current view of file.\n\nExternal links\n\nMPI_File_get_byte_offset man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#Consistency","page":"I/O","title":"Consistency","text":"","category":"section"},{"location":"io/","page":"I/O","title":"I/O","text":"MPI.File.sync","category":"page"},{"location":"io/#MPI.File.sync","page":"I/O","title":"MPI.File.sync","text":"MPI.File.sync(fh::FileHandle)\n\nA collective operation causing all previous writes to fh by the calling process to be transferred to the storage device. If other processes have made updates to the storage device, then all such updates become visible to subsequent reads of fh by the calling process.\n\nExternal links\n\nMPI_File_sync man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#Data-access","page":"I/O","title":"Data access","text":"","category":"section"},{"location":"io/#Individual-pointer","page":"I/O","title":"Individual pointer","text":"","category":"section"},{"location":"io/","page":"I/O","title":"I/O","text":"MPI.File.read!\nMPI.File.read_all!\nMPI.File.write\nMPI.File.write_all","category":"page"},{"location":"io/#MPI.File.read!","page":"I/O","title":"MPI.File.read!","text":"MPI.File.read!(file::FileHandle, data)\n\nReads current view of file into data. data can be a Buffer, or any object for which Buffer(data) is defined.\n\nSee also\n\nMPI.File.set_view! to set the current view of the file\nMPI.File.read_all! for the collective operation\n\nExternal links\n\nMPI_File_read man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.read_all!","page":"I/O","title":"MPI.File.read_all!","text":"MPI.File.read_all!(file::FileHandle, data)\n\nReads current view of file into data. data can be a Buffer, or any object for which Buffer(data) is defined. This is a collective operation, so must be called on all ranks in the communicator on which file was opened.\n\nSee also\n\nMPI.File.set_view! to set the current view of the file\nMPI.File.read! for the noncollective operation\n\nExternal links\n\nMPI_File_read_all man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.write","page":"I/O","title":"MPI.File.write","text":"MPI.File.write(file::FileHandle, data)\n\nWrites data to the current view of file. data can be a Buffer, or any object for which Buffer_send(data) is defined.\n\nSee also\n\nMPI.File.set_view! to set the current view of the file\nMPI.File.write_all for the collective operation\n\nExternal links\n\nMPI_File_write man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.write_all","page":"I/O","title":"MPI.File.write_all","text":"MPI.File.write_all(file::FileHandle, data)\n\nWrites data to the current view of file. data can be a Buffer, or any object for which Buffer_send(data) is defined. This is a collective operation, so must be called on all ranks in the communicator on which file was opened.\n\nSee also\n\nMPI.File.set_view! to set the current view of the file\nMPI.File.write for the noncollective operation\n\nExternal links\n\nMPI_File_write_all man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#Explicit-offsets","page":"I/O","title":"Explicit offsets","text":"","category":"section"},{"location":"io/","page":"I/O","title":"I/O","text":"MPI.File.read_at!\nMPI.File.read_at_all!\nMPI.File.write_at\nMPI.File.write_at_all","category":"page"},{"location":"io/#MPI.File.read_at!","page":"I/O","title":"MPI.File.read_at!","text":"MPI.File.read_at!(file::FileHandle, offset::Integer, data)\n\nReads from file at position offset into data. data can be a Buffer, or any object for which Buffer(data) is defined.\n\nSee also\n\nMPI.File.read_at_all! for the collective operation\n\nExternal links\n\nMPI_File_read_at man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.read_at_all!","page":"I/O","title":"MPI.File.read_at_all!","text":"MPI.File.read_at_all!(file::FileHandle, offset::Integer, data)\n\nReads from file at position offset into data. data can be a Buffer, or any object for which Buffer(data) is defined. This is a collective operation, so must be called on all ranks in the communicator on which file was opened.\n\nSee also\n\nMPI.File.read_at! for the noncollective operation\n\nExternal links\n\nMPI_File_read_at_all man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.write_at","page":"I/O","title":"MPI.File.write_at","text":"MPI.File.write_at(file::FileHandle, offset::Integer, data)\n\nWrites data to file at position offset. data can be a Buffer, or any object for which Buffer_send(data) is defined.\n\nSee also\n\nMPI.File.write_at_all for the collective operation\n\nExternal links\n\nMPI_File_write_at man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.write_at_all","page":"I/O","title":"MPI.File.write_at_all","text":"MPI.File.write_at_all(file::FileHandle, offset::Integer, data)\n\nWrites from data to file at position offset. data can be a Buffer, or any object for which Buffer_send(data) is defined. This is a collective operation, so must be called on all ranks in the communicator on which file was opened.\n\nSee also\n\nMPI.File.write_at for the noncollective operation\n\nExternal links\n\nMPI_File_write_at_all man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#Shared-pointer","page":"I/O","title":"Shared pointer","text":"","category":"section"},{"location":"io/","page":"I/O","title":"I/O","text":"MPI.File.read_shared!\nMPI.File.write_shared\nMPI.File.read_ordered!\nMPI.File.write_ordered\nMPI.File.seek_shared\nMPI.File.get_position_shared","category":"page"},{"location":"io/#MPI.File.read_shared!","page":"I/O","title":"MPI.File.read_shared!","text":"MPI.File.read_shared!(file::FileHandle, data)\n\nReads from file using the shared file pointer into data.  data can be a Buffer, or any object for which Buffer(data) is defined.\n\nSee also\n\nMPI.File.read_ordered! for the collective operation\n\nExternal links\n\nMPI_File_read_shared man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.write_shared","page":"I/O","title":"MPI.File.write_shared","text":"MPI.File.write_shared(file::FileHandle, data)\n\nWrites to file using the shared file pointer from data. data can be a Buffer, or any object for which Buffer(data) is defined.\n\nSee also\n\nMPI.File.write_ordered for the noncollective operation\n\nExternal links\n\nMPI_File_write_shared man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.read_ordered!","page":"I/O","title":"MPI.File.read_ordered!","text":"MPI.File.read_ordered!(file::FileHandle, data)\n\nCollectively reads in rank order from file using the shared file pointer into data. data can be a Buffer, or any object for which Buffer(data) is defined. This is a collective operation, so must be called on all ranks in the communicator on which file was opened.\n\nSee also\n\nMPI.File.read_shared! for the non-collective operation\n\nExternal links\n\nMPI_File_read_ordered man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.write_ordered","page":"I/O","title":"MPI.File.write_ordered","text":"MPI.File.write_ordered(file::FileHandle, data)\n\nCollectively writes in rank order to file using the shared file pointer from data. data can be a Buffer, or any object for which Buffer(data) is defined. This is a collective operation, so must be called on all ranks in the communicator on which file was opened.\n\nSee also\n\nMPI.File.write_shared for the noncollective operation\n\nExternal links\n\nMPI_File_write_ordered man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.seek_shared","page":"I/O","title":"MPI.File.seek_shared","text":"MPI.File.seek_shared(file::FileHandle, offset::Integer, whence::Seek=SEEK_SET)\n\nUpdates the shared file pointer according to whence, which has the following possible values:\n\nMPI.File.SEEK_SET (default): the pointer is set to offset\nMPI.File.SEEK_CUR: the pointer is set to the current pointer position plus offset\nMPI.File.SEEK_END: the pointer is set to the end of file plus offset\n\nThis is a collective operation, and must be called with the same value on all processes in the communicator.\n\nExternal links\n\nMPI_File_seek_shared man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"io/#MPI.File.get_position_shared","page":"I/O","title":"MPI.File.get_position_shared","text":"MPI.File.get_position_shared(file::FileHandle)\n\nThe current position of the shared file pointer (in etype units) relative to the current view.\n\nExternal links\n\nMPI_File_get_position_shared man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"library/#Library-information","page":"Library information","title":"Library information","text":"","category":"section"},{"location":"library/#Constants","page":"Library information","title":"Constants","text":"","category":"section"},{"location":"library/","page":"Library information","title":"Library information","text":"MPI.MPI_VERSION\nMPI.MPI_LIBRARY\nMPI.MPI_LIBRARY_VERSION\nMPI.MPI_LIBRARY_VERSION_STRING","category":"page"},{"location":"library/#MPI.MPI_VERSION","page":"Library information","title":"MPI.MPI_VERSION","text":"MPI_VERSION :: VersionNumber\n\nThe supported version of the MPI standard.\n\nExternal links\n\nMPI_Get_version man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"constant"},{"location":"library/#MPI.MPI_LIBRARY","page":"Library information","title":"MPI.MPI_LIBRARY","text":"MPI_LIBRARY :: MPIImpl\n\nThe current MPI implementation: this is determined by\n\nSee also\n\nMPIImpl\n\n\n\n\n\n","category":"constant"},{"location":"library/#MPI.MPI_LIBRARY_VERSION","page":"Library information","title":"MPI.MPI_LIBRARY_VERSION","text":"MPI_LIBRARY_VERSION :: VersionNumber\n\nThe version of the MPI library\n\n\n\n\n\n","category":"constant"},{"location":"library/#MPI.MPI_LIBRARY_VERSION_STRING","page":"Library information","title":"MPI.MPI_LIBRARY_VERSION_STRING","text":"MPI_LIBRARY_VERSION_STRING :: String\n\nThe full version string provided by the library\n\nExternal links\n\nMPI_Get_library_version man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"constant"},{"location":"library/#Enums","page":"Library information","title":"Enums","text":"","category":"section"},{"location":"library/","page":"Library information","title":"Library information","text":"MPI.MPIImpl","category":"page"},{"location":"library/#MPI.MPIImpl","page":"Library information","title":"MPI.MPIImpl","text":"MPIImpl\n\nAn enum corresponding to known MPI implementations\n\nUnknownMPI: unable to determine MPI implementation\nMPICH: MPICH\nOpenMPI: Open MPI\nMicrosoftMPI: Microsoft MPI\nIntelMPI: Intel MPI\nSpectrimMPI: IBM Spectrum MPI\nMVAPICH: MVAPICH\nCrayMPICH: Part of the Cray Message Passing Toolkit (MPT)\n\nSee also\n\nMPI_LIBRARY\n\n\n\n\n\n","category":"type"},{"location":"library/#Functions","page":"Library information","title":"Functions","text":"","category":"section"},{"location":"library/","page":"Library information","title":"Library information","text":"MPI.has_cuda\nMPI.identify_implementation","category":"page"},{"location":"library/#MPI.has_cuda","page":"Library information","title":"MPI.has_cuda","text":"MPI.has_cuda()\n\nCheck if the MPI implementation is known to have CUDA support. Currently only Open MPI provides a mechanism to check, so it will return false with other implementations (unless overriden).\n\nThis can be overriden by setting the JULIA_MPI_HAS_CUDA environment variable to true or false.\n\n\n\n\n\n","category":"function"},{"location":"library/#MPI.identify_implementation","page":"Library information","title":"MPI.identify_implementation","text":"impl, version = identify_implementation()\n\nAttempt to identify the MPI implementation based on MPI_LIBRARY_VERSION_STRING. Returns a triple of values:\n\nimpl: a value of type MPIImpl\nversion: a VersionNumber of the library, or nothing if it cannot be determined.\n\nThis function is only intended for internal use. Users should use MPI_LIBRARY, MPI_LIBRARY_VERSION.\n\n\n\n\n\n","category":"function"},{"location":"buffers/#Buffers","page":"Buffers","title":"Buffers","text":"","category":"section"},{"location":"buffers/","page":"Buffers","title":"Buffers","text":"Buffers are used for sending and receiving data. MPI.jl provides the following buffer types:","category":"page"},{"location":"buffers/","page":"Buffers","title":"Buffers","text":"MPI.IN_PLACE\nMPI.Buffer\nMPI.Buffer_send\nMPI.UBuffer\nMPI.VBuffer\nMPI.RBuffer\nMPI.MPIPtr","category":"page"},{"location":"buffers/#MPI.IN_PLACE","page":"Buffers","title":"MPI.IN_PLACE","text":"MPI.IN_PLACE\n\nA sentinel value that can be passed as a buffer argument for certain collective operations to use the same buffer for send and receive operations.\n\nScatter! and Scatterv!: can be used as the recvbuf argument on the root process.\nGather! and Gatherv!: can be used as the sendbuf argument on the root process.\nAllgather!, Allgatherv!, Alltoall! and Alltoallv!: can be used as the sendbuf argument on all processes.\nReduce! (root only), Allreduce!, Scan! and Exscan!: can be used as sendbuf argument.\n\n\n\n\n\n","category":"constant"},{"location":"buffers/#MPI.Buffer","page":"Buffers","title":"MPI.Buffer","text":"MPI.Buffer\n\nAn MPI buffer for communication with a single rank. It is used for point-to-point communication and some collective operations.\n\nFields\n\ndata\na Julia object referencing a region of memory to be used for communication. It is required that the object can be cconverted to an MPIPtr.\ncount\nthe number of elements of datatype in the buffer. Note that this may not correspond to the number of elements in the array if derived types are used.\ndatatype\nthe MPI.Datatype stored in the buffer.\n\nUsage\n\nBuffer(data, count::Integer, datatype::Datatype)\n\nGeneric constructor.\n\nBuffer(data)\n\nConstruct a Buffer backed by data, automatically determining the appropriate count and datatype. Methods are provided for\n\nRef\nArray\nCUDA.CuArray if CUDA.jl is loaded\nSubArrays of an Array or CUDA.CuArray where the layout is contiguous, sequential or blocked.\n\nSee also\n\nBuffer_send\n\n\n\n\n\n","category":"type"},{"location":"buffers/#MPI.Buffer_send","page":"Buffers","title":"MPI.Buffer_send","text":"Buffer_send(data)\n\nConstruct a Buffer object for a send operation from data, allowing cases where isbits(data).\n\n\n\n\n\n","category":"function"},{"location":"buffers/#MPI.UBuffer","page":"Buffers","title":"MPI.UBuffer","text":"MPI.UBuffer\n\nAn MPI buffer for chunked collective communication, where all chunks are of uniform size.\n\nFields\n\ndata\nA Julia object referencing a region of memory to be used for communication. It is required that the object can be cconverted to an MPIPtr.\ncount\nThe number of elements of datatype in each chunk.\nnchunks\nThe maximum number of chunks stored in the buffer. This is used only for validation, and can be set to nothing to disable checks.\ndatatype\nThe MPI.Datatype stored in the buffer.\n\nUsage\n\nUBuffer(data, count::Integer, nchunks::Union{Nothing, Integer}, datatype::Datatype)\n\nGeneric constructor.\n\nUBuffer(data, count::Integer)\n\nConstruct a UBuffer backed by data, where count is the number of elements in each chunk.\n\nSee also\n\nVBuffer: similar, but supports chunks of non-uniform sizes.\n\n\n\n\n\n","category":"type"},{"location":"buffers/#MPI.VBuffer","page":"Buffers","title":"MPI.VBuffer","text":"MPI.VBuffer\n\nAn MPI buffer for chunked collective communication, where chunks can be of different sizes and at different offsets.\n\nFields\n\ndata\nA Julia object referencing a region of memory to be used for communication. It is required that the object can be cconverted to an MPIPtr.\ncounts\nAn array containing the length of each chunk.\ndispls\nAn array containing the (0-based) displacements of each chunk.\ndatatype\nThe MPI.Datatype stored in the buffer.\n\nUsage\n\nVBuffer(data, counts[, displs[, datatype]])\n\nConstruct a VBuffer backed by data, where counts[j] is the number of elements in the jth chunk, and displs[j] is the 0-based displacement. In other words, the jth chunk occurs in indices displs[j]+1:displs[j]+counts[j].\n\nThe default value for displs[j] = sum(counts[1:j-1]).\n\nSee also\n\nUBuffer when chunks are all of the same size.\n\n\n\n\n\n","category":"type"},{"location":"buffers/#MPI.RBuffer","page":"Buffers","title":"MPI.RBuffer","text":"MPI.RBuffer\n\nAn MPI buffer for reduction operations (MPI.Reduce!, MPI.Allreduce!, MPI.Scan!, MPI.Exscan!).\n\nFields\n\nsenddata\nA Julia object referencing a region of memory to be used for the send buffer. It is required that the object can be cconverted to an MPIPtr.\nrecvdata\nA Julia object referencing a region of memory to be used for the receive buffer. It is required that the object can be cconverted to an MPIPtr.\ncount\nthe number of elements of datatype in the buffer. Note that this may not correspond to the number of elements in the array if derived types are used.\ndatatype\nthe MPI.Datatype stored in the buffer.\n\nUsage\n\nRBuffer(senddata, recvdata[, count, datatype])\n\nGeneric constructor.\n\nRBuffer(senddata, recvdata)\n\nConstruct a Buffer backed by senddata and recvdata, automatically determining the appropriate count and datatype.\n\nsenddata can be MPI.IN_PLACE\nrecvdata can be nothing on a non-root node with MPI.Reduce!\n\n\n\n\n\n","category":"type"},{"location":"buffers/#MPI.MPIPtr","page":"Buffers","title":"MPI.MPIPtr","text":"MPI.MPIPtr\n\nA pointer to an MPI buffer. This type is used only as part of the implicit conversion in ccall: a Julia object can be passed to MPI by defining methods for Base.cconvert(::Type{MPIPtr}, ...)/Base.unsafe_convert(::Type{MPIPtr}, ...).\n\nCurrently supported are:\n\nPtr\nRef\nArray\nSubArray\nCUDA.CuArray if CUDA.jl is loaded.\n\nAdditionally, certain sentinel values can be used, e.g. MPI_IN_PLACE or MPI_BOTTOM.\n\n\n\n\n\n","category":"type"},{"location":"advanced/#Advanced","page":"Advanced","title":"Advanced","text":"","category":"section"},{"location":"advanced/#Object-handling","page":"Advanced","title":"Object handling","text":"","category":"section"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"MPI.free","category":"page"},{"location":"advanced/#MPI.free","page":"Advanced","title":"MPI.free","text":"MPI.free(obj)\n\nFree the MPI object handle obj. This is typically used as the finalizer, and so need not be called directly unless otherwise noted.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#Datatype-objects","page":"Advanced","title":"Datatype objects","text":"","category":"section"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"MPI.Datatype\nMPI.Types.extent\nMPI.Types.create_contiguous\nMPI.Types.create_vector\nMPI.Types.create_subarray\nMPI.Types.create_struct\nMPI.Types.create_resized\nMPI.Types.commit!","category":"page"},{"location":"advanced/#MPI.Datatype","page":"Advanced","title":"MPI.Datatype","text":"Datatype\n\nA Datatype represents the layout of the data in memory.\n\nUsage\n\nDatatype(T; commit=true)\n\nEither return the predefined Datatype or create a new Datatype for the Julia type T. If commit=true, then the Types.commit! operation will also be applied so that it can be used for communication operations.\n\nNote that this can only be called on types for which isbitstype(T) is true.\n\n\n\n\n\n","category":"type"},{"location":"advanced/#MPI.Types.extent","page":"Advanced","title":"MPI.Types.extent","text":"lb, extent = MPI.Types.extent(dt::MPI.Datatype)\n\nGets the lowerbound lb and the extent extent in bytes.\n\nExternal links\n\nMPI_Type_get_extent man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"advanced/#MPI.Types.create_contiguous","page":"Advanced","title":"MPI.Types.create_contiguous","text":"MPI.Types.create_contiguous(count::Integer, oldtype::MPI.Datatype)\n\nCreate a derived Datatype that replicates oldtype into count contiguous locations.\n\nNote that MPI.Types.commit! must be used before the datatype can be used for communication.\n\nExternal links\n\nMPI_Type_contiguous man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"advanced/#MPI.Types.create_vector","page":"Advanced","title":"MPI.Types.create_vector","text":"MPI.Types.create_vector(count::Integer, blocklength::Integer, stride::Integer, oldtype::MPI.Datatype)\n\nCreate a derived Datatype that replicates oldtype into locations that consist of equally spaced blocks. \n\nNote that MPI.Types.commit! must be used before the datatype can be used for communication.\n\nExample\n\ndatatype = MPI.Types.create_vector(3, 2, 5, MPI.Datatype(Int64))\nMPI.Types.commit!(datatype)\n\nwill create a datatype with the following layout\n\n|<----->|  block length\n\n+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n| X | X |   |   |   | X | X |   |   |   | X | X |   |   |   |\n+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n\n|<---- stride ----->|\n\nwhere each segment represents an Int64.\n\n(image by Jonathan Dursi, https://stackoverflow.com/a/10788351/392585)\n\nExternal links\n\nMPI_Type_vector man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"advanced/#MPI.Types.create_subarray","page":"Advanced","title":"MPI.Types.create_subarray","text":"MPI.Types.create_subarray(sizes, subsizes, offset, oldtype::Datatype;\n                          rowmajor=false)\n\nCreates a derived Datatype describing an N-dimensional subarray of size subsizes of an N-dimensional array of size sizes and element type oldtype, with the first element offset by offset (i.e. the 0-based index of the first element).\n\nColumn-major indexing (used by Julia and Fortran) is assumed; use the keyword rowmajor=true to specify row-major layout (used by C and numpy).\n\nNote that MPI.Types.commit! must be used before the datatype can be used for communication.\n\nExternal links\n\nMPI_Type_create_subarray man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"advanced/#MPI.Types.create_struct","page":"Advanced","title":"MPI.Types.create_struct","text":"MPI.Types.create_struct(blocklengths, displacements, types)\n\nCreates a derived Datatype describing a struct layout.\n\nNote that MPI.Types.commit! must be used before the datatype can be used for communication.\n\nExternal links\n\nMPI_Type_create_struct man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"advanced/#MPI.Types.create_resized","page":"Advanced","title":"MPI.Types.create_resized","text":"MPI.Types.create_resized(oldtype::Datatype, lb::Integer, extent::Integer)\n\nCreates a new Datatype that is identical to oldtype, except that the lower bound of this new datatype is set to be lb, and its upper bound is set to be lb + extent.\n\nNote that MPI.Types.commit! must be used before the datatype can be used for communication.\n\nSee also\n\nMPI.Types.extent\n\nExternal links\n\nMPI_Type_create_resized man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"advanced/#MPI.Types.commit!","page":"Advanced","title":"MPI.Types.commit!","text":"MPI.Types.commit!(newtype::Datatype)\n\nCommits a Datatype so that it can be used for communication.\n\nExternal links\n\nMPI_Type_commit man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"advanced/#Operator-objects","page":"Advanced","title":"Operator objects","text":"","category":"section"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"MPI.Op","category":"page"},{"location":"advanced/#MPI.Op","page":"Advanced","title":"MPI.Op","text":"Op\n\nAn MPI reduction operator, for use with Reduce/Scan collective operations to wrap binary operators. MPI.jl will perform this conversion automatically.\n\nUsage\n\nOp(op, T=Any; iscommutative=false)\n\nWrap the Julia reduction function op for arguments of type T. op is assumed to be associative, and if iscommutative is true, assumed to be commutative as well.\n\nSee also\n\nReduce!/Reduce\nAllreduce!/Allreduce\nScan!/Scan\nExscan!/Exscan\n\n\n\n\n\n","category":"type"},{"location":"advanced/#Info-objects","page":"Advanced","title":"Info objects","text":"","category":"section"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"MPI.Info\nMPI.infoval","category":"page"},{"location":"advanced/#MPI.Info","page":"Advanced","title":"MPI.Info","text":"Info <: AbstractDict{Symbol,String}\n\nMPI.Info objects store key-value pairs, and are typically used for passing optional arguments to MPI functions.\n\nUsage\n\nThese will typically be hidden from user-facing APIs by splatting keywords, e.g.\n\nfunction f(args...; kwargs...)\n    info = Info(kwargs...)\n    # pass `info` object to `ccall`\nend\n\nFor manual usage, Info objects act like Julia Dict objects:\n\ninfo = Info(init=true) # keyword argument is required\ninfo[key] = value\nx = info[key]\ndelete!(info, key)\n\nIf init=false is used in the costructor (the default), a \"null\" Info object will be returned: no keys can be added to such an object.\n\n\n\n\n\n","category":"type"},{"location":"advanced/#MPI.infoval","page":"Advanced","title":"MPI.infoval","text":"infoval(x)\n\nConvert Julia object x to a string representation for storing in an Info object.\n\nThe MPI specification allows passing strings, Boolean values, integers, and lists.\n\n\n\n\n\n","category":"function"},{"location":"topology/#Topology","page":"Topology","title":"Topology","text":"","category":"section"},{"location":"topology/","page":"Topology","title":"Topology","text":"MPI.Cart_coords\nMPI.Cart_coords!\nMPI.Cart_create\nMPI.Cart_get\nMPI.Cart_rank\nMPI.Cart_shift\nMPI.Cart_sub\nMPI.Cartdim_get\nMPI.Dims_create!","category":"page"},{"location":"topology/#MPI.Cart_coords","page":"Topology","title":"MPI.Cart_coords","text":"Cart_coords(comm::Comm)\n\nDetermine coordinates of local process in Cartesian communicator.\n\nSee also Cart_coords!.\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Cart_coords!","page":"Topology","title":"MPI.Cart_coords!","text":"Cart_coords!(comm::Comm, rank::Integer, coords)\n\nDetermine coordinates of a given process in Cartesian communicator.\n\nExternal links\n\nMPI_Cart_coords man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Cart_create","page":"Topology","title":"MPI.Cart_create","text":"comm_cart = Cart_create(comm_old::Comm, [ndims::Integer], dims, periods, reorder)\n\nCreate new MPI communicator with Cartesian structure from an existent communicator.\n\nExternal links\n\nMPI_Cart_create man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Cart_get","page":"Topology","title":"MPI.Cart_get","text":"dims, periods, coords = Cart_get(comm::Comm)\n\nObtain information on the Cartesian topology of dimension N underlying the  communicator comm. This is specified by two Cint arrays of N elements for the number of processes and periodicity properties along each Cartesian dimension.  A third Cint array is returned, containing the Cartesian coordinates of the calling process.\n\nExternal links\n\nMPI_Cart_get man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Cart_rank","page":"Topology","title":"MPI.Cart_rank","text":"rank = Cart_rank(comm::Comm, coords)\n\nDetermine process rank in communicator comm with Cartesian structure. The coords array specifies the Cartesian coordinates of the process.\n\nExternal links\n\nMPI_Cart_rank man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Cart_shift","page":"Topology","title":"MPI.Cart_shift","text":"rank_source, rank_dest = Cart_shift(comm::Comm, direction::Integer, disp::Integer)\n\nReturn the source and destination ranks associated to a shift along a given direction.\n\nExternal links\n\nMPI_Cart_shift man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Cart_sub","page":"Topology","title":"MPI.Cart_sub","text":"comm_sub = Cart_sub(comm::Comm, remain_dims)\n\nCreate lower-dimensional Cartesian communicator from existent Cartesian topology.\n\nremain_dims should be a boolean vector specifying the dimensions that should be kept in the generated subgrid.\n\nExternal links\n\nMPI_Cart_sub man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Cartdim_get","page":"Topology","title":"MPI.Cartdim_get","text":"ndims = Cartdim_get(comm::Comm)\n\nReturn number of dimensions of the Cartesian topology associated with the communicator comm.\n\nExternal links\n\nMPI_Cartdim_get man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"topology/#MPI.Dims_create!","page":"Topology","title":"MPI.Dims_create!","text":"Dims_create!(nnodes::Integer, [ndims::Integer], dims)\n\nCreate a division of nnodes processes in a Cartesian grid.\n\nExternal links\n\nMPI_Dims_create man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"configuration/#Configuration","page":"Configuration","title":"Configuration","text":"","category":"section"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"By default, MPI.jl will download and link against the following MPI implementations:","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"Microsoft MPI on Windows\nMPICH on all other platforms","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"This is suitable for most single-node use cases, but for larger systems, such as HPC clusters or multi-GPU machines, you will probably want to configure against a system-provided MPI implementation in order to exploit feature such as fast network interfaces and CUDA-aware MPI interfaces.","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"MPI.jl will attempt to detect when you are running on a HPC cluster, and warn the user about this. To disable this warning, set the environment variable JULIA_MPI_CLUSTER_WARN=n.","category":"page"},{"location":"configuration/#Using-a-system-provided-MPI","page":"Configuration","title":"Using a system-provided MPI","text":"","category":"section"},{"location":"configuration/#Requirements","page":"Configuration","title":"Requirements","text":"","category":"section"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"MPI.jl requires a shared library installation of a C MPI library, supporting the MPI 3.0 standard or later.","category":"page"},{"location":"configuration/#Building","page":"Configuration","title":"Building","text":"","category":"section"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"To use the the system MPI, set the environment variable JULIA_MPI_BINARY=system and run Pkg.build(\"MPI\"). This can be done by:","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"julia --project -e 'ENV[\"JULIA_MPI_BINARY\"]=\"system\"; using Pkg; Pkg.build(\"MPI\"; verbose=true)'","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"This will attempt find and identify any available MPI implementation.","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"The MPI standard doesn't specify the exact application binary interface (ABI), but the following implementations should work directly:","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"Open MPI\nMPICH (v3.1 or later)\nIntel MPI\nMicrosoft MPI\nIBM Spectrum MPI","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"For other implementations, the build script will attempt to build a small C program to determine the appropriate type definitions and constants. This requires a compatible C compiler (mpicc by default).","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"If the implementation is changed, you will need to re-run Pkg.build(\"MPI\").","category":"page"},{"location":"configuration/#environment_variables","page":"Configuration","title":"Environment variables","text":"","category":"section"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"The following optional environment variables can be used to control certain aspects of the build script and other library behaviour. The results of these will be cached in a configuration file located at ~/.julia/prefs/MPI.toml and so can be used for subsequent MPI builds.","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"JULIA_MPI_BINARY: can be set to either the empty string (to use the default implementations above) or system (to use a system-provided implementation).\nJULIA_MPI_PATH: the top-level installation directory of MPI. i.e. the library should be located in ${JULIA_MPI_PATH}/lib and mpiexec in ${JULIA_MPI_PATH}/bin\nJULIA_MPI_LIBRARY: the library name or full path of the MPI shared library. By default, it will attempt to look for common MPI library names in the standard library paths (e.g. libmpi, libmpich, msmpi).\nJULIA_MPI_ABI: the ABI used by the MPI implementation: one of MPICH, OpenMPI, MicrosoftMPI, or unknown. By default it will attempt to determine this by looking at the MPI_LIBRARY_VERSION_STRING.\nJULIA_MPIEXEC: the name (or full path) of the MPI launcher executable. The default is mpiexec, but some clusters require using the scheduler launcher interface (e.g. srun on Slurm, aprun on PBS).\nJULIA_MPIEXEC_ARGS: Additional arguments to be passed to MPI launcher.","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"If the ABI is unknown and the constant generation program is required, then the following variables are also queried:","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"JULIA_MPI_INCLUDE_PATH: the directory containing the MPI header files.\nJULIA_MPI_CFLAGS: C flags passed to the constant generation build (default: -lmpi)\nJULIA_MPICC: MPI C compiler (default: mpicc)","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"The test suite can also be modified by the following variables:","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"JULIA_MPIEXEC_TEST_ARGS: Additional arguments to be passed to the MPI launcher for the tests only.\nJULIA_MPI_TEST_ARRAYTYPE: Set to CuArray to test the CUDA-aware interface with `CUDA.CuArray buffers.","category":"page"},{"location":"configuration/#Julia-wrapper-for-mpiexec","page":"Configuration","title":"Julia wrapper for mpiexec","text":"","category":"section"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"Since you can configure MPI.jl to use one of several MPI implementations, you may have different Julia projects using different implementation.  Thus, it may be cumbersome to find out which mpiexec executable is associated to a specific project.  To make this easy, on Unix-based systems MPI.jl comes with a thin project-aware wrapper around mpiexec, called mpiexecjl.","category":"page"},{"location":"configuration/#Installation","page":"Configuration","title":"Installation","text":"","category":"section"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"You can install mpiexecjl with MPI.install_mpiexecjl().  The default destination directory is joinpath(DEPOT_PATH[1], \"bin\"), which usually translates to ~/.julia/bin, but check the value on your system.  You can also tell MPI.install_mpiexecjl to install to a different directory.","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"To quickly call this wrapper we recommend you to add the destination directory to your PATH environment variable.","category":"page"},{"location":"configuration/#Usage","page":"Configuration","title":"Usage","text":"","category":"section"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"mpiexecjl has the same syntax as the mpiexec binary that will be called, but it takes in addition a --project option to call the specific binary associated to the MPI.jl version in the given project.  If no --project flag is used, the MPI.jl in the global Julia environment will be used instead.","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"After installing mpiexecjl and adding its directory to PATH, you can run it with:","category":"page"},{"location":"configuration/","page":"Configuration","title":"Configuration","text":"$ mpiexecjl --project=/path/to/project -n 20 julia script.jl","category":"page"},{"location":"pointtopoint/#Point-to-point-communication","page":"Point-to-point communication","title":"Point-to-point communication","text":"","category":"section"},{"location":"pointtopoint/#Types","page":"Point-to-point communication","title":"Types","text":"","category":"section"},{"location":"pointtopoint/","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Request\nMPI.Status","category":"page"},{"location":"pointtopoint/#MPI.Request","page":"Point-to-point communication","title":"MPI.Request","text":"MPI.Request\n\nAn MPI Request object, representing a non-blocking communication. This also contains a reference to the buffer used in the communication to ensure it isn't garbage-collected during communication.\n\nThe status of a Request can be checked by the Wait! and Test! functions or their multiple-request variants, which will deallocate the request once it is determined to be complete. Alternatively, it will be deallocated at finalization, meaning that it is safe to ignore the request objects if the status of the communication can be checked by other means.\n\nSee also Cancel!.\n\n\n\n\n\n","category":"type"},{"location":"pointtopoint/#MPI.Status","page":"Point-to-point communication","title":"MPI.Status","text":"MPI.Status\n\nThe status of an MPI receive communication. It has 3 accessible fields\n\nsource: source of the received message\ntag: tag of the received message\nerror: error code. This is only set if a function returns multiple statuses.\n\nAdditionally, the accessor function MPI.Get_count can be used to determine the number of entries received.\n\n\n\n\n\n","category":"type"},{"location":"pointtopoint/#Accessors","page":"Point-to-point communication","title":"Accessors","text":"","category":"section"},{"location":"pointtopoint/","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Get_count","category":"page"},{"location":"pointtopoint/#MPI.Get_count","page":"Point-to-point communication","title":"MPI.Get_count","text":"MPI.Get_count(status::Status, T)\n\nThe number of entries received. T should match the argument provided by the receive call that set the status variable.\n\nIf the number of entries received exceeds the limits of the count parameter, then it returns MPI_UNDEFINED.\n\nExternal links\n\nMPI_Get_count man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Blocking-communication","page":"Point-to-point communication","title":"Blocking communication","text":"","category":"section"},{"location":"pointtopoint/","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Send\nMPI.send\nMPI.Recv!\nMPI.Recv\nMPI.recv\nMPI.Sendrecv!","category":"page"},{"location":"pointtopoint/#MPI.Send","page":"Point-to-point communication","title":"MPI.Send","text":"Send(buf, dest::Integer, tag::Integer, comm::Comm)\n\nPerform a blocking send from the buffer buf to MPI rank dest of communicator comm using the message tag tag.\n\nExternal links\n\nMPI_Send man page: OpenMPI, MPICH\n\n\n\n\n\nSend(obj::T, dest::Integer, tag::Integer, comm::Comm) where T\n\nComplete a blocking send of obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.send","page":"Point-to-point communication","title":"MPI.send","text":"send(obj, dest::Integer, tag::Integer, comm::Comm)\n\nComplete a blocking send using a serialized version of obj to MPI rank dest of communicator comm using with the message tag tag.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Recv!","page":"Point-to-point communication","title":"MPI.Recv!","text":"Recv!(data, src::Integer, tag::Integer, comm::Comm)\n\nCompletes a blocking receive into the buffer data from MPI rank src of communicator comm using with the message tag tag.\n\ndata can be a Buffer, or any object for which Buffer(data) is defined.\n\nReturns the Status of the receive.\n\nSee also\n\nRecv \nrecv \n\nExternal links\n\nMPI_Recv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Recv","page":"Point-to-point communication","title":"MPI.Recv","text":"Recv(::Type{T}, src::Integer, tag::Integer, comm::Comm)\n\nCompletes a blocking receive of an object of type T from MPI rank src of communicator comm using with the message tag tag.\n\nReturns a tuple of the object of type T and the Status of the receive.\n\nSee also\n\nRecv! \nrecv \n\nExternal links\n\nMPI_Recv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.recv","page":"Point-to-point communication","title":"MPI.recv","text":"recv(src::Integer, tag::Integer, comm::Comm)\n\nCompletes a blocking receive of a serialized object from MPI rank src of communicator comm using with the message tag tag.\n\nReturns the deserialized object and the Status of the receive.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Sendrecv!","page":"Point-to-point communication","title":"MPI.Sendrecv!","text":"Sendrecv!(sendbuf, dest::Integer, sendtag::Integer,\n          recvbuf, source::Integer, recvtag::Integer,\n          comm::Comm)\n\nComplete a blocking send-receive operation over the MPI communicator comm. Send sendcount elements of type sendtype from sendbuf to the MPI rank dest using message tag tag, and receive recvcount elements of type recvtype from MPI rank source into the buffer recvbuf using message tag tag. Return a Status object.\n\nIf not provided, sendtype/recvtype and sendcount/recvcount are derived from the element type and length of sendbuf/recvbuf, respectively.\n\nExternal links\n\nMPI_Sendrecv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Non-blocking-communication","page":"Point-to-point communication","title":"Non-blocking communication","text":"","category":"section"},{"location":"pointtopoint/#Initiation","page":"Point-to-point communication","title":"Initiation","text":"","category":"section"},{"location":"pointtopoint/","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Isend\nMPI.isend\nMPI.Irecv!","category":"page"},{"location":"pointtopoint/#MPI.Isend","page":"Point-to-point communication","title":"MPI.Isend","text":"Isend(data, dest::Integer, tag::Integer, comm::Comm)\n\nStarts a nonblocking send of data to MPI rank dest of communicator comm using with the message tag tag.\n\ndata can be a Buffer, or any object for which Buffer_send is defined.\n\nReturns the Request object for the nonblocking send.\n\nExternal links\n\nMPI_Isend man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.isend","page":"Point-to-point communication","title":"MPI.isend","text":"isend(obj, dest::Integer, tag::Integer, comm::Comm)\n\nStarts a nonblocking send of using a serialized version of obj to MPI rank dest of communicator comm using with the message tag tag.\n\nReturns the commication Request for the nonblocking send.\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Irecv!","page":"Point-to-point communication","title":"MPI.Irecv!","text":"Irecv!(data, src::Integer, tag::Integer, comm::Comm)\n\nStarts a nonblocking receive into the buffer data from MPI rank src of communicator comm using with the message tag tag.\n\ndata can be a Buffer, or any object for which Buffer(data) is defined.\n\nReturns the Request for the nonblocking receive.\n\nExternal links\n\nMPI_Irecv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Completion","page":"Point-to-point communication","title":"Completion","text":"","category":"section"},{"location":"pointtopoint/","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Test!\nMPI.Testall!\nMPI.Testany!\nMPI.Testsome!\nMPI.Wait!\nMPI.Waitall!\nMPI.Waitany!\nMPI.Waitsome!","category":"page"},{"location":"pointtopoint/#MPI.Test!","page":"Point-to-point communication","title":"MPI.Test!","text":"(flag, status) = Test!(req::Request)\n\nCheck if the request req is complete. If so, the request is deallocated and flag is returned true and status as the Status of the request. Otherwise flag is returned false and status is nothing.\n\nExternal links\n\nMPI_Test man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testall!","page":"Point-to-point communication","title":"MPI.Testall!","text":"(flag, statuses) = Testall!(reqs::Vector{Request})\n\nCheck if all active requests in the array reqs are complete. If so, the requests are deallocated and flag is returned as true and statuses is an array of the Status objects corresponding to each request is returned. Otherwise no requests are modified a tuple of false, nothing is returned.\n\nExternal links\n\nMPI_Testall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testany!","page":"Point-to-point communication","title":"MPI.Testany!","text":"(flag, index, status) = Testany!(reqs::Vector{Request})\n\nCheck if any one of the requests in the array reqs is complete.\n\nIf one or more requests are complete, then one is chosen arbitrarily, deallocated and flag is returned as true, along with the index and the Status of the request.\n\nOtherwise, if there are no complete requests, then index is returned as 0, status as nothing, and flag as true if there are no active requests and false otherwise.\n\nExternal links\n\nMPI_Testany man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Testsome!","page":"Point-to-point communication","title":"MPI.Testsome!","text":"(indices, statuses) = Testsome!(reqs::Vector{Request})\n\nSimilar to Waitsome! except that it returns immediately: if no operations have completed then indices and statuses will be empty.\n\nExternal links\n\nMPI_Testsome man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Wait!","page":"Point-to-point communication","title":"MPI.Wait!","text":"status = Wait!(req::Request)\n\nBlock until the request req is complete and deallocated. Returns the Status of the request.\n\nExternal links\n\nMPI_Wait man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitall!","page":"Point-to-point communication","title":"MPI.Waitall!","text":"statuses = Waitall!(reqs::Vector{Request})\n\nBlock until all active requests in the array reqs are complete. Returns an array of the Status objects corresponding to each request.\n\nExternal links\n\nMPI_Waitall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitany!","page":"Point-to-point communication","title":"MPI.Waitany!","text":"(index, status) = Waitany!(reqs::Vector{Request})\n\nBlocks until one of the requests in the array reqs is complete: if more than one is complete, one is chosen arbitrarily. The request is deallocated and a tuple of the index of the completed request and its Status is returned. If there are no active requests, then index is returned as 0.\n\nExternal links\n\nMPI_Waitany man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Waitsome!","page":"Point-to-point communication","title":"MPI.Waitsome!","text":"(indices, statuses) = Waitsome!(reqs::Vector{Request})\n\nBlock until at least one of the active requests in the array reqs is complete. The completed requests are deallocated, and a tuple of their indices in reqs and their corresponding Status objects are returned. If there are no active requests, then the function returns immediately and indices and statuses are empty.\n\nExternal links\n\nMPI_Waitsome man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#Probe/Cancel","page":"Point-to-point communication","title":"Probe/Cancel","text":"","category":"section"},{"location":"pointtopoint/","page":"Point-to-point communication","title":"Point-to-point communication","text":"MPI.Cancel!\nMPI.Iprobe\nMPI.Probe","category":"page"},{"location":"pointtopoint/#MPI.Cancel!","page":"Point-to-point communication","title":"MPI.Cancel!","text":"Cancel!(req::Request)\n\nMarks a pending Irecv! operation for cancellation (cancelling a Isend, while supported in some implementations, is deprecated as of MPI 3.1). Note that the request is not deallocated, and can still be queried using the test or wait functions.\n\nExternal links\n\nMPI_Cancel man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Iprobe","page":"Point-to-point communication","title":"MPI.Iprobe","text":"ismessage, (status|nothing) = Iprobe(src::Integer, tag::Integer, comm::Comm)\n\nChecks if there is a message that can be received matching src, tag and comm. If so, returns a tuple true and a Status object, otherwise returns a tuple false, nothing.\n\nExternal links\n\nMPI_Iprobe man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"pointtopoint/#MPI.Probe","page":"Point-to-point communication","title":"MPI.Probe","text":"status = Probe(src::Integer, tag::Integer, comm::Comm)\n\nBlocks until there is a message that can be received matching src, tag and comm. Returns the corresponding Status object.\n\nExternal links\n\nMPI_Probe man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"refindex/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"refindex/","page":"Index","title":"Index","text":"","category":"page"},{"location":"examples/02-broadcast/#Broadcast","page":"Broadcast","title":"Broadcast","text":"","category":"section"},{"location":"examples/02-broadcast/","page":"Broadcast","title":"Broadcast","text":"# examples/02-broadcast.jl\nimport MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nN = 5\nroot = 0\n\nif MPI.Comm_rank(comm) == root\n    print(\" Running on $(MPI.Comm_size(comm)) processes\\n\")\nend\nMPI.Barrier(comm)\n\nif MPI.Comm_rank(comm) == root\n    A = [i*(1.0 + im*2.0) for i = 1:N]\nelse\n    A = Array{ComplexF64}(undef, N)\nend\n\nMPI.Bcast!(A, root, comm)\n\nprint(\"rank = $(MPI.Comm_rank(comm)), A = $A\\n\")\n\nif MPI.Comm_rank(comm) == root\n    B = Dict(\"foo\" => \"bar\")\nelse\n    B = nothing\nend\n\nB = MPI.bcast(B, root, comm)\nprint(\"rank = $(MPI.Comm_rank(comm)), B = $B\\n\")\n\nif MPI.Comm_rank(comm) == root\n    f = x -> x^2 + 2x - 1\nelse\n    f = nothing\nend\n\nf = MPI.bcast(f, root, comm)\nprint(\"rank = $(MPI.Comm_rank(comm)), f(3) = $(f(3))\\n\")","category":"page"},{"location":"examples/02-broadcast/","page":"Broadcast","title":"Broadcast","text":"> mpiexecjl -n 3 julia examples/02-broadcast.jl\n Running on 3 processes\nrank = 2, A = Complex{Float64}[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\nrank = 1, A = Complex{Float64}[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\nrank = 0, A = Complex{Float64}[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\nrank = 2, B = Dict(\"foo\" => \"bar\")\nrank = 1, B = Dict(\"foo\" => \"bar\")\nrank = 0, B = Dict(\"foo\" => \"bar\")\nrank = 0, f(3) = 14\nrank = 1, f(3) = 14\nrank = 2, f(3) = 14","category":"page"},{"location":"examples/03-reduce/#Reduce","page":"Reduce","title":"Reduce","text":"","category":"section"},{"location":"examples/03-reduce/","page":"Reduce","title":"Reduce","text":"# examples/03-reduce.jl\n# This example shows how to use custom datatypes and reduction operators\n# It computes the variance in parallel in a numerically stable way\n\nusing MPI, Statistics\n\nMPI.Init()\nconst comm = MPI.COMM_WORLD\nconst root = 0\n\n# Define a custom struct\n# This contains the summary statistics (mean, variance, length) of a vector\nstruct SummaryStat\n    mean::Float64\n    var::Float64\n    n::Float64\nend\nfunction SummaryStat(X::AbstractArray)\n    m = mean(X)\n    v = varm(X,m, corrected=false)\n    n = length(X)\n    SummaryStat(m,v,n)\nend\n\n# Define a custom reduction operator\n# this computes the pooled mean, pooled variance and total length\nfunction pool(S1::SummaryStat, S2::SummaryStat)\n    n = S1.n + S2.n\n    m = (S1.mean*S1.n + S2.mean*S2.n) / n\n    v = (S1.n * (S1.var + S1.mean * (S1.mean-m)) +\n         S2.n * (S2.var + S2.mean * (S2.mean-m)))/n\n    SummaryStat(m,v,n)\nend\n\nX = randn(10,3) .* [1,3,7]'\n\n# Perform a scalar reduction\nsumm = MPI.Reduce(SummaryStat(X), pool, root, comm)\n\nif MPI.Comm_rank(comm) == root\n    @show summ.var\nend\n\n# Perform a vector reduction:\n# the reduction operator is applied elementwise\ncol_summ = MPI.Reduce(mapslices(SummaryStat,X,dims=1), pool, root, comm)\n\nif MPI.Comm_rank(comm) == root\n    col_var = map(summ -> summ.var, col_summ)\n    @show col_var\nend","category":"page"},{"location":"examples/03-reduce/","page":"Reduce","title":"Reduce","text":"> mpiexecjl -n 3 julia examples/03-reduce.jl\nsumm.var = 12.755003139241616\ncol_var = [0.771585834116332 9.553951867255034 27.09474945216585]","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"MPI is based on a single program, multiple data (SPMD) model, where multiple processes are launched running independent programs, which then communicate as necessary via messages.","category":"page"},{"location":"usage/#Basic-example","page":"Usage","title":"Basic example","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"A script should include using MPI and MPI.Init() statements before calling any MPI operaions, for example","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"# examples/01-hello.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nprintln(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\")\nMPI.Barrier(comm)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Calling MPI.Finalize() at the end of the program is optional, as it will be called automatically when Julia exits.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"The program can then be launched via an MPI launch command (typically mpiexec, mpirun or srun), e.g.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"$ mpiexec -n 3 julia --project examples/01-hello.jl\nHello world, I am rank 0 of 3\nHello world, I am rank 2 of 3\nHello world, I am rank 1 of 3","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"The mpiexec function is provided for launching MPI programs from Julia itself.","category":"page"},{"location":"usage/#CUDA-aware-MPI-support","page":"Usage","title":"CUDA-aware MPI support","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"If your MPI implementation has been compiled with CUDA support, then CUDA.CuArrays (from the CUDA.jl package) can be passed directly as send and receive buffers for point-to-point and collective operations (they may also work with one-sided operations, but these are not often supported).","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"If using Open MPI, the status of CUDA support can be checked via the MPI.has_cuda() function.","category":"page"},{"location":"collective/#Collective-communication","page":"Collective communication","title":"Collective communication","text":"","category":"section"},{"location":"collective/#Synchronization","page":"Collective communication","title":"Synchronization","text":"","category":"section"},{"location":"collective/","page":"Collective communication","title":"Collective communication","text":"MPI.Barrier","category":"page"},{"location":"collective/#MPI.Barrier","page":"Collective communication","title":"MPI.Barrier","text":"Barrier(comm::Comm)\n\nBlocks until comm is synchronized.\n\nIf comm is an intracommunicator, then it blocks until all members of the group have called it.\n\nIf comm is an intercommunicator, then it blocks until all members of the other group have called it.\n\nExternal links\n\nMPI_Barrier man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#Broadcast","page":"Collective communication","title":"Broadcast","text":"","category":"section"},{"location":"collective/","page":"Collective communication","title":"Collective communication","text":"MPI.Bcast!\nMPI.bcast","category":"page"},{"location":"collective/#MPI.Bcast!","page":"Collective communication","title":"MPI.Bcast!","text":"Bcast!(buf, root::Integer, comm::Comm)\n\nBroadcast the buffer buf from root to all processes in comm.\n\nSee also\n\nbcast\n\nExternal links\n\nMPI_Bcast man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.bcast","page":"Collective communication","title":"MPI.bcast","text":"bcast(obj, root::Integer, comm::Comm)\n\nBroadcast the object obj from rank root to all processes on comm. This is able to handle arbitrary data.\n\nSee also\n\nBcast!\n\n\n\n\n\n","category":"function"},{"location":"collective/#Gather/Scatter","page":"Collective communication","title":"Gather/Scatter","text":"","category":"section"},{"location":"collective/#Gather","page":"Collective communication","title":"Gather","text":"","category":"section"},{"location":"collective/","page":"Collective communication","title":"Collective communication","text":"MPI.Allgather!\nMPI.Allgather\nMPI.Allgatherv!\nMPI.Allgatherv\nMPI.Gather!\nMPI.Gather\nMPI.Gatherv!\nMPI.Gatherv","category":"page"},{"location":"collective/#MPI.Allgather!","page":"Collective communication","title":"MPI.Allgather!","text":"Allgather!(sendbuf, recvbuf::UBuffer, comm::Comm)\nAllgather!(sendrecvbuf::UBuffer, comm::Comm)\n\nEach process sends the contents of sendbuf to the other processes, the result of which is stored in rank order into recvbuf.\n\nsendbuf can be a Buffer object, or any object for which Buffer_send is defined, and should be the same length on all processes.\n\nrecvbuf can be a UBuffer, or can be an AbstractArray if the length can be determined from sendbuf.\n\nIf only one buffer sendrecvbuf is provided, then on each process the data to send is assumed to be in the area where it would receive its own contribution.\n\nSee also\n\nAllgather for the allocating operation\nAllgatherv!/Allgatherv if the number of elements varies between processes.\nGather! to send only to a single root process\n\nExternal links\n\nMPI_Allgather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allgather","page":"Collective communication","title":"MPI.Allgather","text":"Allgather(sendbuf, comm)\n\nEach process sends the contents of sendbuf to the other processes, who store the results in rank order allocating the output buffer.\n\nsendbuf can be an AbstractArray or a scalar, and should be the same size on all processes.\n\nSee also\n\nAllgather! for the mutating operation\nAllgatherv!/Allgatherv if the number of elements varies between processes.\nGather! to send only to a single root process\n\nExternal links\n\nMPI_Allgather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allgatherv!","page":"Collective communication","title":"MPI.Allgatherv!","text":"Allgatherv!(sendbuf, recvbuf::VBuffer, comm::Comm)\nAllgatherv!(sendrecvbuf::VBuffer, comm::Comm)\n\nEach process sends the contents of sendbuf to all other process. Each process stores the received in the VBuffer recvbuf.\n\nsendbuf can be a Buffer object, or any object for which Buffer_send is defined.\n\nIf only one buffer sendrecvbuf is provided, then for each process, the data to be sent is taken from the interval of recvbuf where it would store its own data.\n\nSee also\n\nAllgatherv for the allocating operation\nGatherv!/Gatherv to send the result to a single process\n\nExternal links\n\nMPI_Allgatherv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Gather!","page":"Collective communication","title":"MPI.Gather!","text":"Gather!(sendbuf, recvbuf::Union{UBuffer,Nothing}, root::Integer, comm::Comm)\n\nEach process sends the contents of the buffer sendbuf to the root process. The root process stores elements in rank order in the buffer buffer recvbuf.\n\nsendbuf should be a Buffer object, or any object for which Buffer_send is defined, with the same length on all processes, and should be the same length on all processes.\n\nOn the root process, sendbuf can be MPI.IN_PLACE on the root process, in which case the corresponding entries in recvbuf are assumed to be already in place (this corresponds the behaviour of MPI_IN_PLACE in MPI_Gather). For example:\n\nif root == MPI.Comm_rank(comm)\n    MPI.Gather!(MPI.IN_PLACE, UBuffer(buf, count), root, comm)\nelse\n    MPI.Gather!(buf, nothing, root, comm)\nend\n\nrecvbuf on the root process should be a UBuffer, or can be an AbstractArray if the length can be determined from sendbuf. On non-root processes it is ignored and can be nothing.\n\nSee also\n\nGather for the allocating operation.\nGatherv! if the number of elements varies between processes.\nAllgather! to send the result to all processes.\n\nExternal links\n\nMPI_Gather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Gather","page":"Collective communication","title":"MPI.Gather","text":"Gather(sendbuf, root, comm::Comm)\n\nEach process sends the contents of the buffer sendbuf to the root process. The root allocates the output buffer and stores elements in rank order.\n\nsendbuf can be an AbstractArray or a scalar, and should be the same length on all processes.\n\nSee also\n\nGather! for the mutating operation.\nGatherv!/Gatherv if the number of elements varies between processes.\nAllgather!/Allgather to send the result to all processes.\n\nExternal links\n\nMPI_Gather man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Gatherv!","page":"Collective communication","title":"MPI.Gatherv!","text":"Gatherv!(sendbuf, recvbuf::Union{VBuffer,Nothing}, root, comm)\n\nEach process sends the contents of the buffer sendbuf to the root process. The root stores elements in rank order in the buffer recvbuf.\n\nsendbuf should be a Buffer object, or any object for which Buffer_send is defined, with the same length on all processes.\n\nOn the root process, sendbuf can be MPI.IN_PLACE, in which case the corresponding entries in recvbuf are assumed to be already in place. For example\n\nif root == MPI.Comm_rank(comm)\n    Gatherv!(MPI.IN_PLACE, VBuffer(buf, counts), root, comm)\nelse\n    Gatherv!(buf, nothing, root, comm)\nend\n\nrecvbuf on the root process should be a VBuffer, or can be an AbstractArray if the length can be determined from sendbuf. On non-root processes it is ignored and can be nothing.\n\nSee also\n\nGatherv for the allocating operation\nGather!\nAllgatherv!/Allgatherv to send the result to all processes\n\nExternal links\n\nMPI_Gatherv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#Scatter","page":"Collective communication","title":"Scatter","text":"","category":"section"},{"location":"collective/","page":"Collective communication","title":"Collective communication","text":"MPI.Scatter!\nMPI.Scatterv!","category":"page"},{"location":"collective/#MPI.Scatter!","page":"Collective communication","title":"MPI.Scatter!","text":"Scatter!(sendbuf::Union{UBuffer,Nothing}, recvbuf, root::Integer, comm::Comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks, sending the j-th chunk to the process of rank j-1 into the recvbuf buffer.\n\nsendbuf on the root process should be a UBuffer (an Array can also be passed directly if the sizes can be determined from recvbuf). On non-root processes it is ignored, and nothing can be passed instead.\n\nrecvbuf is a Buffer object, or any object for which Buffer(recvbuf) is defined. On the root process, it can also be MPI.IN_PLACE, in which case it is unmodified. For example:\n\nif root == MPI.Comm_rank(comm)\n    MPI.Scatter!(UBuffer(buf, count), MPI.IN_PLACE, root, comm)\nelse\n    MPI.Scatter!(nothing, buf, root, comm)        \nend\n\nSee also\n\nScatterv! if the number of elements varies between processes.\n\nExternal links\n\nMPI_Scatter man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scatterv!","page":"Collective communication","title":"MPI.Scatterv!","text":"Scatterv!(sendbuf::Union{VBuffer,Nothing}, recvbuf, root, comm)\n\nSplits the buffer sendbuf in the root process into Comm_size(comm) chunks and sends the jth chunk to the process of rank j-1 into the recvbuf buffer.\n\nsendbuf on the root process should be a VBuffer. On non-root processes it is ignored, and nothing can be passed instead.\n\nrecvbuf is a Buffer object, or any object for which Buffer(recvbuf) is defined. On the root process, it can also be MPI.IN_PLACE, in which case it is unmodified. For example:\n\nif root == MPI.Comm_rank(comm)\n    MPI.Scatterv!(VBuffer(buf, counts), MPI.IN_PLACE, root, comm)\nelse\n    MPI.Scatterv!(nothing, buf, root, comm)\nend\n\nSee also\n\nScatter! if the number of elements are the same for all processes\n\nExternal links\n\nMPI_Scatterv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#All-to-all","page":"Collective communication","title":"All-to-all","text":"","category":"section"},{"location":"collective/","page":"Collective communication","title":"Collective communication","text":"MPI.Alltoall!\nMPI.Alltoall\nMPI.Alltoallv!\nMPI.Alltoallv","category":"page"},{"location":"collective/#MPI.Alltoall!","page":"Collective communication","title":"MPI.Alltoall!","text":"Alltoall!(sendbuf::UBuffer, recvbuf::UBuffer, comm::Comm)\nAlltoall!(sendrecvbuf::UBuffer, comm::Comm)\n\nEvery process divides the UBuffer sendbuf into Comm_size(comm) chunks of equal size, sending the j-th chunk to the process of rank j-1.  Every process stores the data received from rank j-1 process in the j-th chunk of the buffer recvbuf.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\nIf only one buffer sendrecvbuf is used, then data is overwritten.\n\nSee also\n\nAlltoall for the allocating operation\n\nExternal links\n\nMPI_Alltoall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Alltoall","page":"Collective communication","title":"MPI.Alltoall","text":"Alltoall(sendbuf::UBuffer, comm::Comm)\n\nEvery process divides the UBuffer sendbuf into Comm_size(comm) chunks of equal size, sending the j-th chunk to the process of rank j-1. Every process allocates the output buffer and stores the data received from the process on rank j-1 in the j-th chunk.\n\nrank    send buf                        recv buf\n----    --------                        --------\n 0      a,b,c,d,e,f       Alltoall      a,b,A,B,α,β\n 1      A,B,C,D,E,F  ---------------->  c,d,C,D,γ,ψ\n 2      α,β,γ,ψ,η,ν                     e,f,E,F,η,ν\n\nSee also\n\nAlltoall! for the mutating operation\n\nExternal links\n\nMPI_Alltoall man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Alltoallv!","page":"Collective communication","title":"MPI.Alltoallv!","text":"Alltoallv!(sendbuf::VBuffer, recvbuf::VBuffer, comm::Comm)\n\nSimilar to Alltoall!, except with different size chunks per process. \n\nSee also\n\nVBuffer\n\nExternal links\n\nMPI_Alltoallv man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#Reduce/Scan","page":"Collective communication","title":"Reduce/Scan","text":"","category":"section"},{"location":"collective/","page":"Collective communication","title":"Collective communication","text":"MPI.Reduce!\nMPI.Reduce\nMPI.Allreduce!\nMPI.Allreduce\nMPI.Scan!\nMPI.Scan\nMPI.Exscan!\nMPI.Exscan","category":"page"},{"location":"collective/#MPI.Reduce!","page":"Collective communication","title":"MPI.Reduce!","text":"Reduce!(sendbuf, recvbuf, op, root::Integer, comm::Comm)\nReduce!(sendrecvbuf, op, root::Integer, comm::Comm)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf and stores the result in recvbuf on the process of rank root.\n\nOn non-root processes recvbuf is ignored, and can be nothing. \n\nTo perform the reduction in place, provide a single buffer sendrecvbuf.\n\nSee also\n\nReduce to handle allocation of the output buffer.\nAllreduce!/Allreduce to send reduction to all ranks.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Reduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Reduce","page":"Collective communication","title":"MPI.Reduce","text":"recvbuf = Reduce(sendbuf, op, root::Integer, comm::Comm)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, returning the result recvbuf on the process of rank root, and nothing on non-root processes.\n\nsendbuf can also be a scalar, in which case recvbuf will be a value of the same type.\n\nSee also\n\nReduce! for mutating and in-place operations\nAllreduce!/Allreduce to send reduction to all ranks.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Reduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allreduce!","page":"Collective communication","title":"MPI.Allreduce!","text":"Allreduce!(sendbuf, recvbuf, op, comm::Comm)\nAllreduce!(sendrecvbuf, op, comm::Comm)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, storing the result in the recvbuf of all processes in the group.\n\nAllreduce! is equivalent to a Reduce! operation followed by a Bcast!, but can lead to better performance.\n\nIf only one sendrecvbuf buffer is provided, then the operation is performed in-place.\n\nSee also\n\nAllreduce, to handle allocation of the output buffer.\nReduce!/Reduce to send reduction to a single rank.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Allreduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Allreduce","page":"Collective communication","title":"MPI.Allreduce","text":"recvbuf = Allreduce(sendbuf, op, comm)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, returning the result in the recvbuf of all processes in the group.\n\nsendbuf can also be a scalar, in which case recvbuf will be a value of the same type.\n\nSee also\n\nAllreduce! for mutating or in-place operations.\nReduce!/Reduce to send reduction to a single rank.\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Allreduce man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scan!","page":"Collective communication","title":"MPI.Scan!","text":"Scan!(sendbuf, recvbuf, op, comm::Comm)\nScan!(sendrecvbuf, op, comm::Comm)\n\nInclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i.\n\nIf only a single buffer sendrecvbuf is provided, then operations will be performed in-place.\n\nSee also\n\nScan to handle allocation of the output buffer\nExscan!/Exscan for exclusive scan\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Scan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Scan","page":"Collective communication","title":"MPI.Scan","text":"recvbuf = Scan(sendbuf, op, comm::Comm)\n\nInclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i.\n\nsendbuf can also be a scalar, in which case recvbuf will also be a scalar of the same type.\n\nSee also\n\nScan! for mutating or in-place operations\nExscan!/Exscan for exclusive scan\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Scan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Exscan!","page":"Collective communication","title":"MPI.Exscan!","text":"Exscan!(sendbuf, recvbuf, op, comm::Comm)\nExscan!(sendrecvbuf, op, comm::Comm)\n\nExclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i-1. The recvbuf on rank 0 is ignored, and the recvbuf on rank 1 will contain the contents of sendbuf on rank 0.\n\nIf only a single sendrecvbuf is provided, then operations are performed in-place, and buf on rank 0 will remain unchanged.\n\nSee also\n\nExscan to handle allocation of the output buffer\nScan!/Scan for inclusive scan\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Exscan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"collective/#MPI.Exscan","page":"Collective communication","title":"MPI.Exscan","text":"recvbuf = Exscan(sendbuf, op, comm::Comm)\n\nExclusive prefix reduction (analagous to accumulate in Julia): recvbuf on rank i will contain the the result of reducing sendbuf by op from ranks 0:i-1. The recvbuf on rank 0 is undefined, and the recvbuf on rank 1 will contain the contents of sendbuf on rank 0.\n\nSee also\n\nExscan! for mutating and in-place operations\nScan!/Scan for inclusive scan\nOp for details on reduction operators.\n\nExternal links\n\nMPI_Scan man page: OpenMPI, MPICH\n\n\n\n\n\n","category":"function"},{"location":"examples/01-hello/#Hello-world","page":"Hello world","title":"Hello world","text":"","category":"section"},{"location":"examples/01-hello/","page":"Hello world","title":"Hello world","text":"# examples/01-hello.jl\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nprint(\"Hello world, I am rank $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\\n\")\nMPI.Barrier(comm)","category":"page"},{"location":"examples/01-hello/","page":"Hello world","title":"Hello world","text":"> mpiexecjl -n 3 julia examples/01-hello.jl\nHello world, I am rank 0 of 3\nHello world, I am rank 2 of 3\nHello world, I am rank 1 of 3","category":"page"},{"location":"#MPI.jl","page":"MPI.jl","title":"MPI.jl","text":"","category":"section"},{"location":"","page":"MPI.jl","title":"MPI.jl","text":"This is a basic Julia wrapper for the portable message passing system Message Passing Interface (MPI). Inspiration is taken from mpi4py, although we generally follow the C and not the C++ MPI API. (The C++ MPI API is deprecated.)","category":"page"}]
}
