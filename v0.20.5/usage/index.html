<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Usage · MPI.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MPI.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MPI.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">MPI.jl</a></li><li><a class="tocitem" href="../configuration/">Configuration</a></li><li class="is-active"><a class="tocitem" href>Usage</a><ul class="internal"><li><a class="tocitem" href="#Basic-example"><span>Basic example</span></a></li><li><a class="tocitem" href="#Julia-wrapper-for-mpiexec"><span>Julia wrapper for <code>mpiexec</code></span></a></li><li><a class="tocitem" href="#CUDA-aware-MPI-support"><span>CUDA-aware MPI support</span></a></li><li><a class="tocitem" href="#ROCm-aware-MPI-support"><span>ROCm-aware MPI support</span></a></li><li><a class="tocitem" href="#Writing-MPI-tests"><span>Writing MPI tests</span></a></li></ul></li><li><a class="tocitem" href="../knownissues/">Known issues</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/01-hello/">Hello world</a></li><li><a class="tocitem" href="../examples/02-broadcast/">Broadcast</a></li><li><a class="tocitem" href="../examples/03-reduce/">Reduce</a></li><li><a class="tocitem" href="../examples/04-sendrecv/">Send/receive</a></li><li><a class="tocitem" href="../examples/05-job_schedule/">Job Scheduling</a></li><li><a class="tocitem" href="../examples/06-scatterv/">Scatterv and Gatherv</a></li><li><a class="tocitem" href="../examples/07-rma_active/">Active RMA</a></li><li><a class="tocitem" href="../examples/08-rma_passive/">Passive RMA</a></li><li><a class="tocitem" href="../examples/09-graph_communication/">Graph Communication</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../reference/mpipreferences/">MPIPreferences.jl</a></li><li><a class="tocitem" href="../reference/library/">Library information</a></li><li><a class="tocitem" href="../reference/environment/">Environment</a></li><li><a class="tocitem" href="../reference/misc/">Miscellanea</a></li><li><a class="tocitem" href="../reference/comm/">Communicators</a></li><li><a class="tocitem" href="../reference/buffers/">Buffers</a></li><li><a class="tocitem" href="../reference/pointtopoint/">Point-to-point communication</a></li><li><a class="tocitem" href="../reference/collective/">Collective communication</a></li><li><a class="tocitem" href="../reference/onesided/">One-sided communication</a></li><li><a class="tocitem" href="../reference/topology/">Topology</a></li><li><a class="tocitem" href="../reference/io/">I/O</a></li><li><a class="tocitem" href="../reference/advanced/">Advanced</a></li><li><a class="tocitem" href="../reference/api/">Low-level API</a></li></ul></li><li><a class="tocitem" href="../refindex/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Usage</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaParallel/MPI.jl/blob/master/docs/src/usage.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Usage"><a class="docs-heading-anchor" href="#Usage">Usage</a><a id="Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Usage" title="Permalink"></a></h1><p>MPI is based on a <a href="https://en.wikipedia.org/wiki/SPMD">single program, multiple data (SPMD)</a> model, where multiple processes are launched running independent programs, which then communicate as necessary via messages.</p><p>As the main entry point for users, <code>MPI.jl</code> provides a high-level interface which loosely follows the MPI C API and is described in details in the following sections. The syntax should look familiar if you know MPI already, but some arguments may not be needed (e.g. the type or the number of elements of arrays, which are inferred automatically), others may be placed slightly differently, and others may be optional keyword arguments (e.g. for the index of the root process, or the source and destination of point-to-point communication functions).</p><p>In addition to the high-level interface, <code>MPI.jl</code> provides a <a href="../reference/api/#Low-level-API">low-level API</a> which closely matches the MPI C API and from which it has been automatically generated. This is not intended for general usage, but it can be employed if a high-level wrapper is not yet available.</p><h2 id="Basic-example"><a class="docs-heading-anchor" href="#Basic-example">Basic example</a><a id="Basic-example-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-example" title="Permalink"></a></h2><p>A script should include <code>using MPI</code> and <a href="../reference/environment/#MPI.Init"><code>MPI.Init()</code></a> statements before calling any MPI operations, for example</p><pre><code class="language-julia hljs"># examples/01-hello.jl
using MPI
MPI.Init()

comm = MPI.COMM_WORLD
println(&quot;Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))&quot;)
MPI.Barrier(comm)</code></pre><p>Calling <a href="../reference/environment/#MPI.Finalize"><code>MPI.Finalize()</code></a> at the end of the program is optional, as it will be called automatically when Julia exits.</p><p>The program can then be launched via an MPI launch command (typically <code>mpiexec</code>, <code>mpirun</code> or <code>srun</code>), e.g.</p><pre><code class="nohighlight hljs">$ mpiexec -n 3 julia --project examples/01-hello.jl
Hello world, I am rank 0 of 3
Hello world, I am rank 2 of 3
Hello world, I am rank 1 of 3</code></pre><p>The <a href="../reference/environment/#MPICH_jll.mpiexec"><code>mpiexec</code></a> function is provided for launching MPI programs from Julia itself.</p><h2 id="Julia-wrapper-for-mpiexec"><a class="docs-heading-anchor" href="#Julia-wrapper-for-mpiexec">Julia wrapper for <code>mpiexec</code></a><a id="Julia-wrapper-for-mpiexec-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-wrapper-for-mpiexec" title="Permalink"></a></h2><p>Since you can configure <code>MPI.jl</code> to use one of several MPI implementations, you may have different Julia projects using different implementation.  Thus, it may be cumbersome to find out which <code>mpiexec</code> executable is associated to a specific project.  To make this easy, on Unix-based systems <code>MPI.jl</code> comes with a thin project-aware wrapper around <code>mpiexec</code>, called <code>mpiexecjl</code>.</p><h3 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h3><p>You can install <code>mpiexecjl</code> with <a href="../reference/environment/#MPI.install_mpiexecjl"><code>MPI.install_mpiexecjl()</code></a>.  The default destination directory is <code>joinpath(DEPOT_PATH[1], &quot;bin&quot;)</code>, which usually translates to <code>~/.julia/bin</code>, but check the value on your system.  You can also tell <code>MPI.install_mpiexecjl</code> to install to a different directory.</p><pre><code class="language-sh hljs">$ julia
julia&gt; using MPI
julia&gt; MPI.install_mpiexecjl()</code></pre><p>To quickly call this wrapper we recommend you to add the destination directory to your <a href="https://en.wikipedia.org/wiki/PATH_(variable)"><code>PATH</code></a> environment variable.</p><h3 id="Usage-2"><a class="docs-heading-anchor" href="#Usage-2">Usage</a><a class="docs-heading-anchor-permalink" href="#Usage-2" title="Permalink"></a></h3><p><code>mpiexecjl</code> has the same syntax as the <code>mpiexec</code> binary that will be called, but it takes in addition a <code>--project</code> option to call the specific binary associated to the <code>MPI.jl</code> version in the given project.  If no <code>--project</code> flag is used, the <code>MPI.jl</code> in the global Julia environment will be used instead.</p><p>After installing <code>mpiexecjl</code> and adding its directory to <code>PATH</code>, you can run it with:</p><pre><code class="language-sh hljs">$ mpiexecjl --project=/path/to/project -n 20 julia script.jl</code></pre><h2 id="CUDA-aware-MPI-support"><a class="docs-heading-anchor" href="#CUDA-aware-MPI-support">CUDA-aware MPI support</a><a id="CUDA-aware-MPI-support-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-aware-MPI-support" title="Permalink"></a></h2><p>If your MPI implementation has been compiled with CUDA support, then <code>CUDA.CuArray</code>s (from the <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> package) can be passed directly as send and receive buffers for point-to-point and collective operations (they may also work with one-sided operations, but these are not often supported).</p><p>Successfully running the <a href="https://gist.github.com/luraess/0063e90cb08eb2208b7fe204bbd90ed2">alltoall_test_cuda.jl</a>  should confirm your MPI implementation to have the CUDA support enabled. Moreover, successfully running the  <a href="https://gist.github.com/luraess/ed93cc09ba04fe16f63b4219c1811566">alltoall_test_cuda_multigpu.jl</a> should confirm  your CUDA-aware MPI implementation to use multiple Nvidia GPUs (one GPU per rank).</p><p>If using OpenMPI, the status of CUDA support can be checked via the <a href="../reference/library/#MPI.has_cuda"><code>MPI.has_cuda()</code></a> function.</p><h2 id="ROCm-aware-MPI-support"><a class="docs-heading-anchor" href="#ROCm-aware-MPI-support">ROCm-aware MPI support</a><a id="ROCm-aware-MPI-support-1"></a><a class="docs-heading-anchor-permalink" href="#ROCm-aware-MPI-support" title="Permalink"></a></h2><p>If your MPI implementation has been compiled with ROCm support (AMDGPU), then <code>AMDGPU.ROCArray</code>s (from the <a href="https://github.com/JuliaGPU/AMDGPU.jl">AMDGPU.jl</a> package) can be passed directly as send and receive buffers for point-to-point and collective operations (they may also work with one-sided operations, but these are not often supported).</p><p>Successfully running the <a href="https://gist.github.com/luraess/c228ec08629737888a18c6a1e397643c">alltoall_test_rocm.jl</a>  should confirm your MPI implementation to have the ROCm support (AMDGPU) enabled. Moreover, successfully running the  <a href="https://gist.github.com/luraess/a47931d7fb668bd4348a2c730d5489f4">alltoall_test_rocm_multigpu.jl</a> should confirm  your ROCm-aware MPI implementation to use multiple AMD GPUs (one GPU per rank).</p><p>The status of ROCm (AMDGPU) support cannot currently be queried.</p><h2 id="Writing-MPI-tests"><a class="docs-heading-anchor" href="#Writing-MPI-tests">Writing MPI tests</a><a id="Writing-MPI-tests-1"></a><a class="docs-heading-anchor-permalink" href="#Writing-MPI-tests" title="Permalink"></a></h2><p>It is recommended to use the <code>mpiexec()</code> wrapper when writing your package tests in <code>runtests.jl</code>:</p><pre><code class="language-julia hljs"># test/runtests.jl
using MPI
using Test

@testset &quot;hello&quot; begin
    n = 2  # number of processes
    mpiexec() do exe  # MPI wrapper
        run(`$exe -n $n $(Base.julia_cmd()) [...]/01-hello.jl`)
        # alternatively:
        # p = run(ignorestatus(`...`))
        # @test success(p)
    end
end</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../configuration/">« Configuration</a><a class="docs-footer-nextpage" href="../knownissues/">Known issues »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 8 December 2022 01:04">Thursday 8 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
